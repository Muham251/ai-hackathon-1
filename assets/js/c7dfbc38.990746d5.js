"use strict";(globalThis.webpackChunkwebsite=globalThis.webpackChunkwebsite||[]).push([[267],{454(e,n,i){i.r(n),i.d(n,{assets:()=>l,contentTitle:()=>a,default:()=>u,frontMatter:()=>r,metadata:()=>t,toc:()=>c});const t=JSON.parse('{"id":"module-4/module-4","title":"Module 4: Vision-Language-Action (VLA)","description":"1. Business Understanding","source":"@site/docs/module-4/index.md","sourceDirName":"module-4","slug":"/module-4/","permalink":"/ai-hackathon-1/docs/module-4/","draft":false,"unlisted":false,"editUrl":"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/module-4/index.md","tags":[],"version":"current","frontMatter":{"id":"module-4","title":"Module 4: Vision-Language-Action (VLA)","stage":"spec","branch":"main"},"sidebar":"tutorialSidebar","previous":{"title":"Chapter 3: Nav2: Path Planning for Humanoids","permalink":"/ai-hackathon-1/docs/module-3/nav2-path-planning"},"next":{"title":"Chapter 1: Voice-to-Action: Whisper and Command Extraction","permalink":"/ai-hackathon-1/docs/module-4/voice-to-action"}}');var s=i(4848),o=i(8453);const r={id:"module-4",title:"Module 4: Vision-Language-Action (VLA)",stage:"spec",branch:"main"},a="Spec: Module 4: Vision-Language-Action (VLA)",l={},c=[{value:"1. Business Understanding",id:"1-business-understanding",level:2},{value:"1.1. Goal",id:"11-goal",level:3},{value:"1.2. Justification",id:"12-justification",level:3},{value:"1.3. Target Audience",id:"13-target-audience",level:3},{value:"2. Functional Requirements",id:"2-functional-requirements",level:2},{value:"2.1. In Scope (Features)",id:"21-in-scope-features",level:3},{value:"2.2. Out of Scope",id:"22-out-of-scope",level:3},{value:"2.3. Success Criteria",id:"23-success-criteria",level:3},{value:"2.4. Constraints",id:"24-constraints",level:3},{value:"3. UX/UI (Reader Experience)",id:"3-uxui-reader-experience",level:2},{value:"3.1. Content Structure",id:"31-content-structure",level:3},{value:"4. Technical Considerations",id:"4-technical-considerations",level:2},{value:"4.1. Key Decisions",id:"41-key-decisions",level:3},{value:"4.2. External Dependencies",id:"42-external-dependencies",level:3}];function d(e){const n={code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",ol:"ol",p:"p",strong:"strong",ul:"ul",...(0,o.R)(),...e.components};return(0,s.jsxs)(s.Fragment,{children:[(0,s.jsx)(n.header,{children:(0,s.jsx)(n.h1,{id:"spec-module-4-vision-language-action-vla",children:"Spec: Module 4: Vision-Language-Action (VLA)"})}),"\n",(0,s.jsx)(n.h2,{id:"1-business-understanding",children:"1. Business Understanding"}),"\n",(0,s.jsx)(n.h3,{id:"11-goal",children:"1.1. Goal"}),"\n",(0,s.jsx)(n.p,{children:"The primary goal of this module is to teach students how to build a complete Vision-Language-Action (VLA) pipeline. This involves integrating Large Language Models (LLMs) with robotics, using Whisper for voice command input, employing LLMs for cognitive planning, and executing tasks through ROS 2 actions."}),"\n",(0,s.jsx)(n.h3,{id:"12-justification",children:"1.2. Justification"}),"\n",(0,s.jsx)(n.p,{children:"VLA is a transformative paradigm in robotics, enabling more natural human-robot interaction and a higher degree of autonomy. This module provides a crucial bridge between theoretical AI concepts (like LLMs) and practical, physical robotics, equipping students with highly sought-after skills in creating intelligent, responsive robots."}),"\n",(0,s.jsx)(n.h3,{id:"13-target-audience",children:"1.3. Target Audience"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Primary:"})," Beginner to intermediate students in robotics and AI."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Secondary:"})," Developers and researchers interested in the practical application of LLMs in autonomous systems."]}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"2-functional-requirements",children:"2. Functional Requirements"}),"\n",(0,s.jsx)(n.h3,{id:"21-in-scope-features",children:"2.1. In Scope (Features)"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"VLA Pipeline Overview:"})," A clear, conceptual explanation of the entire Vision-Language-Action pipeline, from voice input to robot action."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Voice Command Processing:"})," A tutorial on using OpenAI Whisper to transcribe spoken language into text and extract actionable commands."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"LLM-based Planning:"}),' A guide on how to prompt an LLM to translate a high-level natural language task (e.g., "get me the apple") into a sequence of concrete ROS 2 action goals.']}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Mini-Capstone Project:"})," A complete, runnable example demonstrating the full VLA loop. A user will issue a voice command, and a simulated humanoid robot will plan and execute a simple multi-step task involving navigation, perception, and manipulation."]}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"22-out-of-scope",children:"2.2. Out of Scope"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"The development of a production-grade, highly reliable robotics stack."}),"\n",(0,s.jsx)(n.li,{children:"Training or fine-tuning custom Large Language Models."}),"\n",(0,s.jsx)(n.li,{children:"Complex, long-horizon tasks or navigation in dynamic, multi-room environments."}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"23-success-criteria",children:"2.3. Success Criteria"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Understanding:"})," Students can draw a diagram of the VLA pipeline and explain the role of each component (Voice, LLM, ROS 2)."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Application (Whisper):"})," Students are able to run a demo that successfully converts a spoken command into a text string."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Application (LLM Planning):"})," Students can explain how a natural language goal is decomposed into a JSON or YAML sequence of ROS 2 actions by an LLM."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Completion:"})," The mini-capstone example runs end-to-end: a voice command triggers a sequence of autonomous actions in the simulated robot."]}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"24-constraints",children:"2.4. Constraints"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Word Count:"})," 2000\u20133000 words."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Format:"})," Docusaurus Markdown, including code snippets for Python (for Whisper and LLM interaction) and configuration files (for ROS 2 actions)."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Sources:"})," Content will be based on official OpenAI Whisper documentation, ROS 2 action design patterns, and foundational VLA research."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Timeline:"})," 1 week."]}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"3-uxui-reader-experience",children:"3. UX/UI (Reader Experience)"}),"\n",(0,s.jsx)(n.h3,{id:"31-content-structure",children:"3.1. Content Structure"}),"\n",(0,s.jsx)(n.p,{children:"The module will be structured to guide the learner from individual components to a fully integrated system."}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Chapter 1: Voice-to-Action: Whisper and Command Extraction"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Introduction to Whisper for speech-to-text."}),"\n",(0,s.jsx)(n.li,{children:"Tutorial: Setting up Whisper and running a simple voice transcription demo."}),"\n",(0,s.jsxs)(n.li,{children:['Guide: Extracting key intents and entities from the transcribed text (e.g., turning "Can you please find the red block" into ',(0,s.jsx)(n.code,{children:'{action: "find", object: "red block"}'}),")."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Chapter 2: Cognitive Planning with LLMs"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:'The role of LLMs as a "reasoning engine" for robots.'}),"\n",(0,s.jsx)(n.li,{children:"Tutorial: Prompt engineering techniques to make an LLM translate a task into a machine-readable plan (a sequence of ROS 2 actions)."}),"\n",(0,s.jsxs)(n.li,{children:["Example: Converting ",(0,s.jsx)(n.code,{children:'{action: "find", object: "red block"}'})," into a plan like ",(0,s.jsx)(n.code,{children:'[{navigate: "table"}, {detect: "red_block"}]'}),"."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Chapter 3: Mini-Capstone: The Full VLA Loop"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"An overview of the complete system architecture."}),"\n",(0,s.jsx)(n.li,{children:"A step-by-step guide to launching all components (Whisper listener, LLM planner, ROS 2 action servers)."}),"\n",(0,s.jsx)(n.li,{children:'A full demonstration of an autonomous task: e.g., "Go to the table, find the blue cup, and pick it up."'}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"4-technical-considerations",children:"4. Technical Considerations"}),"\n",(0,s.jsx)(n.h3,{id:"41-key-decisions",children:"4.1. Key Decisions"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"LLM Abstraction:"})," We will use a major LLM provider's API (e.g., OpenAI, Anthropic, or Google) to avoid the complexity of self-hosting. The focus is on the integration, not the model itself."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"ROS 2 Actions:"})," The module will use the standard ROS 2 action interface for all robot tasks (e.g., ",(0,s.jsx)(n.code,{children:"NavigateToPose"}),", ",(0,s.jsx)(n.code,{children:"DetectObject"}),", ",(0,s.jsx)(n.code,{children:"PickAndPlace"}),"). This ensures a modular and scalable design."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Focus on Integration:"}),' The code provided will be primarily "glue code" written in Python, connecting the various components of the VLA pipeline.']}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"42-external-dependencies",children:"4.2. External Dependencies"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Software:"})," Python 3, ROS 2, and the OpenAI Whisper library."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"APIs:"})," An API key for an LLM provider will be required for the cognitive planning chapter."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Hardware:"})," A microphone for voice commands."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Prerequisites:"})," Completion of prior modules, including ROS 2 basics, simulation, and navigation."]}),"\n"]})]})}function u(e={}){const{wrapper:n}={...(0,o.R)(),...e.components};return n?(0,s.jsx)(n,{...e,children:(0,s.jsx)(d,{...e})}):d(e)}},8453(e,n,i){i.d(n,{R:()=>r,x:()=>a});var t=i(6540);const s={},o=t.createContext(s);function r(e){const n=t.useContext(o);return t.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function a(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(s):e.components||s:r(e.components),t.createElement(o.Provider,{value:n},e.children)}}}]);