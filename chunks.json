[
  {
    "url": "https://Muham251.github.io/ai-hackathon-1/",
    "text": "Easy to Use\nDocusaurus was designed from the ground up to be easily installed and used to get your website up and running quickly.\nFocus on What Matters\nDocusaurus lets you focus on your docs, and we'll do the chores. Go ahead and move your docs into the docs\ndirectory.\nPowered by React\nExtend or customize your website layout by reusing React. Docusaurus can be extended while reusing the same header and footer.\n--------------------------------------------------------------------------------"
  },
  {
    "url": "https://Muham251.github.io/ai-hackathon-1/#__docusaurus_skipToContent_fallback",
    "text": "Easy to Use\nDocusaurus was designed from the ground up to be easily installed and used to get your website up and running quickly.\nFocus on What Matters\nDocusaurus lets you focus on your docs, and we'll do the chores. Go ahead and move your docs into the docs\ndirectory.\nPowered by React\nExtend or customize your website layout by reusing React. Docusaurus can be extended while reusing the same header and footer.\n--------------------------------------------------------------------------------"
  },
  {
    "url": "https://Muham251.github.io/ai-hackathon-1/docs/intro",
    "text": "Tutorial Intro\nLet's discover Docusaurus in less than 5 minutes.\nGetting Started\nGet started by creating a new site.\nOr try Docusaurus immediately with docusaurus.new.\nWhat you'll need\n- Node.js version 20.0 or above:\n- When installing Node.js, you are recommended to check all checkboxes related to dependencies.\nGenerate a new site\nGenerate a new Docusaurus site using the classic template.\nThe classic template will automatically be added to your project after you run the command:\nnpm init docusaurus@latest my-website classic\nYou can type this command into Command Prompt, Powershell, Terminal, or any other integrated terminal of your code editor.\nThe command also installs all necessary dependencies you need to run Docusaurus.\nStart your site\nRun the development server:\ncd my-website\nnpm run start\nThe cd\ncommand changes the directory you're working with. In order to work with your newly created Docusaurus site, you'll need to navigate the terminal there.\nThe npm run start\ncommand builds your website locally and serves it through a development server, ready for you to view at http://localhost:3000/.\nOpen docs/intro.md\n(this page) and edit some lines: the site reloads automatically and displays your changes.\n--------------------------------------------------------------------------------"
  },
  {
    "url": "https://Muham251.github.io/ai-hackathon-1/docs/intro#__docusaurus_skipToContent_fallback",
    "text": "Tutorial Intro\nLet's discover Docusaurus in less than 5 minutes.\nGetting Started\nGet started by creating a new site.\nOr try Docusaurus immediately with docusaurus.new.\nWhat you'll need\n- Node.js version 20.0 or above:\n- When installing Node.js, you are recommended to check all checkboxes related to dependencies.\nGenerate a new site\nGenerate a new Docusaurus site using the classic template.\nThe classic template will automatically be added to your project after you run the command:\nnpm init docusaurus@latest my-website classic\nYou can type this command into Command Prompt, Powershell, Terminal, or any other integrated terminal of your code editor.\nThe command also installs all necessary dependencies you need to run Docusaurus.\nStart your site\nRun the development server:\ncd my-website\nnpm run start\nThe cd\ncommand changes the directory you're working with. In order to work with your newly created Docusaurus site, you'll need to navigate the terminal there.\nThe npm run start\ncommand builds your website locally and serves it through a development server, ready for you to view at http://localhost:3000/.\nOpen docs/intro.md\n(this page) and edit some lines: the site reloads automatically and displays your changes.\n--------------------------------------------------------------------------------"
  },
  {
    "url": "https://Muham251.github.io/ai-hackathon-1/docs/module-1/",
    "text": "Feature Specification: Module 1 \u2014 The Robotic Nervous System (ROS 2)\nFeature Branch: module-1\nCreated: December 7, 2025\nStatus: Draft\nInput: User description: \"Module: 1 \u2014 The Robotic Nervous System (ROS 2)\"\nUser Scenarios & Testing (mandatory)\nUser Story 1 - ROS 2 Fundamentals (Priority: P1)\nAs a beginner robotics student, I want to understand ROS 2 nodes and topics so I can run basic robot programs.\nWhy this priority: This is the foundational knowledge required to work with any ROS 2 system, and without understanding these core concepts, the learner cannot progress to more advanced topics.\nIndependent Test: Can be fully tested by successfully running a basic publisher/subscriber example and explaining the communication patterns.\nAcceptance Scenarios:\n- Given a basic understanding of programming concepts, When the user reads the ROS 2 fundamentals chapter, Then they can explain the roles of nodes and topics\n- Given a working ROS 2 environment, When the user runs the provided publisher/subscriber example, Then they can observe the message passing between nodes\nUser Story 2 - Python Integration with rclpy (Priority: P2)\nAs an AI developer, I want to use rclpy to connect Python agents to robot controllers.\nWhy this priority: Python is the primary language for AI development, so connecting AI agents to ROS 2 systems is critical for the target audience.\nIndependent Test: Can be fully tested by successfully running a Python script that communicates with ROS 2 and demonstrates control concepts.\nAcceptance Scenarios:\n- Given a ROS 2 environment with rclpy installed, When the user runs the Python examples, Then they can see the interaction between Python code and robot controllers\nUser Story 3 - URDF for Humanoid Robots (Priority: P3)\nAs a humanoid robotics learner, I want to understand URDF so I can model a robot body.\nWhy this priority: Understanding robot modeling is essential for working with humanoid robots, which is the main focus of this curriculum.\nIndependent Test: Can be fully tested by successfully loading and modifying a basic humanoid URDF model.\nAcceptance Scenarios:\n- Given a basic URDF file, When the user modifies it following the tutorial, Then they can create a valid robot body model\nEdge Cases\n- ROS 2 distribution mismatch\n- Missing dependencies during setup\n- URDF syntax errors causing launch failures\n- Python environment misconfiguration\nRequirements (mandatory)\nFunctional Requirements\n- FR-001: The module MUST explain ROS 2 nodes, topics, and services accurately.\n- FR-002: The module MUST include executable rclpy examples.\n- FR-003: The module MUST include a valid humanoid URDF example.\n- FR-004: The module MUST avoid ROS 1 concepts.\nKey Entities\n- ROS 2 Nodes\n- Topics and Services\n- rclpy Python scripts\n- URDF files (links, joints, hierarchy)\n- Code blocks and CLI commands\n- Diagrams (conceptual or ASCII)\nSuccess Criteria (mandatory)\nMeasurable Outcomes\n- SC-001: Users can successfully run a ROS 2 publisher/subscriber example.\n- SC-002: Users can modify and execute a Python rclpy node.\n- SC-003: Users can explain the role of nodes and topics.\n- SC-004: Users can understand and edit a basic humanoid URDF model.\nOut of Scope\n- ROS 1\n- Simulation tools (Gazebo / Isaac)\n- Hardware deployment\nContent Structure\n- Chapter 1: ROS 2 Fundamentals for Humanoid Robots\n- Chapter 2: Nodes, Topics, Services & rclpy Integration\n- Chapter 3: URDF Basics for Humanoids\n--------------------------------------------------------------------------------"
  },
  {
    "url": "https://Muham251.github.io/ai-hackathon-1/docs/module-1/#__docusaurus_skipToContent_fallback",
    "text": "Feature Specification: Module 1 \u2014 The Robotic Nervous System (ROS 2)\nFeature Branch: module-1\nCreated: December 7, 2025\nStatus: Draft\nInput: User description: \"Module: 1 \u2014 The Robotic Nervous System (ROS 2)\"\nUser Scenarios & Testing (mandatory)\nUser Story 1 - ROS 2 Fundamentals (Priority: P1)\nAs a beginner robotics student, I want to understand ROS 2 nodes and topics so I can run basic robot programs.\nWhy this priority: This is the foundational knowledge required to work with any ROS 2 system, and without understanding these core concepts, the learner cannot progress to more advanced topics.\nIndependent Test: Can be fully tested by successfully running a basic publisher/subscriber example and explaining the communication patterns.\nAcceptance Scenarios:\n- Given a basic understanding of programming concepts, When the user reads the ROS 2 fundamentals chapter, Then they can explain the roles of nodes and topics\n- Given a working ROS 2 environment, When the user runs the provided publisher/subscriber example, Then they can observe the message passing between nodes\nUser Story 2 - Python Integration with rclpy (Priority: P2)\nAs an AI developer, I want to use rclpy to connect Python agents to robot controllers.\nWhy this priority: Python is the primary language for AI development, so connecting AI agents to ROS 2 systems is critical for the target audience.\nIndependent Test: Can be fully tested by successfully running a Python script that communicates with ROS 2 and demonstrates control concepts.\nAcceptance Scenarios:\n- Given a ROS 2 environment with rclpy installed, When the user runs the Python examples, Then they can see the interaction between Python code and robot controllers\nUser Story 3 - URDF for Humanoid Robots (Priority: P3)\nAs a humanoid robotics learner, I want to understand URDF so I can model a robot body.\nWhy this priority: Understanding robot modeling is essential for working with humanoid robots, which is the main focus of this curriculum.\nIndependent Test: Can be fully tested by successfully loading and modifying a basic humanoid URDF model.\nAcceptance Scenarios:\n- Given a basic URDF file, When the user modifies it following the tutorial, Then they can create a valid robot body model\nEdge Cases\n- ROS 2 distribution mismatch\n- Missing dependencies during setup\n- URDF syntax errors causing launch failures\n- Python environment misconfiguration\nRequirements (mandatory)\nFunctional Requirements\n- FR-001: The module MUST explain ROS 2 nodes, topics, and services accurately.\n- FR-002: The module MUST include executable rclpy examples.\n- FR-003: The module MUST include a valid humanoid URDF example.\n- FR-004: The module MUST avoid ROS 1 concepts.\nKey Entities\n- ROS 2 Nodes\n- Topics and Services\n- rclpy Python scripts\n- URDF files (links, joints, hierarchy)\n- Code blocks and CLI commands\n- Diagrams (conceptual or ASCII)\nSuccess Criteria (mandatory)\nMeasurable Outcomes\n- SC-001: Users can successfully run a ROS 2 publisher/subscriber example.\n- SC-002: Users can modify and execute a Python rclpy node.\n- SC-003: Users can explain the role of nodes and topics.\n- SC-004: Users can understand and edit a basic humanoid URDF model.\nOut of Scope\n- ROS 1\n- Simulation tools (Gazebo / Isaac)\n- Hardware deployment\nContent Structure\n- Chapter 1: ROS 2 Fundamentals for Humanoid Robots\n- Chapter 2: Nodes, Topics, Services & rclpy Integration\n- Chapter 3: URDF Basics for Humanoids\n--------------------------------------------------------------------------------"
  },
  {
    "url": "https://Muham251.github.io/ai-hackathon-1/docs/module-1/nodes-topics-services-rclpy",
    "text": "ROS 2 Fundamentals & Python\nThis chapter covers the three fundamental communication concepts in ROS 2: Nodes, Topics, and Services. We'll also see how to implement them using rclpy\n, the official Python client library for ROS 2.\n1. ROS 2 Nodes\nA Node is the smallest, most fundamental unit of a ROS 2 system. Think of a node as a single, independent process in your robot's software architecture. Each node should have a single, well-defined purpose.\nFor a humanoid robot, you might have nodes for:\n- Reading sensor data from an IMU.\n- Controlling the motors in the right leg.\n- Processing camera images.\n- Planning footsteps for walking.\nNodes are what allow ROS 2 systems to be modular and fault-tolerant. If one node crashes, the rest of the system can continue to run.\nExample: A Simple rclpy\nNode\nHere is the \"hello world\" of rclpy\n: a simple node that initializes itself, prints a message, and then shuts down.\n# 'hello_node.py'\nimport rclpy\nfrom rclpy.node import Node\ndef main(args=None):\n# 1. Initialize the rclpy library\nrclpy.init(args=args)\n# 2. Create a Node\n# The node is named 'hello_ros_node'\nnode = Node('hello_ros_node')\n# 3. From this point, the node is running.\n# You can add your logic here.\nnode.get_logger().info('Hello, ROS 2 World!')\n# 4. The node is destroyed when the 'with' statement exits.\n# This is important for cleanup.\nnode.destroy_node()\n# 5. Shutdown the rclpy library\nrclpy.shutdown()\nif __name__ == '__main__':\nmain()\nTo run this code:\n- Save it as\nhello_node.py\n. - Make sure you have a ROS 2 environment sourced.\n- Run\npython hello_node.py\n.\n2. ROS 2 Topics (Publish/Subscribe)\nTopics are the primary mechanism for one-way, asynchronous communication in ROS 2. Nodes can publish messages to a topic, and other nodes can subscribe to that topic to receive those messages. This is a decoupled communication model; publishers and subscribers don't need to know about each other.\n- Use Case: Continuously streaming data, like sensor readings or motor commands.\nDiagram: Publisher-Subscriber Model\ngraph TD\nA[Publisher Node] -- Publishes message --> B(Topic: /sensor_data);\nB -- Message is delivered --> C[Subscriber Node 1];\nB -- Message is delivered --> D[Subscriber Node 2];\nExample: A Simple Publisher and Subscriber\nFirst, we define a publisher node that sends a simple string message every second.\n# 'publisher_node.py'\nimport rclpy\nfrom rclpy.node import Node\nfrom std_msgs.msg import String\nclass SimplePublisher(Node):\ndef __init__(self):\nsuper().__init__('simple_publisher')\n# Create a publisher on the 'chatter' topic\nself.publisher_ = self.create_publisher(String, 'chatter', 10)\nself.timer = self.create_timer(1.0, self.timer_callback)\nself.i = 0\ndef timer_callback(self):\nmsg = String()\nmsg.data = f'Hello from publisher: {self.i}'\nself.publisher_.publish(msg)\nself.get_logger().info(f'Publishing: \"{msg.data}\"')\nself.i += 1\ndef main(args=None):\nrclpy.init(args=args)\npublisher = SimplePublisher()\nrclpy.spin(publisher) # Keep the node alive\npublisher.destroy_node()\nrclpy.shutdown()\nif __name__ == '__main__':\nmain()\nNext, a subscriber node that listens to the chatter\ntopic.\n# 'subscriber_node.py'\nimport rclpy\nfrom rclpy.node import Node\nfrom std_msgs.msg import String\nclass SimpleSubscriber(Node):\ndef __init__(self):\nsuper().__init__('simple_subscriber')\n# Create a subscription to the 'chatter' topic\nself.subscription = self.create_subscription(\nString,\n'chatter',\nself.listener_callback,\n10)\nself.subscription # prevent unused variable warning\ndef listener_callback(self, msg):\nself.get_logger().info(f'I heard: \"{msg.data}\"')\ndef main(args=None):\nrclpy.init(args=args)\nsubscriber = SimpleSubscriber()\nrclpy.spin(subscriber)\nsubscriber.destroy_node()\nrclpy.shutdown()\nif __name__ == '__main__':\nmain()\nTo run this example:\n- Open two terminals with your ROS 2 environment sourced.\n- In the first terminal, run\npython publisher_node.py\n. - In the second terminal, run\npython subscriber_node.py\n. You will see the messages from the publisher being received.\n3. ROS 2 Services (Request/Response)\nServices are for two-way, synchronous communication. A Service Server node provides a service, and a Service Client node can send a request and wait for a response. This is a tightly coupled, blocking communication model.\n- Use Case: Triggering a specific action that has a clear start and end, like \"open gripper\" or \"calculate inverse kinematics\".\nDiagram: Service Client/Server Model\ngraph TD\nA[Service Client Node] -- Sends Request --> B(Service: /add_two_ints);\nB -- Delivers Request --> C[Service Server Node];\nC -- Processes and sends Response --> B;\nB -- Delivers Response --> A;\nExample: A Simple Service Server and Client\nFirst, the server node that offers a service to add two integers. ROS 2 has a standard service definition for this, example_interfaces/srv/AddTwoInts\n.\n# 'service_server_node.py'\nimport rclpy\nfrom rclpy.node import Node\nfrom example_interfaces.srv import AddTwoInts\nclass SimpleServiceServer(Node):\ndef __init__(self):\nsuper().__init__('simple_service_server')\n# Create a service named 'add_two_ints'\nself.srv = self.create_service(AddTwoInts, 'add_two_ints', self.add_two_ints_callback)\ndef add_two_ints_callback(self, request, response):\nresponse.sum = request.a + request.b\nself.get_logger().info(f'Incoming request: a={request.a}, b={request.b}. Returning sum={response.sum}')\nreturn response\ndef main(args=None):\nrclpy.init(args=args)\nserver = SimpleServiceServer()\nrclpy.spin(server)\nserver.destroy_node()\nrclpy.shutdown()\nif __name__ == '__main__':\nmain()\nNext, the client node that calls the service.\n# 'service_client_node.py'\nimport rclpy\nfrom rclpy.node import Node\nfrom example_interfaces.srv import AddTwoInts\nimport sys\nclass SimpleServiceClient(Node):\ndef __init__(self):\nsuper().__init__('simple_service_client')\n# Create a client for the 'add_two_ints' service\nself.client = self.create_client(AddTwoInts, 'add_two_ints')\nwhile not self.client.wait_for_service(timeout_sec=1.0):\nself.get_logger().info('Service not available, waiting again...')\nself.req = AddTwoInts.Request()\ndef send_request(self, a, b):\nself.req.a = a\nself.req.b = b\nself.future = self.client.call_async(self.req)\nrclpy.spin_until_future_complete(self, self.future)\nreturn self.future.result()\ndef main(args=None):\nrclpy.init(args=args)\nif len(sys.argv) != 3:\nprint(\"Usage: python service_client_node.py <int1> <int2>\")\nreturn\nclient = SimpleServiceClient()\nresponse = client.send_request(int(sys.argv[1]), int(sys.argv[2]))\nif response:\nclient.get_logger().info(\nf'Result of add_two_ints: for {sys.argv[1]} + {sys.argv[2]} = {response.sum}')\nelse:\nclient.get_logger().error('Service call failed')\nclient.destroy_node()\nrclpy.shutdown()\nif __name__ == '__main__':\nmain()\nTo run this example:\n- Open two terminals with your ROS 2 environment sourced.\n- In the first, run\npython service_server_node.py\n. - In the second, run\npython service_client_node.py 5 10\n. The client will send the request and print the response.\n--------------------------------------------------------------------------------"
  },
  {
    "url": "https://Muham251.github.io/ai-hackathon-1/docs/module-1/nodes-topics-services-rclpy#__docusaurus_skipToContent_fallback",
    "text": "ROS 2 Fundamentals & Python\nThis chapter covers the three fundamental communication concepts in ROS 2: Nodes, Topics, and Services. We'll also see how to implement them using rclpy\n, the official Python client library for ROS 2.\n1. ROS 2 Nodes\nA Node is the smallest, most fundamental unit of a ROS 2 system. Think of a node as a single, independent process in your robot's software architecture. Each node should have a single, well-defined purpose.\nFor a humanoid robot, you might have nodes for:\n- Reading sensor data from an IMU.\n- Controlling the motors in the right leg.\n- Processing camera images.\n- Planning footsteps for walking.\nNodes are what allow ROS 2 systems to be modular and fault-tolerant. If one node crashes, the rest of the system can continue to run.\nExample: A Simple rclpy\nNode\nHere is the \"hello world\" of rclpy\n: a simple node that initializes itself, prints a message, and then shuts down.\n# 'hello_node.py'\nimport rclpy\nfrom rclpy.node import Node\ndef main(args=None):\n# 1. Initialize the rclpy library\nrclpy.init(args=args)\n# 2. Create a Node\n# The node is named 'hello_ros_node'\nnode = Node('hello_ros_node')\n# 3. From this point, the node is running.\n# You can add your logic here.\nnode.get_logger().info('Hello, ROS 2 World!')\n# 4. The node is destroyed when the 'with' statement exits.\n# This is important for cleanup.\nnode.destroy_node()\n# 5. Shutdown the rclpy library\nrclpy.shutdown()\nif __name__ == '__main__':\nmain()\nTo run this code:\n- Save it as\nhello_node.py\n. - Make sure you have a ROS 2 environment sourced.\n- Run\npython hello_node.py\n.\n2. ROS 2 Topics (Publish/Subscribe)\nTopics are the primary mechanism for one-way, asynchronous communication in ROS 2. Nodes can publish messages to a topic, and other nodes can subscribe to that topic to receive those messages. This is a decoupled communication model; publishers and subscribers don't need to know about each other.\n- Use Case: Continuously streaming data, like sensor readings or motor commands.\nDiagram: Publisher-Subscriber Model\ngraph TD\nA[Publisher Node] -- Publishes message --> B(Topic: /sensor_data);\nB -- Message is delivered --> C[Subscriber Node 1];\nB -- Message is delivered --> D[Subscriber Node 2];\nExample: A Simple Publisher and Subscriber\nFirst, we define a publisher node that sends a simple string message every second.\n# 'publisher_node.py'\nimport rclpy\nfrom rclpy.node import Node\nfrom std_msgs.msg import String\nclass SimplePublisher(Node):\ndef __init__(self):\nsuper().__init__('simple_publisher')\n# Create a publisher on the 'chatter' topic\nself.publisher_ = self.create_publisher(String, 'chatter', 10)\nself.timer = self.create_timer(1.0, self.timer_callback)\nself.i = 0\ndef timer_callback(self):\nmsg = String()\nmsg.data = f'Hello from publisher: {self.i}'\nself.publisher_.publish(msg)\nself.get_logger().info(f'Publishing: \"{msg.data}\"')\nself.i += 1\ndef main(args=None):\nrclpy.init(args=args)\npublisher = SimplePublisher()\nrclpy.spin(publisher) # Keep the node alive\npublisher.destroy_node()\nrclpy.shutdown()\nif __name__ == '__main__':\nmain()\nNext, a subscriber node that listens to the chatter\ntopic.\n# 'subscriber_node.py'\nimport rclpy\nfrom rclpy.node import Node\nfrom std_msgs.msg import String\nclass SimpleSubscriber(Node):\ndef __init__(self):\nsuper().__init__('simple_subscriber')\n# Create a subscription to the 'chatter' topic\nself.subscription = self.create_subscription(\nString,\n'chatter',\nself.listener_callback,\n10)\nself.subscription # prevent unused variable warning\ndef listener_callback(self, msg):\nself.get_logger().info(f'I heard: \"{msg.data}\"')\ndef main(args=None):\nrclpy.init(args=args)\nsubscriber = SimpleSubscriber()\nrclpy.spin(subscriber)\nsubscriber.destroy_node()\nrclpy.shutdown()\nif __name__ == '__main__':\nmain()\nTo run this example:\n- Open two terminals with your ROS 2 environment sourced.\n- In the first terminal, run\npython publisher_node.py\n. - In the second terminal, run\npython subscriber_node.py\n. You will see the messages from the publisher being received.\n3. ROS 2 Services (Request/Response)\nServices are for two-way, synchronous communication. A Service Server node provides a service, and a Service Client node can send a request and wait for a response. This is a tightly coupled, blocking communication model.\n- Use Case: Triggering a specific action that has a clear start and end, like \"open gripper\" or \"calculate inverse kinematics\".\nDiagram: Service Client/Server Model\ngraph TD\nA[Service Client Node] -- Sends Request --> B(Service: /add_two_ints);\nB -- Delivers Request --> C[Service Server Node];\nC -- Processes and sends Response --> B;\nB -- Delivers Response --> A;\nExample: A Simple Service Server and Client\nFirst, the server node that offers a service to add two integers. ROS 2 has a standard service definition for this, example_interfaces/srv/AddTwoInts\n.\n# 'service_server_node.py'\nimport rclpy\nfrom rclpy.node import Node\nfrom example_interfaces.srv import AddTwoInts\nclass SimpleServiceServer(Node):\ndef __init__(self):\nsuper().__init__('simple_service_server')\n# Create a service named 'add_two_ints'\nself.srv = self.create_service(AddTwoInts, 'add_two_ints', self.add_two_ints_callback)\ndef add_two_ints_callback(self, request, response):\nresponse.sum = request.a + request.b\nself.get_logger().info(f'Incoming request: a={request.a}, b={request.b}. Returning sum={response.sum}')\nreturn response\ndef main(args=None):\nrclpy.init(args=args)\nserver = SimpleServiceServer()\nrclpy.spin(server)\nserver.destroy_node()\nrclpy.shutdown()\nif __name__ == '__main__':\nmain()\nNext, the client node that calls the service.\n# 'service_client_node.py'\nimport rclpy\nfrom rclpy.node import Node\nfrom example_interfaces.srv import AddTwoInts\nimport sys\nclass SimpleServiceClient(Node):\ndef __init__(self):\nsuper().__init__('simple_service_client')\n# Create a client for the 'add_two_ints' service\nself.client = self.create_client(AddTwoInts, 'add_two_ints')\nwhile not self.client.wait_for_service(timeout_sec=1.0):\nself.get_logger().info('Service not available, waiting again...')\nself.req = AddTwoInts.Request()\ndef send_request(self, a, b):\nself.req.a = a\nself.req.b = b\nself.future = self.client.call_async(self.req)\nrclpy.spin_until_future_complete(self, self.future)\nreturn self.future.result()\ndef main(args=None):\nrclpy.init(args=args)\nif len(sys.argv) != 3:\nprint(\"Usage: python service_client_node.py <int1> <int2>\")\nreturn\nclient = SimpleServiceClient()\nresponse = client.send_request(int(sys.argv[1]), int(sys.argv[2]))\nif response:\nclient.get_logger().info(\nf'Result of add_two_ints: for {sys.argv[1]} + {sys.argv[2]} = {response.sum}')\nelse:\nclient.get_logger().error('Service call failed')\nclient.destroy_node()\nrclpy.shutdown()\nif __name__ == '__main__':\nmain()\nTo run this example:\n- Open two terminals with your ROS 2 environment sourced.\n- In the first, run\npython service_server_node.py\n. - In the second, run\npython service_client_node.py 5 10\n. The client will send the request and print the response.\n--------------------------------------------------------------------------------"
  },
  {
    "url": "https://Muham251.github.io/ai-hackathon-1/docs/module-1/ros-2-basics-for-humanoid-control",
    "text": "Module 1: The Robotic Nervous System (ROS 2)\nWelcome to Module 1! This module introduces the Robot Operating System (ROS 2), the fundamental software framework that acts as the nervous system for modern robots, including humanoids. As we journey into controlling complex humanoid robots, understanding ROS 2 is the essential first step.\nTarget Audience\nThis module is designed for students and developers who are new to robotics but have some programming experience, particularly in Python. If you're aiming to understand how to make a humanoid robot walk, talk, or interact with its environment, you're in the right place.\nWhat is ROS 2?\nROS 2 is a flexible framework for writing robot software. It is a set of software libraries and tools that help you build robot applications. Think of it as the middleware that handles all the complex communication between different parts of your robot's software. From low-level motor control to high-level planning and perception, ROS 2 provides the tools to manage it all.\nFor a humanoid robot, this is crucial. A humanoid may have dozens of motors (actuators), a suite of sensors (cameras, IMUs, force sensors), and multiple computers running different software modules. ROS 2 allows these components to communicate with each other in a standardized way.\nLearning Objectives\nBy the end of this module, you will be able to:\n- Explain the core components of the ROS 2 architecture.\n- Create and run ROS 2 nodes, topics, and services.\n- Write a simple Python agent that communicates with a ROS 2 system.\n- Understand and write a basic URDF file for a humanoid robot model.\nModule Chapters\nThis module is divided into the following chapters:\n- ROS 2 Fundamentals (Nodes, Topics, Services): A deep dive into the core communication patterns in ROS 2.\n- URDF Basics for Humanoid Models: An introduction to the Unified Robot Description Format (URDF) for modeling your humanoid.\nLet's get started!\n--------------------------------------------------------------------------------"
  },
  {
    "url": "https://Muham251.github.io/ai-hackathon-1/docs/module-1/ros-2-basics-for-humanoid-control#__docusaurus_skipToContent_fallback",
    "text": "Module 1: The Robotic Nervous System (ROS 2)\nWelcome to Module 1! This module introduces the Robot Operating System (ROS 2), the fundamental software framework that acts as the nervous system for modern robots, including humanoids. As we journey into controlling complex humanoid robots, understanding ROS 2 is the essential first step.\nTarget Audience\nThis module is designed for students and developers who are new to robotics but have some programming experience, particularly in Python. If you're aiming to understand how to make a humanoid robot walk, talk, or interact with its environment, you're in the right place.\nWhat is ROS 2?\nROS 2 is a flexible framework for writing robot software. It is a set of software libraries and tools that help you build robot applications. Think of it as the middleware that handles all the complex communication between different parts of your robot's software. From low-level motor control to high-level planning and perception, ROS 2 provides the tools to manage it all.\nFor a humanoid robot, this is crucial. A humanoid may have dozens of motors (actuators), a suite of sensors (cameras, IMUs, force sensors), and multiple computers running different software modules. ROS 2 allows these components to communicate with each other in a standardized way.\nLearning Objectives\nBy the end of this module, you will be able to:\n- Explain the core components of the ROS 2 architecture.\n- Create and run ROS 2 nodes, topics, and services.\n- Write a simple Python agent that communicates with a ROS 2 system.\n- Understand and write a basic URDF file for a humanoid robot model.\nModule Chapters\nThis module is divided into the following chapters:\n- ROS 2 Fundamentals (Nodes, Topics, Services): A deep dive into the core communication patterns in ROS 2.\n- URDF Basics for Humanoid Models: An introduction to the Unified Robot Description Format (URDF) for modeling your humanoid.\nLet's get started!\n--------------------------------------------------------------------------------"
  },
  {
    "url": "https://Muham251.github.io/ai-hackathon-1/docs/module-1/urdf-essentials",
    "text": "URDF Basics for Humanoid Models\nThe Unified Robot Description Format (URDF) is an XML format used in ROS to describe all the physical elements of a robot model. For a humanoid, the URDF file is its digital blueprint. It defines the robot's body parts, how they are connected, and their physical properties.\nA URDF file describes the robot as a tree of links connected by joints.\n<link>\n: A physical part of the robot (e.g., a forearm, a foot, a torso).<joint>\n: The connection between two links. It defines how one link moves relative to another (e.g., rotating like an elbow, sliding, or being fixed).\nKey URDF Tags\nThe <robot>\nTag\nEvery URDF file must start and end with the <robot>\ntag. It's the root element, and you give your robot a name here.\n<robot name=\"simple_humanoid\">\n... all your links and joints go here ...\n</robot>\nThe <link>\nTag\nEach link has a name and can contain three important sub-tags:\n<visual>\n: Defines the appearance of the link (shape, size, color, texture). This is what you see in simulation tools like RViz.<collision>\n: Defines the collision geometry of the link. This is what the physics engine uses to calculate collisions with other objects. It's often a simpler shape than the visual one for performance reasons.<inertial>\n: Defines the dynamic properties of the link: its mass, center of mass, and inertia matrix. This is crucial for realistic physics simulation.\n<link name=\"torso\">\n<visual>\n<geometry>\n<box size=\"0.2 0.3 0.5\"/>\n</geometry>\n<material name=\"grey\">\n<color rgba=\"0.5 0.5 0.5 1\"/>\n</material>\n</visual>\n<collision>\n<geometry>\n<box size=\"0.2 0.3 0.5\"/>\n</geometry>\n</collision>\n<inertial>\n<mass value=\"10\"/>\n<inertia ixx=\"0.4\" ixy=\"0.0\" ixz=\"0.0\" iyy=\"0.4\" iyz=\"0.0\" izz=\"0.2\"/>\n</inertial>\n</link>\nThe <joint>\nTag\nEach joint connects two links, called the parent and the child. It also defines the motion allowed between them.\nname\n: The name of the joint (e.g., \"left_shoulder_joint\").type\n: The type of motion. Common types are:revolute\n: Rotates around an axis (like an elbow). Requires<limit>\ntags for upper and lower angles.continuous\n: Rotates around an axis with no angle limits.prismatic\n: Slides along an axis (like a piston).fixed\n: No motion is allowed between the two links.\n<parent link=\"...\" />\n: The name of the parent link.<child link=\"...\" />\n: The name of the child link.<origin xyz=\"...\" rpy=\"...\" />\n: The transform (position and orientation) from the parent link's origin to the joint's origin.<axis xyz=\"...\" />\n: The axis of rotation or translation forrevolute\nandprismatic\njoints.\n<joint name=\"neck_joint\" type=\"revolute\">\n<parent link=\"torso\"/>\n<child link=\"head\"/>\n<origin xyz=\"0 0 0.25\" rpy=\"0 0 0\"/>\n<axis xyz=\"0 0 1\"/>\n<limit lower=\"-0.5\" upper=\"0.5\" effort=\"10\" velocity=\"1.0\"/>\n</joint>\nSimple Humanoid URDF Example\nThis example describes a very simple humanoid robot. It has a torso, a head, and a single arm with a shoulder and elbow joint. This demonstrates the parent-child tree structure. The \"torso\" is the root link of our tree.\n<!-- simple_humanoid.urdf -->\n<robot name=\"simple_humanoid\">\n<!-- A. Materials (for color) -->\n<material name=\"blue\">\n<color rgba=\"0.0 0.0 0.8 1.0\"/>\n</material>\n<material name=\"white\">\n<color rgba=\"1.0 1.0 1.0 1.0\"/>\n</material>\n<!-- B. Base Link (Torso) -->\n<link name=\"torso\">\n<visual>\n<geometry><box size=\"0.2 0.3 0.5\"/></geometry>\n<origin xyz=\"0 0 0\" rpy=\"0 0 0\"/>\n<material name=\"blue\"/>\n</visual>\n<collision><geometry><box size=\"0.2 0.3 0.5\"/></geometry></collision>\n<inertial>\n<mass value=\"10\"/>\n<inertia ixx=\"1.0\" ixy=\"0\" ixz=\"0\" iyy=\"1.0\" iyz=\"0\" izz=\"1.0\"/>\n</inertial>\n</link>\n<!-- C. Head Link -->\n<link name=\"head\">\n<visual>\n<geometry><sphere radius=\"0.1\"/></geometry>\n<origin xyz=\"0 0 0\" rpy=\"0 0 0\"/>\n<material name=\"white\"/>\n</visual>\n<collision><geometry><sphere radius=\"0.1\"/></geometry></collision>\n<inertial>\n<mass value=\"1\"/>\n<inertia ixx=\"0.1\" ixy=\"0\" ixz=\"0\" iyy=\"0.1\" iyz=\"0\" izz=\"0.1\"/>\n</inertial>\n</link>\n<!-- D. Neck Joint (Connects Torso to Head) -->\n<joint name=\"neck_joint\" type=\"revolute\">\n<parent link=\"torso\"/>\n<child link=\"head\"/>\n<origin xyz=\"0 0 0.30\" rpy=\"0 0 0\"/> <!-- Position of head relative to torso -->\n<axis xyz=\"0 0 1\"/>\n<limit lower=\"-0.7\" upper=\"0.7\" effort=\"5\" velocity=\"0.5\"/>\n</joint>\n<!-- E. Right Arm -->\n<link name=\"right_upper_arm\">\n<visual>\n<geometry><cylinder length=\"0.3\" radius=\"0.05\"/></geometry>\n<origin xyz=\"0 0 -0.15\" rpy=\"0 0 0\"/>\n<material name=\"white\"/>\n</visual>\n<collision><geometry><cylinder length=\"0.3\" radius=\"0.05\"/></geometry></collision>\n<inertial>\n<mass value=\"2\"/>\n<inertia ixx=\"0.1\" ixy=\"0\" ixz=\"0\" iyy=\"0.1\" iyz=\"0\" izz=\"0.1\"/>\n</inertial>\n</link>\n<!-- F. Right Shoulder Joint -->\n<joint name=\"right_shoulder_joint\" type=\"revolute\">\n<parent link=\"torso\"/>\n<child link=\"right_upper_arm\"/>\n<origin xyz=\"0 -0.20 0.15\" rpy=\"1.5707 0 0\"/> <!-- Position of arm relative to torso -->\n<axis xyz=\"0 0 1\"/>\n<limit lower=\"-1.57\" upper=\"1.57\" effort=\"10\" velocity=\"1.0\"/>\n</joint>\n</robot>\nHow to Use This URDF\n- Save the code above as\nsimple_humanoid.urdf\n. - You can check its validity using the\ncheck_urdf\ncommand:check_urdf simple_humanoid.urdf\n. - To visualize it, you can use ROS tools like RViz. This typically requires a small launch file to publish the robot's state, which is a topic for a later module.\nThis simple example provides the foundation. A full humanoid model would extend this tree with more links and joints for the legs, a more complex arm, and hands.\n--------------------------------------------------------------------------------"
  },
  {
    "url": "https://Muham251.github.io/ai-hackathon-1/docs/module-1/urdf-essentials#__docusaurus_skipToContent_fallback",
    "text": "URDF Basics for Humanoid Models\nThe Unified Robot Description Format (URDF) is an XML format used in ROS to describe all the physical elements of a robot model. For a humanoid, the URDF file is its digital blueprint. It defines the robot's body parts, how they are connected, and their physical properties.\nA URDF file describes the robot as a tree of links connected by joints.\n<link>\n: A physical part of the robot (e.g., a forearm, a foot, a torso).<joint>\n: The connection between two links. It defines how one link moves relative to another (e.g., rotating like an elbow, sliding, or being fixed).\nKey URDF Tags\nThe <robot>\nTag\nEvery URDF file must start and end with the <robot>\ntag. It's the root element, and you give your robot a name here.\n<robot name=\"simple_humanoid\">\n... all your links and joints go here ...\n</robot>\nThe <link>\nTag\nEach link has a name and can contain three important sub-tags:\n<visual>\n: Defines the appearance of the link (shape, size, color, texture). This is what you see in simulation tools like RViz.<collision>\n: Defines the collision geometry of the link. This is what the physics engine uses to calculate collisions with other objects. It's often a simpler shape than the visual one for performance reasons.<inertial>\n: Defines the dynamic properties of the link: its mass, center of mass, and inertia matrix. This is crucial for realistic physics simulation.\n<link name=\"torso\">\n<visual>\n<geometry>\n<box size=\"0.2 0.3 0.5\"/>\n</geometry>\n<material name=\"grey\">\n<color rgba=\"0.5 0.5 0.5 1\"/>\n</material>\n</visual>\n<collision>\n<geometry>\n<box size=\"0.2 0.3 0.5\"/>\n</geometry>\n</collision>\n<inertial>\n<mass value=\"10\"/>\n<inertia ixx=\"0.4\" ixy=\"0.0\" ixz=\"0.0\" iyy=\"0.4\" iyz=\"0.0\" izz=\"0.2\"/>\n</inertial>\n</link>\nThe <joint>\nTag\nEach joint connects two links, called the parent and the child. It also defines the motion allowed between them.\nname\n: The name of the joint (e.g., \"left_shoulder_joint\").type\n: The type of motion. Common types are:revolute\n: Rotates around an axis (like an elbow). Requires<limit>\ntags for upper and lower angles.continuous\n: Rotates around an axis with no angle limits.prismatic\n: Slides along an axis (like a piston).fixed\n: No motion is allowed between the two links.\n<parent link=\"...\" />\n: The name of the parent link.<child link=\"...\" />\n: The name of the child link.<origin xyz=\"...\" rpy=\"...\" />\n: The transform (position and orientation) from the parent link's origin to the joint's origin.<axis xyz=\"...\" />\n: The axis of rotation or translation forrevolute\nandprismatic\njoints.\n<joint name=\"neck_joint\" type=\"revolute\">\n<parent link=\"torso\"/>\n<child link=\"head\"/>\n<origin xyz=\"0 0 0.25\" rpy=\"0 0 0\"/>\n<axis xyz=\"0 0 1\"/>\n<limit lower=\"-0.5\" upper=\"0.5\" effort=\"10\" velocity=\"1.0\"/>\n</joint>\nSimple Humanoid URDF Example\nThis example describes a very simple humanoid robot. It has a torso, a head, and a single arm with a shoulder and elbow joint. This demonstrates the parent-child tree structure. The \"torso\" is the root link of our tree.\n<!-- simple_humanoid.urdf -->\n<robot name=\"simple_humanoid\">\n<!-- A. Materials (for color) -->\n<material name=\"blue\">\n<color rgba=\"0.0 0.0 0.8 1.0\"/>\n</material>\n<material name=\"white\">\n<color rgba=\"1.0 1.0 1.0 1.0\"/>\n</material>\n<!-- B. Base Link (Torso) -->\n<link name=\"torso\">\n<visual>\n<geometry><box size=\"0.2 0.3 0.5\"/></geometry>\n<origin xyz=\"0 0 0\" rpy=\"0 0 0\"/>\n<material name=\"blue\"/>\n</visual>\n<collision><geometry><box size=\"0.2 0.3 0.5\"/></geometry></collision>\n<inertial>\n<mass value=\"10\"/>\n<inertia ixx=\"1.0\" ixy=\"0\" ixz=\"0\" iyy=\"1.0\" iyz=\"0\" izz=\"1.0\"/>\n</inertial>\n</link>\n<!-- C. Head Link -->\n<link name=\"head\">\n<visual>\n<geometry><sphere radius=\"0.1\"/></geometry>\n<origin xyz=\"0 0 0\" rpy=\"0 0 0\"/>\n<material name=\"white\"/>\n</visual>\n<collision><geometry><sphere radius=\"0.1\"/></geometry></collision>\n<inertial>\n<mass value=\"1\"/>\n<inertia ixx=\"0.1\" ixy=\"0\" ixz=\"0\" iyy=\"0.1\" iyz=\"0\" izz=\"0.1\"/>\n</inertial>\n</link>\n<!-- D. Neck Joint (Connects Torso to Head) -->\n<joint name=\"neck_joint\" type=\"revolute\">\n<parent link=\"torso\"/>\n<child link=\"head\"/>\n<origin xyz=\"0 0 0.30\" rpy=\"0 0 0\"/> <!-- Position of head relative to torso -->\n<axis xyz=\"0 0 1\"/>\n<limit lower=\"-0.7\" upper=\"0.7\" effort=\"5\" velocity=\"0.5\"/>\n</joint>\n<!-- E. Right Arm -->\n<link name=\"right_upper_arm\">\n<visual>\n<geometry><cylinder length=\"0.3\" radius=\"0.05\"/></geometry>\n<origin xyz=\"0 0 -0.15\" rpy=\"0 0 0\"/>\n<material name=\"white\"/>\n</visual>\n<collision><geometry><cylinder length=\"0.3\" radius=\"0.05\"/></geometry></collision>\n<inertial>\n<mass value=\"2\"/>\n<inertia ixx=\"0.1\" ixy=\"0\" ixz=\"0\" iyy=\"0.1\" iyz=\"0\" izz=\"0.1\"/>\n</inertial>\n</link>\n<!-- F. Right Shoulder Joint -->\n<joint name=\"right_shoulder_joint\" type=\"revolute\">\n<parent link=\"torso\"/>\n<child link=\"right_upper_arm\"/>\n<origin xyz=\"0 -0.20 0.15\" rpy=\"1.5707 0 0\"/> <!-- Position of arm relative to torso -->\n<axis xyz=\"0 0 1\"/>\n<limit lower=\"-1.57\" upper=\"1.57\" effort=\"10\" velocity=\"1.0\"/>\n</joint>\n</robot>\nHow to Use This URDF\n- Save the code above as\nsimple_humanoid.urdf\n. - You can check its validity using the\ncheck_urdf\ncommand:check_urdf simple_humanoid.urdf\n. - To visualize it, you can use ROS tools like RViz. This typically requires a small launch file to publish the robot's state, which is a topic for a later module.\nThis simple example provides the foundation. A full humanoid model would extend this tree with more links and joints for the legs, a more complex arm, and hands.\n--------------------------------------------------------------------------------"
  },
  {
    "url": "https://Muham251.github.io/ai-hackathon-1/docs/category/tutorial---basics",
    "text": "\ud83d\udcc4\ufe0f Create a Page\nAdd Markdown or React files to src/pages to create a standalone page:\n\ud83d\udcc4\ufe0f Create a Document\nDocuments are groups of pages connected through:\n\ud83d\udcc4\ufe0f Create a Blog Post\nDocusaurus creates a page for each blog post, but also a blog index page, a tag system, an RSS feed...\n\ud83d\udcc4\ufe0f Markdown Features\nDocusaurus supports Markdown and a few additional features.\n\ud83d\udcc4\ufe0f Deploy your site\nDocusaurus is a static-site-generator (also called Jamstack).\n\ud83d\udcc4\ufe0f Congratulations!\nYou have just learned the basics of Docusaurus and made some changes to the initial template.\n--------------------------------------------------------------------------------"
  },
  {
    "url": "https://Muham251.github.io/ai-hackathon-1/docs/category/tutorial---basics#__docusaurus_skipToContent_fallback",
    "text": "\ud83d\udcc4\ufe0f Create a Page\nAdd Markdown or React files to src/pages to create a standalone page:\n\ud83d\udcc4\ufe0f Create a Document\nDocuments are groups of pages connected through:\n\ud83d\udcc4\ufe0f Create a Blog Post\nDocusaurus creates a page for each blog post, but also a blog index page, a tag system, an RSS feed...\n\ud83d\udcc4\ufe0f Markdown Features\nDocusaurus supports Markdown and a few additional features.\n\ud83d\udcc4\ufe0f Deploy your site\nDocusaurus is a static-site-generator (also called Jamstack).\n\ud83d\udcc4\ufe0f Congratulations!\nYou have just learned the basics of Docusaurus and made some changes to the initial template.\n--------------------------------------------------------------------------------"
  },
  {
    "url": "https://Muham251.github.io/ai-hackathon-1/docs/tutorial-basics/create-a-page",
    "text": "Create a Page\nAdd Markdown or React files to src/pages\nto create a standalone page:\nsrc/pages/index.js\n\u2192localhost:3000/\nsrc/pages/foo.md\n\u2192localhost:3000/foo\nsrc/pages/foo/bar.js\n\u2192localhost:3000/foo/bar\nCreate your first React Page\nCreate a file at src/pages/my-react-page.js\n:\nsrc/pages/my-react-page.js\nimport React from 'react';\nimport Layout from '@theme/Layout';\nexport default function MyReactPage() {\nreturn (\n<Layout>\n<h1>My React page</h1>\n<p>This is a React page</p>\n</Layout>\n);\n}\nA new page is now available at http://localhost:3000/my-react-page.\nCreate your first Markdown Page\nCreate a file at src/pages/my-markdown-page.md\n:\nsrc/pages/my-markdown-page.md\n# My Markdown page\nThis is a Markdown page\nA new page is now available at http://localhost:3000/my-markdown-page.\n--------------------------------------------------------------------------------"
  },
  {
    "url": "https://Muham251.github.io/ai-hackathon-1/docs/tutorial-basics/create-a-page#__docusaurus_skipToContent_fallback",
    "text": "Create a Page\nAdd Markdown or React files to src/pages\nto create a standalone page:\nsrc/pages/index.js\n\u2192localhost:3000/\nsrc/pages/foo.md\n\u2192localhost:3000/foo\nsrc/pages/foo/bar.js\n\u2192localhost:3000/foo/bar\nCreate your first React Page\nCreate a file at src/pages/my-react-page.js\n:\nsrc/pages/my-react-page.js\nimport React from 'react';\nimport Layout from '@theme/Layout';\nexport default function MyReactPage() {\nreturn (\n<Layout>\n<h1>My React page</h1>\n<p>This is a React page</p>\n</Layout>\n);\n}\nA new page is now available at http://localhost:3000/my-react-page.\nCreate your first Markdown Page\nCreate a file at src/pages/my-markdown-page.md\n:\nsrc/pages/my-markdown-page.md\n# My Markdown page\nThis is a Markdown page\nA new page is now available at http://localhost:3000/my-markdown-page.\n--------------------------------------------------------------------------------"
  },
  {
    "url": "https://Muham251.github.io/ai-hackathon-1/docs/tutorial-basics/create-a-document",
    "text": "Create a Document\nDocuments are groups of pages connected through:\n- a sidebar\n- previous/next navigation\n- versioning\nCreate your first Doc\nCreate a Markdown file at docs/hello.md\n:\ndocs/hello.md\n# Hello\nThis is my **first Docusaurus document**!\nA new document is now available at http://localhost:3000/docs/hello.\nConfigure the Sidebar\nDocusaurus automatically creates a sidebar from the docs\nfolder.\nAdd metadata to customize the sidebar label and position:\ndocs/hello.md\n---\nsidebar_label: 'Hi!'\nsidebar_position: 3\n---\n# Hello\nThis is my **first Docusaurus document**!\nIt is also possible to create your sidebar explicitly in sidebars.js\n:\nsidebars.js\nexport default {\ntutorialSidebar: [\n'intro',\n'hello',\n{\ntype: 'category',\nlabel: 'Tutorial',\nitems: ['tutorial-basics/create-a-document'],\n},\n],\n};\n--------------------------------------------------------------------------------"
  },
  {
    "url": "https://Muham251.github.io/ai-hackathon-1/docs/tutorial-basics/create-a-document#__docusaurus_skipToContent_fallback",
    "text": "Create a Document\nDocuments are groups of pages connected through:\n- a sidebar\n- previous/next navigation\n- versioning\nCreate your first Doc\nCreate a Markdown file at docs/hello.md\n:\ndocs/hello.md\n# Hello\nThis is my **first Docusaurus document**!\nA new document is now available at http://localhost:3000/docs/hello.\nConfigure the Sidebar\nDocusaurus automatically creates a sidebar from the docs\nfolder.\nAdd metadata to customize the sidebar label and position:\ndocs/hello.md\n---\nsidebar_label: 'Hi!'\nsidebar_position: 3\n---\n# Hello\nThis is my **first Docusaurus document**!\nIt is also possible to create your sidebar explicitly in sidebars.js\n:\nsidebars.js\nexport default {\ntutorialSidebar: [\n'intro',\n'hello',\n{\ntype: 'category',\nlabel: 'Tutorial',\nitems: ['tutorial-basics/create-a-document'],\n},\n],\n};\n--------------------------------------------------------------------------------"
  },
  {
    "url": "https://Muham251.github.io/ai-hackathon-1/docs/tutorial-basics/create-a-blog-post",
    "text": "Create a Blog Post\nDocusaurus creates a page for each blog post, but also a blog index page, a tag system, an RSS feed...\nCreate your first Post\nCreate a file at blog/2021-02-28-greetings.md\n:\nblog/2021-02-28-greetings.md\n---\nslug: greetings\ntitle: Greetings!\nauthors:\n- name: Joel Marcey\ntitle: Co-creator of Docusaurus 1\nurl: https://github.com/JoelMarcey\nimage_url: https://github.com/JoelMarcey.png\n- name: S\u00e9bastien Lorber\ntitle: Docusaurus maintainer\nurl: https://sebastienlorber.com\nimage_url: https://github.com/slorber.png\ntags: [greetings]\n---\nCongratulations, you have made your first post!\nFeel free to play around and edit this post as much as you like.\nA new blog post is now available at http://localhost:3000/blog/greetings.\n--------------------------------------------------------------------------------"
  },
  {
    "url": "https://Muham251.github.io/ai-hackathon-1/docs/tutorial-basics/create-a-blog-post#__docusaurus_skipToContent_fallback",
    "text": "Create a Blog Post\nDocusaurus creates a page for each blog post, but also a blog index page, a tag system, an RSS feed...\nCreate your first Post\nCreate a file at blog/2021-02-28-greetings.md\n:\nblog/2021-02-28-greetings.md\n---\nslug: greetings\ntitle: Greetings!\nauthors:\n- name: Joel Marcey\ntitle: Co-creator of Docusaurus 1\nurl: https://github.com/JoelMarcey\nimage_url: https://github.com/JoelMarcey.png\n- name: S\u00e9bastien Lorber\ntitle: Docusaurus maintainer\nurl: https://sebastienlorber.com\nimage_url: https://github.com/slorber.png\ntags: [greetings]\n---\nCongratulations, you have made your first post!\nFeel free to play around and edit this post as much as you like.\nA new blog post is now available at http://localhost:3000/blog/greetings.\n--------------------------------------------------------------------------------"
  },
  {
    "url": "https://Muham251.github.io/ai-hackathon-1/docs/tutorial-basics/markdown-features",
    "text": "Markdown Features\nDocusaurus supports Markdown and a few additional features.\nFront Matter\nMarkdown documents have metadata at the top called Front Matter:\n---\nid: my-doc-id\ntitle: My document title\ndescription: My document description\nslug: /my-custom-url\n---\n## Markdown heading\nMarkdown text with [links](./hello.md)\nLinks\nRegular Markdown links are supported, using url paths or relative file paths.\nLet's see how to [Create a page](/create-a-page).\nLet's see how to [Create a page](./create-a-page.md).\nResult: Let's see how to Create a page.\nImages\nRegular Markdown images are supported.\nYou can use absolute paths to reference images in the static directory (static/img/docusaurus.png\n):\n![Docusaurus logo](/img/docusaurus.png)\nYou can reference images relative to the current file as well. This is particularly useful to colocate images close to the Markdown files using them:\n![Docusaurus logo](./img/docusaurus.png)\nCode Blocks\nMarkdown code blocks are supported with Syntax highlighting.\n```jsx title=\"src/components/HelloDocusaurus.js\"\nfunction HelloDocusaurus() {\nreturn <h1>Hello, Docusaurus!</h1>;\n}\n```\nfunction HelloDocusaurus() {\nreturn <h1>Hello, Docusaurus!</h1>;\n}\nAdmonitions\nDocusaurus has a special syntax to create admonitions and callouts:\n:::tip[My tip]\nUse this awesome feature option\n:::\n:::danger[Take care]\nThis action is dangerous\n:::\nUse this awesome feature option\nThis action is dangerous\nMDX and React Components\nMDX can make your documentation more interactive and allows using any React components inside Markdown:\nexport const Highlight = ({children, color}) => (\n<span\nstyle={{\nbackgroundColor: color,\nborderRadius: '20px',\ncolor: '#fff',\npadding: '10px',\ncursor: 'pointer',\n}}\nonClick={() => {\nalert(`You clicked the color ${color} with label ${children}`)\n}}>\n{children}\n</span>\n);\nThis is <Highlight color=\"#25c2a0\">Docusaurus green</Highlight> !\nThis is <Highlight color=\"#1877F2\">Facebook blue</Highlight> !\nThis is Docusaurus green !\nThis is Facebook blue !\n--------------------------------------------------------------------------------"
  },
  {
    "url": "https://Muham251.github.io/ai-hackathon-1/docs/tutorial-basics/markdown-features#__docusaurus_skipToContent_fallback",
    "text": "Markdown Features\nDocusaurus supports Markdown and a few additional features.\nFront Matter\nMarkdown documents have metadata at the top called Front Matter:\n---\nid: my-doc-id\ntitle: My document title\ndescription: My document description\nslug: /my-custom-url\n---\n## Markdown heading\nMarkdown text with [links](./hello.md)\nLinks\nRegular Markdown links are supported, using url paths or relative file paths.\nLet's see how to [Create a page](/create-a-page).\nLet's see how to [Create a page](./create-a-page.md).\nResult: Let's see how to Create a page.\nImages\nRegular Markdown images are supported.\nYou can use absolute paths to reference images in the static directory (static/img/docusaurus.png\n):\n![Docusaurus logo](/img/docusaurus.png)\nYou can reference images relative to the current file as well. This is particularly useful to colocate images close to the Markdown files using them:\n![Docusaurus logo](./img/docusaurus.png)\nCode Blocks\nMarkdown code blocks are supported with Syntax highlighting.\n```jsx title=\"src/components/HelloDocusaurus.js\"\nfunction HelloDocusaurus() {\nreturn <h1>Hello, Docusaurus!</h1>;\n}\n```\nfunction HelloDocusaurus() {\nreturn <h1>Hello, Docusaurus!</h1>;\n}\nAdmonitions\nDocusaurus has a special syntax to create admonitions and callouts:\n:::tip[My tip]\nUse this awesome feature option\n:::\n:::danger[Take care]\nThis action is dangerous\n:::\nUse this awesome feature option\nThis action is dangerous\nMDX and React Components\nMDX can make your documentation more interactive and allows using any React components inside Markdown:\nexport const Highlight = ({children, color}) => (\n<span\nstyle={{\nbackgroundColor: color,\nborderRadius: '20px',\ncolor: '#fff',\npadding: '10px',\ncursor: 'pointer',\n}}\nonClick={() => {\nalert(`You clicked the color ${color} with label ${children}`)\n}}>\n{children}\n</span>\n);\nThis is <Highlight color=\"#25c2a0\">Docusaurus green</Highlight> !\nThis is <Highlight color=\"#1877F2\">Facebook blue</Highlight> !\nThis is Docusaurus green !\nThis is Facebook blue !\n--------------------------------------------------------------------------------"
  },
  {
    "url": "https://Muham251.github.io/ai-hackathon-1/docs/tutorial-basics/deploy-your-site",
    "text": "Deploy your site\nDocusaurus is a static-site-generator (also called Jamstack).\nIt builds your site as simple static HTML, JavaScript and CSS files.\nBuild your site\nBuild your site for production:\nnpm run build\nThe static files are generated in the build\nfolder.\nDeploy your site\nTest your production build locally:\nnpm run serve\nThe build\nfolder is now served at http://localhost:3000/.\nYou can now deploy the build\nfolder almost anywhere easily, for free or very small cost (read the Deployment Guide).\n--------------------------------------------------------------------------------"
  },
  {
    "url": "https://Muham251.github.io/ai-hackathon-1/docs/tutorial-basics/deploy-your-site#__docusaurus_skipToContent_fallback",
    "text": "Deploy your site\nDocusaurus is a static-site-generator (also called Jamstack).\nIt builds your site as simple static HTML, JavaScript and CSS files.\nBuild your site\nBuild your site for production:\nnpm run build\nThe static files are generated in the build\nfolder.\nDeploy your site\nTest your production build locally:\nnpm run serve\nThe build\nfolder is now served at http://localhost:3000/.\nYou can now deploy the build\nfolder almost anywhere easily, for free or very small cost (read the Deployment Guide).\n--------------------------------------------------------------------------------"
  },
  {
    "url": "https://Muham251.github.io/ai-hackathon-1/docs/tutorial-basics/congratulations",
    "text": "Congratulations!\nYou have just learned the basics of Docusaurus and made some changes to the initial template.\nDocusaurus has much more to offer!\nHave 5 more minutes? Take a look at versioning and i18n.\nAnything unclear or buggy in this tutorial? Please report it!\nWhat's next?\n- Read the official documentation\n- Modify your site configuration with\ndocusaurus.config.js\n- Add navbar and footer items with\nthemeConfig\n- Add a custom Design and Layout\n- Add a search bar\n- Find inspirations in the Docusaurus showcase\n- Get involved in the Docusaurus Community\n--------------------------------------------------------------------------------"
  },
  {
    "url": "https://Muham251.github.io/ai-hackathon-1/docs/tutorial-basics/congratulations#__docusaurus_skipToContent_fallback",
    "text": "Congratulations!\nYou have just learned the basics of Docusaurus and made some changes to the initial template.\nDocusaurus has much more to offer!\nHave 5 more minutes? Take a look at versioning and i18n.\nAnything unclear or buggy in this tutorial? Please report it!\nWhat's next?\n- Read the official documentation\n- Modify your site configuration with\ndocusaurus.config.js\n- Add navbar and footer items with\nthemeConfig\n- Add a custom Design and Layout\n- Add a search bar\n- Find inspirations in the Docusaurus showcase\n- Get involved in the Docusaurus Community\n--------------------------------------------------------------------------------"
  },
  {
    "url": "https://Muham251.github.io/ai-hackathon-1/docs/module-2/",
    "text": "Spec: Module 2: The Digital Twin (Gazebo & Unity)\n1. Business Understanding\n1.1. Goal\nThe goal of this module is to educate students on creating and utilizing digital twins for humanoid robotics. The focus is on mastering physics simulation, environment construction, and sensor simulation within Gazebo and leveraging Unity for high-fidelity rendering.\n1.2. Justification\nDigital twins are fundamental to modern robotics development, enabling safe, cost-effective, and efficient testing of robot behaviors before deployment. This module provides the essential skills for simulating robots and their interactions with the environment, which is a cornerstone of Physical AI.\n1.3. Target Audience\n- Primary: Beginner to intermediate students in Physical AI and Humanoid Robotics.\n- Secondary: Hobbyists and developers looking to expand their skills into robotics simulation.\n2. Functional Requirements\n2.1. In Scope (Features)\n- Gazebo Physics Simulation:\n- Detailed explanation of core physics concepts: gravity, friction, and collision models.\n- A tutorial on building a simple environment (e.g., a room with obstacles) using SDF.\n- A demonstration of a humanoid robot interacting with the environment.\n- Unity for High-Fidelity Rendering:\n- An overview of Unity's rendering pipeline and its application in robotics.\n- A guide to setting up a simple Unity scene for robotic visualization.\n- Sensor Simulation:\n- A guide to simulating common robotic sensors: LiDAR, Depth Cameras, and IMUs in both Gazebo and Unity.\n- Configuration examples for each sensor.\n- Runnable Examples:\n- The module must include 2\u20133 complete, runnable code examples demonstrating the concepts.\n- Format: The final output must be a Docusaurus-compatible Markdown file.\n2.2. Out of Scope\n- Full-scale game development or advanced Unity features.\n- Complex humanoid control algorithms (this is reserved for Module 3).\n- Networked or multi-agent simulations.\n2.3. Success Criteria\n- Knowledge: Students can clearly explain the role of physics engines (Gazebo) and rendering engines (Unity) in creating a digital twin.\n- Practical Skill (Gazebo): Students can successfully build a simple Gazebo world and simulate a humanoid robot within it.\n- Practical Skill (Unity): Students can set up a Unity scene to render a robot model.\n- Practical Skill (Sensors): Students are able to add and configure virtual LiDAR, Depth Camera, and IMU sensors to a simulated robot.\n- Completion: The module contains 2-3 fully functional examples that run without errors.\n2.4. Constraints\n- Word Count: 2000\u20133000 words.\n- Format: Docusaurus Markdown, including formatted code blocks for SDF, C#, and Python/C++.\n- Sources: Content must be based on official documentation from Gazebo, Unity, and relevant sensor simulation libraries.\n- Timeline: 1 week for completion.\n3. UX/UI (Reader Experience)\n3.1. Content Structure\nThe module will be structured into clear, logical chapters to guide the reader progressively.\n- Chapter 1: Gazebo Physics & Environments\n- Introduction to Gazebo as a physics simulator.\n- Core concepts: gravity, collisions, and inertia.\n- Tutorial: Creating a\n.world\nfile with ground plane and basic shapes. - Example: Spawning a humanoid model and observing basic physical interactions.\n- Chapter 2: High-Fidelity Rendering with Unity\n- The role of Unity in robotics for high-quality visualization.\n- Setting up a Unity project for robotics (e.g., using ROS-Unity connectors).\n- Example: Importing a URDF model and creating a simple visualization scene.\n- Chapter 3: Simulating the Senses\n- Introduction to sensor simulation.\n- Simulating LiDAR and visualizing point clouds.\n- Simulating Depth Cameras and interpreting depth images.\n- Simulating an IMU to get orientation and acceleration data.\n- Example: A single robot model equipped with all three sensors, with configuration snippets for each.\n4. Technical Considerations\n4.1. Key Decisions\n- Simulation Tools: Gazebo will be used for robust physics simulation, while Unity will be highlighted for its superior rendering capabilities. The text will explain the trade-offs.\n- Code Examples: Examples will be provided as self-contained code blocks that can be easily copied and run.\n- Structure: The content will follow the chapter structure defined in section 3.1.\n4.2. External Dependencies\n- The module assumes readers have access to and a basic understanding of a Linux environment (for Gazebo/ROS).\n- Readers will need to have Gazebo and Unity Hub (with a recent Unity version) installed.\n- Code examples may rely on ROS 2,\nrclpy\n, orroscpp\nfor integration, building on concepts from Module 1.\n--------------------------------------------------------------------------------"
  },
  {
    "url": "https://Muham251.github.io/ai-hackathon-1/docs/module-2/#__docusaurus_skipToContent_fallback",
    "text": "Spec: Module 2: The Digital Twin (Gazebo & Unity)\n1. Business Understanding\n1.1. Goal\nThe goal of this module is to educate students on creating and utilizing digital twins for humanoid robotics. The focus is on mastering physics simulation, environment construction, and sensor simulation within Gazebo and leveraging Unity for high-fidelity rendering.\n1.2. Justification\nDigital twins are fundamental to modern robotics development, enabling safe, cost-effective, and efficient testing of robot behaviors before deployment. This module provides the essential skills for simulating robots and their interactions with the environment, which is a cornerstone of Physical AI.\n1.3. Target Audience\n- Primary: Beginner to intermediate students in Physical AI and Humanoid Robotics.\n- Secondary: Hobbyists and developers looking to expand their skills into robotics simulation.\n2. Functional Requirements\n2.1. In Scope (Features)\n- Gazebo Physics Simulation:\n- Detailed explanation of core physics concepts: gravity, friction, and collision models.\n- A tutorial on building a simple environment (e.g., a room with obstacles) using SDF.\n- A demonstration of a humanoid robot interacting with the environment.\n- Unity for High-Fidelity Rendering:\n- An overview of Unity's rendering pipeline and its application in robotics.\n- A guide to setting up a simple Unity scene for robotic visualization.\n- Sensor Simulation:\n- A guide to simulating common robotic sensors: LiDAR, Depth Cameras, and IMUs in both Gazebo and Unity.\n- Configuration examples for each sensor.\n- Runnable Examples:\n- The module must include 2\u20133 complete, runnable code examples demonstrating the concepts.\n- Format: The final output must be a Docusaurus-compatible Markdown file.\n2.2. Out of Scope\n- Full-scale game development or advanced Unity features.\n- Complex humanoid control algorithms (this is reserved for Module 3).\n- Networked or multi-agent simulations.\n2.3. Success Criteria\n- Knowledge: Students can clearly explain the role of physics engines (Gazebo) and rendering engines (Unity) in creating a digital twin.\n- Practical Skill (Gazebo): Students can successfully build a simple Gazebo world and simulate a humanoid robot within it.\n- Practical Skill (Unity): Students can set up a Unity scene to render a robot model.\n- Practical Skill (Sensors): Students are able to add and configure virtual LiDAR, Depth Camera, and IMU sensors to a simulated robot.\n- Completion: The module contains 2-3 fully functional examples that run without errors.\n2.4. Constraints\n- Word Count: 2000\u20133000 words.\n- Format: Docusaurus Markdown, including formatted code blocks for SDF, C#, and Python/C++.\n- Sources: Content must be based on official documentation from Gazebo, Unity, and relevant sensor simulation libraries.\n- Timeline: 1 week for completion.\n3. UX/UI (Reader Experience)\n3.1. Content Structure\nThe module will be structured into clear, logical chapters to guide the reader progressively.\n- Chapter 1: Gazebo Physics & Environments\n- Introduction to Gazebo as a physics simulator.\n- Core concepts: gravity, collisions, and inertia.\n- Tutorial: Creating a\n.world\nfile with ground plane and basic shapes. - Example: Spawning a humanoid model and observing basic physical interactions.\n- Chapter 2: High-Fidelity Rendering with Unity\n- The role of Unity in robotics for high-quality visualization.\n- Setting up a Unity project for robotics (e.g., using ROS-Unity connectors).\n- Example: Importing a URDF model and creating a simple visualization scene.\n- Chapter 3: Simulating the Senses\n- Introduction to sensor simulation.\n- Simulating LiDAR and visualizing point clouds.\n- Simulating Depth Cameras and interpreting depth images.\n- Simulating an IMU to get orientation and acceleration data.\n- Example: A single robot model equipped with all three sensors, with configuration snippets for each.\n4. Technical Considerations\n4.1. Key Decisions\n- Simulation Tools: Gazebo will be used for robust physics simulation, while Unity will be highlighted for its superior rendering capabilities. The text will explain the trade-offs.\n- Code Examples: Examples will be provided as self-contained code blocks that can be easily copied and run.\n- Structure: The content will follow the chapter structure defined in section 3.1.\n4.2. External Dependencies\n- The module assumes readers have access to and a basic understanding of a Linux environment (for Gazebo/ROS).\n- Readers will need to have Gazebo and Unity Hub (with a recent Unity version) installed.\n- Code examples may rely on ROS 2,\nrclpy\n, orroscpp\nfor integration, building on concepts from Module 1.\n--------------------------------------------------------------------------------"
  },
  {
    "url": "https://Muham251.github.io/ai-hackathon-1/docs/module-2/gazebo-physics-environments",
    "text": "Chapter 1: Gazebo Physics & Environments\n- Introduction to Gazebo as a physics simulator.\n- Core concepts: gravity, collisions, and inertia.\n- Tutorial: Creating a\n.world\nfile with ground plane and basic shapes. - Example: Spawning a humanoid model and observing basic physical interactions.\n--------------------------------------------------------------------------------"
  },
  {
    "url": "https://Muham251.github.io/ai-hackathon-1/docs/module-2/gazebo-physics-environments#__docusaurus_skipToContent_fallback",
    "text": "Chapter 1: Gazebo Physics & Environments\n- Introduction to Gazebo as a physics simulator.\n- Core concepts: gravity, collisions, and inertia.\n- Tutorial: Creating a\n.world\nfile with ground plane and basic shapes. - Example: Spawning a humanoid model and observing basic physical interactions.\n--------------------------------------------------------------------------------"
  },
  {
    "url": "https://Muham251.github.io/ai-hackathon-1/docs/module-2/high-fidelity-rendering-with-unity",
    "text": "Chapter 2: High-Fidelity Rendering with Unity\n- The role of Unity in robotics for high-quality visualization.\n- Setting up a Unity project for robotics (e.g., using ROS-Unity connectors).\n- Example: Importing a URDF model and creating a simple visualization scene.\n--------------------------------------------------------------------------------"
  },
  {
    "url": "https://Muham251.github.io/ai-hackathon-1/docs/module-2/high-fidelity-rendering-with-unity#__docusaurus_skipToContent_fallback",
    "text": "Chapter 2: High-Fidelity Rendering with Unity\n- The role of Unity in robotics for high-quality visualization.\n- Setting up a Unity project for robotics (e.g., using ROS-Unity connectors).\n- Example: Importing a URDF model and creating a simple visualization scene.\n--------------------------------------------------------------------------------"
  },
  {
    "url": "https://Muham251.github.io/ai-hackathon-1/docs/module-2/simulating-the-senses",
    "text": "Chapter 3: Simulating the Senses\n- Introduction to sensor simulation.\n- Simulating LiDAR and visualizing point clouds.\n- Simulating Depth Cameras and interpreting depth images.\n- Simulating an IMU to get orientation and acceleration data.\n- Example: A single robot model equipped with all three sensors, with configuration snippets for each.\n--------------------------------------------------------------------------------"
  },
  {
    "url": "https://Muham251.github.io/ai-hackathon-1/docs/module-2/simulating-the-senses#__docusaurus_skipToContent_fallback",
    "text": "Chapter 3: Simulating the Senses\n- Introduction to sensor simulation.\n- Simulating LiDAR and visualizing point clouds.\n- Simulating Depth Cameras and interpreting depth images.\n- Simulating an IMU to get orientation and acceleration data.\n- Example: A single robot model equipped with all three sensors, with configuration snippets for each.\n--------------------------------------------------------------------------------"
  },
  {
    "url": "https://Muham251.github.io/ai-hackathon-1/docs/category/tutorial---extras",
    "text": "\ud83d\udcc4\ufe0f Manage Docs VersionsDocusaurus can manage multiple versions of your docs.\ud83d\udcc4\ufe0f Translate your siteLet's translate docs/intro.md to French.\n--------------------------------------------------------------------------------"
  },
  {
    "url": "https://Muham251.github.io/ai-hackathon-1/docs/category/tutorial---extras#__docusaurus_skipToContent_fallback",
    "text": "\ud83d\udcc4\ufe0f Manage Docs VersionsDocusaurus can manage multiple versions of your docs.\ud83d\udcc4\ufe0f Translate your siteLet's translate docs/intro.md to French.\n--------------------------------------------------------------------------------"
  },
  {
    "url": "https://Muham251.github.io/ai-hackathon-1/docs/tutorial-extras/manage-docs-versions",
    "text": "Manage Docs Versions\nDocusaurus can manage multiple versions of your docs.\nCreate a docs version\nRelease a version 1.0 of your project:\nnpm run docusaurus docs:version 1.0\nThe docs\nfolder is copied into versioned_docs/version-1.0\nand versions.json\nis created.\nYour docs now have 2 versions:\n1.0\nathttp://localhost:3000/docs/\nfor the version 1.0 docscurrent\nathttp://localhost:3000/docs/next/\nfor the upcoming, unreleased docs\nAdd a Version Dropdown\nTo navigate seamlessly across versions, add a version dropdown.\nModify the docusaurus.config.js\nfile:\ndocusaurus.config.js\nexport default {\nthemeConfig: {\nnavbar: {\nitems: [\n{\ntype: 'docsVersionDropdown',\n},\n],\n},\n},\n};\nThe docs version dropdown appears in your navbar:\nUpdate an existing version\nIt is possible to edit versioned docs in their respective folder:\nversioned_docs/version-1.0/hello.md\nupdateshttp://localhost:3000/docs/hello\ndocs/hello.md\nupdateshttp://localhost:3000/docs/next/hello\n--------------------------------------------------------------------------------"
  },
  {
    "url": "https://Muham251.github.io/ai-hackathon-1/docs/tutorial-extras/manage-docs-versions#__docusaurus_skipToContent_fallback",
    "text": "Manage Docs Versions\nDocusaurus can manage multiple versions of your docs.\nCreate a docs version\nRelease a version 1.0 of your project:\nnpm run docusaurus docs:version 1.0\nThe docs\nfolder is copied into versioned_docs/version-1.0\nand versions.json\nis created.\nYour docs now have 2 versions:\n1.0\nathttp://localhost:3000/docs/\nfor the version 1.0 docscurrent\nathttp://localhost:3000/docs/next/\nfor the upcoming, unreleased docs\nAdd a Version Dropdown\nTo navigate seamlessly across versions, add a version dropdown.\nModify the docusaurus.config.js\nfile:\ndocusaurus.config.js\nexport default {\nthemeConfig: {\nnavbar: {\nitems: [\n{\ntype: 'docsVersionDropdown',\n},\n],\n},\n},\n};\nThe docs version dropdown appears in your navbar:\nUpdate an existing version\nIt is possible to edit versioned docs in their respective folder:\nversioned_docs/version-1.0/hello.md\nupdateshttp://localhost:3000/docs/hello\ndocs/hello.md\nupdateshttp://localhost:3000/docs/next/hello\n--------------------------------------------------------------------------------"
  },
  {
    "url": "https://Muham251.github.io/ai-hackathon-1/docs/tutorial-extras/translate-your-site",
    "text": "Translate your site\nLet's translate docs/intro.md\nto French.\nConfigure i18n\nModify docusaurus.config.js\nto add support for the fr\nlocale:\ndocusaurus.config.js\nexport default {\ni18n: {\ndefaultLocale: 'en',\nlocales: ['en', 'fr'],\n},\n};\nTranslate a doc\nCopy the docs/intro.md\nfile to the i18n/fr\nfolder:\nmkdir -p i18n/fr/docusaurus-plugin-content-docs/current/\ncp docs/intro.md i18n/fr/docusaurus-plugin-content-docs/current/intro.md\nTranslate i18n/fr/docusaurus-plugin-content-docs/current/intro.md\nin French.\nStart your localized site\nStart your site on the French locale:\nnpm run start -- --locale fr\nYour localized site is accessible at http://localhost:3000/fr/ and the Getting Started\npage is translated.\ncaution\nIn development, you can only use one locale at a time.\nAdd a Locale Dropdown\nTo navigate seamlessly across languages, add a locale dropdown.\nModify the docusaurus.config.js\nfile:\ndocusaurus.config.js\nexport default {\nthemeConfig: {\nnavbar: {\nitems: [\n{\ntype: 'localeDropdown',\n},\n],\n},\n},\n};\nThe locale dropdown now appears in your navbar:\nBuild your localized site\nBuild your site for a specific locale:\nnpm run build -- --locale fr\nOr build your site to include all the locales at once:\nnpm run build\n--------------------------------------------------------------------------------"
  },
  {
    "url": "https://Muham251.github.io/ai-hackathon-1/docs/tutorial-extras/translate-your-site#__docusaurus_skipToContent_fallback",
    "text": "Translate your site\nLet's translate docs/intro.md\nto French.\nConfigure i18n\nModify docusaurus.config.js\nto add support for the fr\nlocale:\ndocusaurus.config.js\nexport default {\ni18n: {\ndefaultLocale: 'en',\nlocales: ['en', 'fr'],\n},\n};\nTranslate a doc\nCopy the docs/intro.md\nfile to the i18n/fr\nfolder:\nmkdir -p i18n/fr/docusaurus-plugin-content-docs/current/\ncp docs/intro.md i18n/fr/docusaurus-plugin-content-docs/current/intro.md\nTranslate i18n/fr/docusaurus-plugin-content-docs/current/intro.md\nin French.\nStart your localized site\nStart your site on the French locale:\nnpm run start -- --locale fr\nYour localized site is accessible at http://localhost:3000/fr/ and the Getting Started\npage is translated.\ncaution\nIn development, you can only use one locale at a time.\nAdd a Locale Dropdown\nTo navigate seamlessly across languages, add a locale dropdown.\nModify the docusaurus.config.js\nfile:\ndocusaurus.config.js\nexport default {\nthemeConfig: {\nnavbar: {\nitems: [\n{\ntype: 'localeDropdown',\n},\n],\n},\n},\n};\nThe locale dropdown now appears in your navbar:\nBuild your localized site\nBuild your site for a specific locale:\nnpm run build -- --locale fr\nOr build your site to include all the locales at once:\nnpm run build\n--------------------------------------------------------------------------------"
  },
  {
    "url": "https://Muham251.github.io/ai-hackathon-1/docs/module-3/",
    "text": "Spec: Module 3: The AI-Robot Brain (NVIDIA Isaac\u2122)\n1. Business Understanding\n1.1. Goal\nThe objective of this module is to educate beginner to intermediate robotics students on leveraging the NVIDIA Isaac\u2122 ecosystem to build a robot's core intelligence. The curriculum will focus on Isaac Sim for photorealistic simulation, Isaac ROS for perception and navigation (specifically VSLAM), and Nav2 for autonomous path planning.\n1.2. Justification\nThe NVIDIA Isaac\u2122 platform is at the forefront of AI-driven robotics, offering powerful tools for developing and testing intelligent robots in realistic virtual environments. This module provides learners with critical, industry-relevant skills in simulation, perception, and navigation, forming the foundation of a robot's \"brain.\"\n1.3. Target Audience\n- Primary: Beginner to intermediate students in robotics and AI who have a foundational understanding of ROS and simulation concepts.\n- Secondary: Developers and engineers looking to get started with the NVIDIA Isaac\u2122 toolchain for robotics projects.\n2. Functional Requirements\n2.1. In Scope (Features)\n- Isaac Sim Fundamentals:\n- An introduction to creating scenes in Isaac Sim.\n- A guide on generating synthetic data (e.g., camera images, LiDAR data) for training and testing.\n- Isaac ROS Workflow:\n- A conceptual overview of the Visual SLAM (VSLAM) and navigation pipeline provided by Isaac ROS.\n- A tutorial on setting up and running the Isaac ROS navigation stack on a simulated robot.\n- Nav2 Path Planning:\n- A practical example demonstrating how to use Nav2 for path planning with a humanoid robot in Isaac Sim.\n- Runnable Demos:\n- The module must include 2\u20133 complete, runnable demonstrations that users can execute.\n- Format: The final content will be delivered as a Docusaurus-compatible Markdown file.\n2.2. Out of Scope\n- A complete, end-to-end humanoid control pipeline (e.g., manipulation, complex behaviors).\n- The development or deep-dive analysis of custom SLAM or navigation algorithms.\n- Advanced Isaac Sim features like custom Python scripting or detailed RTX rendering settings.\n2.3. Success Criteria\n- Understanding: Learners can explain the significance of photorealistic simulation and synthetic data generation for robotics.\n- Explanation: Learners can describe the high-level workflow of the Isaac ROS VSLAM and navigation stack.\n- Application: Learners can successfully launch and run a basic Nav2 path-planning demonstration within Isaac Sim.\n- Completion: The module is published with 2\u20133 fully functional and validated demos.\n2.4. Constraints\n- Word Count: Approximately 2000\u20133000 words.\n- Format: Docusaurus Markdown with appropriate code blocks for commands and configurations.\n- Sources: All technical information must be derived from and cite official NVIDIA Isaac\u2122 documentation.\n- Timeline: The module is to be completed within a 1-week timeframe.\n3. UX/UI (Reader Experience)\n3.1. Content Structure\nThe module will be organized into a clear, three-chapter structure to guide the learner from environment creation to autonomous navigation.\n- Chapter 1: Isaac Sim Basics: Scenes & Synthetic Data\n- Introduction to the Isaac Sim interface and its core concepts.\n- Tutorial: Setting up a basic scene with a robot and obstacles.\n- Guide: Generating and exporting synthetic sensor data from the simulation.\n- Chapter 2: Isaac ROS: VSLAM & Navigation Workflow\n- Overview of the Isaac ROS ecosystem and its advantages.\n- Step-by-step guide to launching the Isaac ROS Docker container.\n- Tutorial: Running the VSLAM and navigation stack on a robot within Isaac Sim.\n- Chapter 3: Nav2: Path Planning for Humanoids\n- Introduction to Nav2 and its role in autonomous navigation.\n- Example: Sending a navigation goal to a humanoid robot and having it execute a path using Nav2.\n4. Technical Considerations\n4.1. Key Decisions\n- Ecosystem Focus: This module will exclusively use the NVIDIA Isaac\u2122 toolchain to provide a cohesive learning experience.\n- Runnable Demos: Demos will be provided as shell scripts or clear, step-by-step command sequences to ensure they are easy to run.\n- Configuration over Code: The focus will be on configuring and running existing Isaac ROS packages rather than writing new code.\n4.2. External Dependencies\n- Hardware: A computer with a compatible NVIDIA GPU is required.\n- Software: Users must have NVIDIA Isaac Sim, Docker, and NVIDIA Container Toolkit installed.\n- Prerequisites: A foundational knowledge of ROS 2 concepts is assumed (as taught in Module 1).\n--------------------------------------------------------------------------------"
  },
  {
    "url": "https://Muham251.github.io/ai-hackathon-1/docs/module-3/#__docusaurus_skipToContent_fallback",
    "text": "Spec: Module 3: The AI-Robot Brain (NVIDIA Isaac\u2122)\n1. Business Understanding\n1.1. Goal\nThe objective of this module is to educate beginner to intermediate robotics students on leveraging the NVIDIA Isaac\u2122 ecosystem to build a robot's core intelligence. The curriculum will focus on Isaac Sim for photorealistic simulation, Isaac ROS for perception and navigation (specifically VSLAM), and Nav2 for autonomous path planning.\n1.2. Justification\nThe NVIDIA Isaac\u2122 platform is at the forefront of AI-driven robotics, offering powerful tools for developing and testing intelligent robots in realistic virtual environments. This module provides learners with critical, industry-relevant skills in simulation, perception, and navigation, forming the foundation of a robot's \"brain.\"\n1.3. Target Audience\n- Primary: Beginner to intermediate students in robotics and AI who have a foundational understanding of ROS and simulation concepts.\n- Secondary: Developers and engineers looking to get started with the NVIDIA Isaac\u2122 toolchain for robotics projects.\n2. Functional Requirements\n2.1. In Scope (Features)\n- Isaac Sim Fundamentals:\n- An introduction to creating scenes in Isaac Sim.\n- A guide on generating synthetic data (e.g., camera images, LiDAR data) for training and testing.\n- Isaac ROS Workflow:\n- A conceptual overview of the Visual SLAM (VSLAM) and navigation pipeline provided by Isaac ROS.\n- A tutorial on setting up and running the Isaac ROS navigation stack on a simulated robot.\n- Nav2 Path Planning:\n- A practical example demonstrating how to use Nav2 for path planning with a humanoid robot in Isaac Sim.\n- Runnable Demos:\n- The module must include 2\u20133 complete, runnable demonstrations that users can execute.\n- Format: The final content will be delivered as a Docusaurus-compatible Markdown file.\n2.2. Out of Scope\n- A complete, end-to-end humanoid control pipeline (e.g., manipulation, complex behaviors).\n- The development or deep-dive analysis of custom SLAM or navigation algorithms.\n- Advanced Isaac Sim features like custom Python scripting or detailed RTX rendering settings.\n2.3. Success Criteria\n- Understanding: Learners can explain the significance of photorealistic simulation and synthetic data generation for robotics.\n- Explanation: Learners can describe the high-level workflow of the Isaac ROS VSLAM and navigation stack.\n- Application: Learners can successfully launch and run a basic Nav2 path-planning demonstration within Isaac Sim.\n- Completion: The module is published with 2\u20133 fully functional and validated demos.\n2.4. Constraints\n- Word Count: Approximately 2000\u20133000 words.\n- Format: Docusaurus Markdown with appropriate code blocks for commands and configurations.\n- Sources: All technical information must be derived from and cite official NVIDIA Isaac\u2122 documentation.\n- Timeline: The module is to be completed within a 1-week timeframe.\n3. UX/UI (Reader Experience)\n3.1. Content Structure\nThe module will be organized into a clear, three-chapter structure to guide the learner from environment creation to autonomous navigation.\n- Chapter 1: Isaac Sim Basics: Scenes & Synthetic Data\n- Introduction to the Isaac Sim interface and its core concepts.\n- Tutorial: Setting up a basic scene with a robot and obstacles.\n- Guide: Generating and exporting synthetic sensor data from the simulation.\n- Chapter 2: Isaac ROS: VSLAM & Navigation Workflow\n- Overview of the Isaac ROS ecosystem and its advantages.\n- Step-by-step guide to launching the Isaac ROS Docker container.\n- Tutorial: Running the VSLAM and navigation stack on a robot within Isaac Sim.\n- Chapter 3: Nav2: Path Planning for Humanoids\n- Introduction to Nav2 and its role in autonomous navigation.\n- Example: Sending a navigation goal to a humanoid robot and having it execute a path using Nav2.\n4. Technical Considerations\n4.1. Key Decisions\n- Ecosystem Focus: This module will exclusively use the NVIDIA Isaac\u2122 toolchain to provide a cohesive learning experience.\n- Runnable Demos: Demos will be provided as shell scripts or clear, step-by-step command sequences to ensure they are easy to run.\n- Configuration over Code: The focus will be on configuring and running existing Isaac ROS packages rather than writing new code.\n4.2. External Dependencies\n- Hardware: A computer with a compatible NVIDIA GPU is required.\n- Software: Users must have NVIDIA Isaac Sim, Docker, and NVIDIA Container Toolkit installed.\n- Prerequisites: A foundational knowledge of ROS 2 concepts is assumed (as taught in Module 1).\n--------------------------------------------------------------------------------"
  },
  {
    "url": "https://Muham251.github.io/ai-hackathon-1/docs/module-3/isaac-sim-basics",
    "text": "Chapter 1: Isaac Sim Basics: Scenes & Synthetic Data\n- Introduction to the Isaac Sim interface and its core concepts.\n- Tutorial: Setting up a basic scene with a robot and obstacles.\n- Guide: Generating and exporting synthetic sensor data from the simulation.\n--------------------------------------------------------------------------------"
  },
  {
    "url": "https://Muham251.github.io/ai-hackathon-1/docs/module-3/isaac-sim-basics#__docusaurus_skipToContent_fallback",
    "text": "Chapter 1: Isaac Sim Basics: Scenes & Synthetic Data\n- Introduction to the Isaac Sim interface and its core concepts.\n- Tutorial: Setting up a basic scene with a robot and obstacles.\n- Guide: Generating and exporting synthetic sensor data from the simulation.\n--------------------------------------------------------------------------------"
  },
  {
    "url": "https://Muham251.github.io/ai-hackathon-1/docs/module-3/isaac-ros-vslam-navigation",
    "text": "Module 3: The AI-Robot Brain (NVIDIA Isaac\u2122)Chapter 2: Isaac ROS: VSLAM & Navigation WorkflowChapter 2: Isaac ROS: VSLAM & Navigation Workflow Overview of the Isaac ROS ecosystem and its advantages. Step-by-step guide to launching the Isaac ROS Docker container. Tutorial: Running the VSLAM and navigation stack on a robot within Isaac Sim.\n--------------------------------------------------------------------------------"
  },
  {
    "url": "https://Muham251.github.io/ai-hackathon-1/docs/module-3/isaac-ros-vslam-navigation#__docusaurus_skipToContent_fallback",
    "text": "Module 3: The AI-Robot Brain (NVIDIA Isaac\u2122)Chapter 2: Isaac ROS: VSLAM & Navigation WorkflowChapter 2: Isaac ROS: VSLAM & Navigation Workflow Overview of the Isaac ROS ecosystem and its advantages. Step-by-step guide to launching the Isaac ROS Docker container. Tutorial: Running the VSLAM and navigation stack on a robot within Isaac Sim.\n--------------------------------------------------------------------------------"
  },
  {
    "url": "https://Muham251.github.io/ai-hackathon-1/docs/module-3/nav2-path-planning",
    "text": "Module 3: The AI-Robot Brain (NVIDIA Isaac\u2122)Chapter 3: Nav2: Path Planning for HumanoidsChapter 3: Nav2: Path Planning for Humanoids Introduction to Nav2 and its role in autonomous navigation. Example: Sending a navigation goal to a humanoid robot and having it execute a path using Nav2.\n--------------------------------------------------------------------------------"
  },
  {
    "url": "https://Muham251.github.io/ai-hackathon-1/docs/module-3/nav2-path-planning#__docusaurus_skipToContent_fallback",
    "text": "Module 3: The AI-Robot Brain (NVIDIA Isaac\u2122)Chapter 3: Nav2: Path Planning for HumanoidsChapter 3: Nav2: Path Planning for Humanoids Introduction to Nav2 and its role in autonomous navigation. Example: Sending a navigation goal to a humanoid robot and having it execute a path using Nav2.\n--------------------------------------------------------------------------------"
  },
  {
    "url": "https://Muham251.github.io/ai-hackathon-1/docs/module-4/",
    "text": "Spec: Module 4: Vision-Language-Action (VLA)\n1. Business Understanding\n1.1. Goal\nThe primary goal of this module is to teach students how to build a complete Vision-Language-Action (VLA) pipeline. This involves integrating Large Language Models (LLMs) with robotics, using Whisper for voice command input, employing LLMs for cognitive planning, and executing tasks through ROS 2 actions.\n1.2. Justification\nVLA is a transformative paradigm in robotics, enabling more natural human-robot interaction and a higher degree of autonomy. This module provides a crucial bridge between theoretical AI concepts (like LLMs) and practical, physical robotics, equipping students with highly sought-after skills in creating intelligent, responsive robots.\n1.3. Target Audience\n- Primary: Beginner to intermediate students in robotics and AI.\n- Secondary: Developers and researchers interested in the practical application of LLMs in autonomous systems.\n2. Functional Requirements\n2.1. In Scope (Features)\n- VLA Pipeline Overview: A clear, conceptual explanation of the entire Vision-Language-Action pipeline, from voice input to robot action.\n- Voice Command Processing: A tutorial on using OpenAI Whisper to transcribe spoken language into text and extract actionable commands.\n- LLM-based Planning: A guide on how to prompt an LLM to translate a high-level natural language task (e.g., \"get me the apple\") into a sequence of concrete ROS 2 action goals.\n- Mini-Capstone Project: A complete, runnable example demonstrating the full VLA loop. A user will issue a voice command, and a simulated humanoid robot will plan and execute a simple multi-step task involving navigation, perception, and manipulation.\n2.2. Out of Scope\n- The development of a production-grade, highly reliable robotics stack.\n- Training or fine-tuning custom Large Language Models.\n- Complex, long-horizon tasks or navigation in dynamic, multi-room environments.\n2.3. Success Criteria\n- Understanding: Students can draw a diagram of the VLA pipeline and explain the role of each component (Voice, LLM, ROS 2).\n- Application (Whisper): Students are able to run a demo that successfully converts a spoken command into a text string.\n- Application (LLM Planning): Students can explain how a natural language goal is decomposed into a JSON or YAML sequence of ROS 2 actions by an LLM.\n- Completion: The mini-capstone example runs end-to-end: a voice command triggers a sequence of autonomous actions in the simulated robot.\n2.4. Constraints\n- Word Count: 2000\u20133000 words.\n- Format: Docusaurus Markdown, including code snippets for Python (for Whisper and LLM interaction) and configuration files (for ROS 2 actions).\n- Sources: Content will be based on official OpenAI Whisper documentation, ROS 2 action design patterns, and foundational VLA research.\n- Timeline: 1 week.\n3. UX/UI (Reader Experience)\n3.1. Content Structure\nThe module will be structured to guide the learner from individual components to a fully integrated system.\n- Chapter 1: Voice-to-Action: Whisper and Command Extraction\n- Introduction to Whisper for speech-to-text.\n- Tutorial: Setting up Whisper and running a simple voice transcription demo.\n- Guide: Extracting key intents and entities from the transcribed text (e.g., turning \"Can you please find the red block\" into\n{action: \"find\", object: \"red block\"}\n).\n- Chapter 2: Cognitive Planning with LLMs\n- The role of LLMs as a \"reasoning engine\" for robots.\n- Tutorial: Prompt engineering techniques to make an LLM translate a task into a machine-readable plan (a sequence of ROS 2 actions).\n- Example: Converting\n{action: \"find\", object: \"red block\"}\ninto a plan like[{navigate: \"table\"}, {detect: \"red_block\"}]\n.\n- Chapter 3: Mini-Capstone: The Full VLA Loop\n- An overview of the complete system architecture.\n- A step-by-step guide to launching all components (Whisper listener, LLM planner, ROS 2 action servers).\n- A full demonstration of an autonomous task: e.g., \"Go to the table, find the blue cup, and pick it up.\"\n4. Technical Considerations\n4.1. Key Decisions\n- LLM Abstraction: We will use a major LLM provider's API (e.g., OpenAI, Anthropic, or Google) to avoid the complexity of self-hosting. The focus is on the integration, not the model itself.\n- ROS 2 Actions: The module will use the standard ROS 2 action interface for all robot tasks (e.g.,\nNavigateToPose\n,DetectObject\n,PickAndPlace\n). This ensures a modular and scalable design. - Focus on Integration: The code provided will be primarily \"glue code\" written in Python, connecting the various components of the VLA pipeline.\n4.2. External Dependencies\n- Software: Python 3, ROS 2, and the OpenAI Whisper library.\n- APIs: An API key for an LLM provider will be required for the cognitive planning chapter.\n- Hardware: A microphone for voice commands.\n- Prerequisites: Completion of prior modules, including ROS 2 basics, simulation, and navigation.\n--------------------------------------------------------------------------------"
  },
  {
    "url": "https://Muham251.github.io/ai-hackathon-1/docs/module-4/#__docusaurus_skipToContent_fallback",
    "text": "Spec: Module 4: Vision-Language-Action (VLA)\n1. Business Understanding\n1.1. Goal\nThe primary goal of this module is to teach students how to build a complete Vision-Language-Action (VLA) pipeline. This involves integrating Large Language Models (LLMs) with robotics, using Whisper for voice command input, employing LLMs for cognitive planning, and executing tasks through ROS 2 actions.\n1.2. Justification\nVLA is a transformative paradigm in robotics, enabling more natural human-robot interaction and a higher degree of autonomy. This module provides a crucial bridge between theoretical AI concepts (like LLMs) and practical, physical robotics, equipping students with highly sought-after skills in creating intelligent, responsive robots.\n1.3. Target Audience\n- Primary: Beginner to intermediate students in robotics and AI.\n- Secondary: Developers and researchers interested in the practical application of LLMs in autonomous systems.\n2. Functional Requirements\n2.1. In Scope (Features)\n- VLA Pipeline Overview: A clear, conceptual explanation of the entire Vision-Language-Action pipeline, from voice input to robot action.\n- Voice Command Processing: A tutorial on using OpenAI Whisper to transcribe spoken language into text and extract actionable commands.\n- LLM-based Planning: A guide on how to prompt an LLM to translate a high-level natural language task (e.g., \"get me the apple\") into a sequence of concrete ROS 2 action goals.\n- Mini-Capstone Project: A complete, runnable example demonstrating the full VLA loop. A user will issue a voice command, and a simulated humanoid robot will plan and execute a simple multi-step task involving navigation, perception, and manipulation.\n2.2. Out of Scope\n- The development of a production-grade, highly reliable robotics stack.\n- Training or fine-tuning custom Large Language Models.\n- Complex, long-horizon tasks or navigation in dynamic, multi-room environments.\n2.3. Success Criteria\n- Understanding: Students can draw a diagram of the VLA pipeline and explain the role of each component (Voice, LLM, ROS 2).\n- Application (Whisper): Students are able to run a demo that successfully converts a spoken command into a text string.\n- Application (LLM Planning): Students can explain how a natural language goal is decomposed into a JSON or YAML sequence of ROS 2 actions by an LLM.\n- Completion: The mini-capstone example runs end-to-end: a voice command triggers a sequence of autonomous actions in the simulated robot.\n2.4. Constraints\n- Word Count: 2000\u20133000 words.\n- Format: Docusaurus Markdown, including code snippets for Python (for Whisper and LLM interaction) and configuration files (for ROS 2 actions).\n- Sources: Content will be based on official OpenAI Whisper documentation, ROS 2 action design patterns, and foundational VLA research.\n- Timeline: 1 week.\n3. UX/UI (Reader Experience)\n3.1. Content Structure\nThe module will be structured to guide the learner from individual components to a fully integrated system.\n- Chapter 1: Voice-to-Action: Whisper and Command Extraction\n- Introduction to Whisper for speech-to-text.\n- Tutorial: Setting up Whisper and running a simple voice transcription demo.\n- Guide: Extracting key intents and entities from the transcribed text (e.g., turning \"Can you please find the red block\" into\n{action: \"find\", object: \"red block\"}\n).\n- Chapter 2: Cognitive Planning with LLMs\n- The role of LLMs as a \"reasoning engine\" for robots.\n- Tutorial: Prompt engineering techniques to make an LLM translate a task into a machine-readable plan (a sequence of ROS 2 actions).\n- Example: Converting\n{action: \"find\", object: \"red block\"}\ninto a plan like[{navigate: \"table\"}, {detect: \"red_block\"}]\n.\n- Chapter 3: Mini-Capstone: The Full VLA Loop\n- An overview of the complete system architecture.\n- A step-by-step guide to launching all components (Whisper listener, LLM planner, ROS 2 action servers).\n- A full demonstration of an autonomous task: e.g., \"Go to the table, find the blue cup, and pick it up.\"\n4. Technical Considerations\n4.1. Key Decisions\n- LLM Abstraction: We will use a major LLM provider's API (e.g., OpenAI, Anthropic, or Google) to avoid the complexity of self-hosting. The focus is on the integration, not the model itself.\n- ROS 2 Actions: The module will use the standard ROS 2 action interface for all robot tasks (e.g.,\nNavigateToPose\n,DetectObject\n,PickAndPlace\n). This ensures a modular and scalable design. - Focus on Integration: The code provided will be primarily \"glue code\" written in Python, connecting the various components of the VLA pipeline.\n4.2. External Dependencies\n- Software: Python 3, ROS 2, and the OpenAI Whisper library.\n- APIs: An API key for an LLM provider will be required for the cognitive planning chapter.\n- Hardware: A microphone for voice commands.\n- Prerequisites: Completion of prior modules, including ROS 2 basics, simulation, and navigation.\n--------------------------------------------------------------------------------"
  },
  {
    "url": "https://Muham251.github.io/ai-hackathon-1/docs/module-4/voice-to-action",
    "text": "Chapter 1: Voice-to-Action: Whisper and Command Extraction\n- Introduction to Whisper for speech-to-text.\n- Tutorial: Setting up Whisper and running a simple voice transcription demo.\n- Guide: Extracting key intents and entities from the transcribed text (e.g., turning \"Can you please find the red block\" into\n{action: \"find\", object: \"red block\"}\n).\n--------------------------------------------------------------------------------"
  },
  {
    "url": "https://Muham251.github.io/ai-hackathon-1/docs/module-4/voice-to-action#__docusaurus_skipToContent_fallback",
    "text": "Chapter 1: Voice-to-Action: Whisper and Command Extraction\n- Introduction to Whisper for speech-to-text.\n- Tutorial: Setting up Whisper and running a simple voice transcription demo.\n- Guide: Extracting key intents and entities from the transcribed text (e.g., turning \"Can you please find the red block\" into\n{action: \"find\", object: \"red block\"}\n).\n--------------------------------------------------------------------------------"
  },
  {
    "url": "https://Muham251.github.io/ai-hackathon-1/docs/module-4/cognitive-planning-with-llms",
    "text": "Chapter 2: Cognitive Planning with LLMs\n- The role of LLMs as a \"reasoning engine\" for robots.\n- Tutorial: Prompt engineering techniques to make an LLM translate a task into a machine-readable plan (a sequence of ROS 2 actions).\n- Example: Converting\n{action: \"find\", object: \"red block\"}\ninto a plan like[{navigate: \"table\"}, {detect: \"red_block\"}]\n.\n--------------------------------------------------------------------------------"
  },
  {
    "url": "https://Muham251.github.io/ai-hackathon-1/docs/module-4/cognitive-planning-with-llms#__docusaurus_skipToContent_fallback",
    "text": "Chapter 2: Cognitive Planning with LLMs\n- The role of LLMs as a \"reasoning engine\" for robots.\n- Tutorial: Prompt engineering techniques to make an LLM translate a task into a machine-readable plan (a sequence of ROS 2 actions).\n- Example: Converting\n{action: \"find\", object: \"red block\"}\ninto a plan like[{navigate: \"table\"}, {detect: \"red_block\"}]\n.\n--------------------------------------------------------------------------------"
  },
  {
    "url": "https://Muham251.github.io/ai-hackathon-1/docs/module-4/mini-capstone",
    "text": "Chapter 3: Mini-Capstone: The Full VLA Loop\n- An overview of the complete system architecture.\n- A step-by-step guide to launching all components (Whisper listener, LLM planner, ROS 2 action servers).\n- A full demonstration of an autonomous task: e.g., \"Go to the table, find the blue cup, and pick it up.\"\n--------------------------------------------------------------------------------"
  },
  {
    "url": "https://Muham251.github.io/ai-hackathon-1/docs/module-4/mini-capstone#__docusaurus_skipToContent_fallback",
    "text": "Chapter 3: Mini-Capstone: The Full VLA Loop\n- An overview of the complete system architecture.\n- A step-by-step guide to launching all components (Whisper listener, LLM planner, ROS 2 action servers).\n- A full demonstration of an autonomous task: e.g., \"Go to the table, find the blue cup, and pick it up.\"\n--------------------------------------------------------------------------------"
  },
  {
    "url": "https://Muham251.github.io/ai-hackathon-1/docs/module-4/#1-business-understanding",
    "text": "Spec: Module 4: Vision-Language-Action (VLA)\n1. Business Understanding\n1.1. Goal\nThe primary goal of this module is to teach students how to build a complete Vision-Language-Action (VLA) pipeline. This involves integrating Large Language Models (LLMs) with robotics, using Whisper for voice command input, employing LLMs for cognitive planning, and executing tasks through ROS 2 actions.\n1.2. Justification\nVLA is a transformative paradigm in robotics, enabling more natural human-robot interaction and a higher degree of autonomy. This module provides a crucial bridge between theoretical AI concepts (like LLMs) and practical, physical robotics, equipping students with highly sought-after skills in creating intelligent, responsive robots.\n1.3. Target Audience\n- Primary: Beginner to intermediate students in robotics and AI.\n- Secondary: Developers and researchers interested in the practical application of LLMs in autonomous systems.\n2. Functional Requirements\n2.1. In Scope (Features)\n- VLA Pipeline Overview: A clear, conceptual explanation of the entire Vision-Language-Action pipeline, from voice input to robot action.\n- Voice Command Processing: A tutorial on using OpenAI Whisper to transcribe spoken language into text and extract actionable commands.\n- LLM-based Planning: A guide on how to prompt an LLM to translate a high-level natural language task (e.g., \"get me the apple\") into a sequence of concrete ROS 2 action goals.\n- Mini-Capstone Project: A complete, runnable example demonstrating the full VLA loop. A user will issue a voice command, and a simulated humanoid robot will plan and execute a simple multi-step task involving navigation, perception, and manipulation.\n2.2. Out of Scope\n- The development of a production-grade, highly reliable robotics stack.\n- Training or fine-tuning custom Large Language Models.\n- Complex, long-horizon tasks or navigation in dynamic, multi-room environments.\n2.3. Success Criteria\n- Understanding: Students can draw a diagram of the VLA pipeline and explain the role of each component (Voice, LLM, ROS 2).\n- Application (Whisper): Students are able to run a demo that successfully converts a spoken command into a text string.\n- Application (LLM Planning): Students can explain how a natural language goal is decomposed into a JSON or YAML sequence of ROS 2 actions by an LLM.\n- Completion: The mini-capstone example runs end-to-end: a voice command triggers a sequence of autonomous actions in the simulated robot.\n2.4. Constraints\n- Word Count: 2000\u20133000 words.\n- Format: Docusaurus Markdown, including code snippets for Python (for Whisper and LLM interaction) and configuration files (for ROS 2 actions).\n- Sources: Content will be based on official OpenAI Whisper documentation, ROS 2 action design patterns, and foundational VLA research.\n- Timeline: 1 week.\n3. UX/UI (Reader Experience)\n3.1. Content Structure\nThe module will be structured to guide the learner from individual components to a fully integrated system.\n- Chapter 1: Voice-to-Action: Whisper and Command Extraction\n- Introduction to Whisper for speech-to-text.\n- Tutorial: Setting up Whisper and running a simple voice transcription demo.\n- Guide: Extracting key intents and entities from the transcribed text (e.g., turning \"Can you please find the red block\" into\n{action: \"find\", object: \"red block\"}\n).\n- Chapter 2: Cognitive Planning with LLMs\n- The role of LLMs as a \"reasoning engine\" for robots.\n- Tutorial: Prompt engineering techniques to make an LLM translate a task into a machine-readable plan (a sequence of ROS 2 actions).\n- Example: Converting\n{action: \"find\", object: \"red block\"}\ninto a plan like[{navigate: \"table\"}, {detect: \"red_block\"}]\n.\n- Chapter 3: Mini-Capstone: The Full VLA Loop\n- An overview of the complete system architecture.\n- A step-by-step guide to launching all components (Whisper listener, LLM planner, ROS 2 action servers).\n- A full demonstration of an autonomous task: e.g., \"Go to the table, find the blue cup, and pick it up.\"\n4. Technical Considerations\n4.1. Key Decisions\n- LLM Abstraction: We will use a major LLM provider's API (e.g., OpenAI, Anthropic, or Google) to avoid the complexity of self-hosting. The focus is on the integration, not the model itself.\n- ROS 2 Actions: The module will use the standard ROS 2 action interface for all robot tasks (e.g.,\nNavigateToPose\n,DetectObject\n,PickAndPlace\n). This ensures a modular and scalable design. - Focus on Integration: The code provided will be primarily \"glue code\" written in Python, connecting the various components of the VLA pipeline.\n4.2. External Dependencies\n- Software: Python 3, ROS 2, and the OpenAI Whisper library.\n- APIs: An API key for an LLM provider will be required for the cognitive planning chapter.\n- Hardware: A microphone for voice commands.\n- Prerequisites: Completion of prior modules, including ROS 2 basics, simulation, and navigation.\n--------------------------------------------------------------------------------"
  },
  {
    "url": "https://Muham251.github.io/ai-hackathon-1/docs/module-4/#11-goal",
    "text": "Spec: Module 4: Vision-Language-Action (VLA)\n1. Business Understanding\n1.1. Goal\nThe primary goal of this module is to teach students how to build a complete Vision-Language-Action (VLA) pipeline. This involves integrating Large Language Models (LLMs) with robotics, using Whisper for voice command input, employing LLMs for cognitive planning, and executing tasks through ROS 2 actions.\n1.2. Justification\nVLA is a transformative paradigm in robotics, enabling more natural human-robot interaction and a higher degree of autonomy. This module provides a crucial bridge between theoretical AI concepts (like LLMs) and practical, physical robotics, equipping students with highly sought-after skills in creating intelligent, responsive robots.\n1.3. Target Audience\n- Primary: Beginner to intermediate students in robotics and AI.\n- Secondary: Developers and researchers interested in the practical application of LLMs in autonomous systems.\n2. Functional Requirements\n2.1. In Scope (Features)\n- VLA Pipeline Overview: A clear, conceptual explanation of the entire Vision-Language-Action pipeline, from voice input to robot action.\n- Voice Command Processing: A tutorial on using OpenAI Whisper to transcribe spoken language into text and extract actionable commands.\n- LLM-based Planning: A guide on how to prompt an LLM to translate a high-level natural language task (e.g., \"get me the apple\") into a sequence of concrete ROS 2 action goals.\n- Mini-Capstone Project: A complete, runnable example demonstrating the full VLA loop. A user will issue a voice command, and a simulated humanoid robot will plan and execute a simple multi-step task involving navigation, perception, and manipulation.\n2.2. Out of Scope\n- The development of a production-grade, highly reliable robotics stack.\n- Training or fine-tuning custom Large Language Models.\n- Complex, long-horizon tasks or navigation in dynamic, multi-room environments.\n2.3. Success Criteria\n- Understanding: Students can draw a diagram of the VLA pipeline and explain the role of each component (Voice, LLM, ROS 2).\n- Application (Whisper): Students are able to run a demo that successfully converts a spoken command into a text string.\n- Application (LLM Planning): Students can explain how a natural language goal is decomposed into a JSON or YAML sequence of ROS 2 actions by an LLM.\n- Completion: The mini-capstone example runs end-to-end: a voice command triggers a sequence of autonomous actions in the simulated robot.\n2.4. Constraints\n- Word Count: 2000\u20133000 words.\n- Format: Docusaurus Markdown, including code snippets for Python (for Whisper and LLM interaction) and configuration files (for ROS 2 actions).\n- Sources: Content will be based on official OpenAI Whisper documentation, ROS 2 action design patterns, and foundational VLA research.\n- Timeline: 1 week.\n3. UX/UI (Reader Experience)\n3.1. Content Structure\nThe module will be structured to guide the learner from individual components to a fully integrated system.\n- Chapter 1: Voice-to-Action: Whisper and Command Extraction\n- Introduction to Whisper for speech-to-text.\n- Tutorial: Setting up Whisper and running a simple voice transcription demo.\n- Guide: Extracting key intents and entities from the transcribed text (e.g., turning \"Can you please find the red block\" into\n{action: \"find\", object: \"red block\"}\n).\n- Chapter 2: Cognitive Planning with LLMs\n- The role of LLMs as a \"reasoning engine\" for robots.\n- Tutorial: Prompt engineering techniques to make an LLM translate a task into a machine-readable plan (a sequence of ROS 2 actions).\n- Example: Converting\n{action: \"find\", object: \"red block\"}\ninto a plan like[{navigate: \"table\"}, {detect: \"red_block\"}]\n.\n- Chapter 3: Mini-Capstone: The Full VLA Loop\n- An overview of the complete system architecture.\n- A step-by-step guide to launching all components (Whisper listener, LLM planner, ROS 2 action servers).\n- A full demonstration of an autonomous task: e.g., \"Go to the table, find the blue cup, and pick it up.\"\n4. Technical Considerations\n4.1. Key Decisions\n- LLM Abstraction: We will use a major LLM provider's API (e.g., OpenAI, Anthropic, or Google) to avoid the complexity of self-hosting. The focus is on the integration, not the model itself.\n- ROS 2 Actions: The module will use the standard ROS 2 action interface for all robot tasks (e.g.,\nNavigateToPose\n,DetectObject\n,PickAndPlace\n). This ensures a modular and scalable design. - Focus on Integration: The code provided will be primarily \"glue code\" written in Python, connecting the various components of the VLA pipeline.\n4.2. External Dependencies\n- Software: Python 3, ROS 2, and the OpenAI Whisper library.\n- APIs: An API key for an LLM provider will be required for the cognitive planning chapter.\n- Hardware: A microphone for voice commands.\n- Prerequisites: Completion of prior modules, including ROS 2 basics, simulation, and navigation.\n--------------------------------------------------------------------------------"
  },
  {
    "url": "https://Muham251.github.io/ai-hackathon-1/docs/module-4/#12-justification",
    "text": "Spec: Module 4: Vision-Language-Action (VLA)\n1. Business Understanding\n1.1. Goal\nThe primary goal of this module is to teach students how to build a complete Vision-Language-Action (VLA) pipeline. This involves integrating Large Language Models (LLMs) with robotics, using Whisper for voice command input, employing LLMs for cognitive planning, and executing tasks through ROS 2 actions.\n1.2. Justification\nVLA is a transformative paradigm in robotics, enabling more natural human-robot interaction and a higher degree of autonomy. This module provides a crucial bridge between theoretical AI concepts (like LLMs) and practical, physical robotics, equipping students with highly sought-after skills in creating intelligent, responsive robots.\n1.3. Target Audience\n- Primary: Beginner to intermediate students in robotics and AI.\n- Secondary: Developers and researchers interested in the practical application of LLMs in autonomous systems.\n2. Functional Requirements\n2.1. In Scope (Features)\n- VLA Pipeline Overview: A clear, conceptual explanation of the entire Vision-Language-Action pipeline, from voice input to robot action.\n- Voice Command Processing: A tutorial on using OpenAI Whisper to transcribe spoken language into text and extract actionable commands.\n- LLM-based Planning: A guide on how to prompt an LLM to translate a high-level natural language task (e.g., \"get me the apple\") into a sequence of concrete ROS 2 action goals.\n- Mini-Capstone Project: A complete, runnable example demonstrating the full VLA loop. A user will issue a voice command, and a simulated humanoid robot will plan and execute a simple multi-step task involving navigation, perception, and manipulation.\n2.2. Out of Scope\n- The development of a production-grade, highly reliable robotics stack.\n- Training or fine-tuning custom Large Language Models.\n- Complex, long-horizon tasks or navigation in dynamic, multi-room environments.\n2.3. Success Criteria\n- Understanding: Students can draw a diagram of the VLA pipeline and explain the role of each component (Voice, LLM, ROS 2).\n- Application (Whisper): Students are able to run a demo that successfully converts a spoken command into a text string.\n- Application (LLM Planning): Students can explain how a natural language goal is decomposed into a JSON or YAML sequence of ROS 2 actions by an LLM.\n- Completion: The mini-capstone example runs end-to-end: a voice command triggers a sequence of autonomous actions in the simulated robot.\n2.4. Constraints\n- Word Count: 2000\u20133000 words.\n- Format: Docusaurus Markdown, including code snippets for Python (for Whisper and LLM interaction) and configuration files (for ROS 2 actions).\n- Sources: Content will be based on official OpenAI Whisper documentation, ROS 2 action design patterns, and foundational VLA research.\n- Timeline: 1 week.\n3. UX/UI (Reader Experience)\n3.1. Content Structure\nThe module will be structured to guide the learner from individual components to a fully integrated system.\n- Chapter 1: Voice-to-Action: Whisper and Command Extraction\n- Introduction to Whisper for speech-to-text.\n- Tutorial: Setting up Whisper and running a simple voice transcription demo.\n- Guide: Extracting key intents and entities from the transcribed text (e.g., turning \"Can you please find the red block\" into\n{action: \"find\", object: \"red block\"}\n).\n- Chapter 2: Cognitive Planning with LLMs\n- The role of LLMs as a \"reasoning engine\" for robots.\n- Tutorial: Prompt engineering techniques to make an LLM translate a task into a machine-readable plan (a sequence of ROS 2 actions).\n- Example: Converting\n{action: \"find\", object: \"red block\"}\ninto a plan like[{navigate: \"table\"}, {detect: \"red_block\"}]\n.\n- Chapter 3: Mini-Capstone: The Full VLA Loop\n- An overview of the complete system architecture.\n- A step-by-step guide to launching all components (Whisper listener, LLM planner, ROS 2 action servers).\n- A full demonstration of an autonomous task: e.g., \"Go to the table, find the blue cup, and pick it up.\"\n4. Technical Considerations\n4.1. Key Decisions\n- LLM Abstraction: We will use a major LLM provider's API (e.g., OpenAI, Anthropic, or Google) to avoid the complexity of self-hosting. The focus is on the integration, not the model itself.\n- ROS 2 Actions: The module will use the standard ROS 2 action interface for all robot tasks (e.g.,\nNavigateToPose\n,DetectObject\n,PickAndPlace\n). This ensures a modular and scalable design. - Focus on Integration: The code provided will be primarily \"glue code\" written in Python, connecting the various components of the VLA pipeline.\n4.2. External Dependencies\n- Software: Python 3, ROS 2, and the OpenAI Whisper library.\n- APIs: An API key for an LLM provider will be required for the cognitive planning chapter.\n- Hardware: A microphone for voice commands.\n- Prerequisites: Completion of prior modules, including ROS 2 basics, simulation, and navigation.\n--------------------------------------------------------------------------------"
  },
  {
    "url": "https://Muham251.github.io/ai-hackathon-1/docs/module-4/#13-target-audience",
    "text": "Spec: Module 4: Vision-Language-Action (VLA)\n1. Business Understanding\n1.1. Goal\nThe primary goal of this module is to teach students how to build a complete Vision-Language-Action (VLA) pipeline. This involves integrating Large Language Models (LLMs) with robotics, using Whisper for voice command input, employing LLMs for cognitive planning, and executing tasks through ROS 2 actions.\n1.2. Justification\nVLA is a transformative paradigm in robotics, enabling more natural human-robot interaction and a higher degree of autonomy. This module provides a crucial bridge between theoretical AI concepts (like LLMs) and practical, physical robotics, equipping students with highly sought-after skills in creating intelligent, responsive robots.\n1.3. Target Audience\n- Primary: Beginner to intermediate students in robotics and AI.\n- Secondary: Developers and researchers interested in the practical application of LLMs in autonomous systems.\n2. Functional Requirements\n2.1. In Scope (Features)\n- VLA Pipeline Overview: A clear, conceptual explanation of the entire Vision-Language-Action pipeline, from voice input to robot action.\n- Voice Command Processing: A tutorial on using OpenAI Whisper to transcribe spoken language into text and extract actionable commands.\n- LLM-based Planning: A guide on how to prompt an LLM to translate a high-level natural language task (e.g., \"get me the apple\") into a sequence of concrete ROS 2 action goals.\n- Mini-Capstone Project: A complete, runnable example demonstrating the full VLA loop. A user will issue a voice command, and a simulated humanoid robot will plan and execute a simple multi-step task involving navigation, perception, and manipulation.\n2.2. Out of Scope\n- The development of a production-grade, highly reliable robotics stack.\n- Training or fine-tuning custom Large Language Models.\n- Complex, long-horizon tasks or navigation in dynamic, multi-room environments.\n2.3. Success Criteria\n- Understanding: Students can draw a diagram of the VLA pipeline and explain the role of each component (Voice, LLM, ROS 2).\n- Application (Whisper): Students are able to run a demo that successfully converts a spoken command into a text string.\n- Application (LLM Planning): Students can explain how a natural language goal is decomposed into a JSON or YAML sequence of ROS 2 actions by an LLM.\n- Completion: The mini-capstone example runs end-to-end: a voice command triggers a sequence of autonomous actions in the simulated robot.\n2.4. Constraints\n- Word Count: 2000\u20133000 words.\n- Format: Docusaurus Markdown, including code snippets for Python (for Whisper and LLM interaction) and configuration files (for ROS 2 actions).\n- Sources: Content will be based on official OpenAI Whisper documentation, ROS 2 action design patterns, and foundational VLA research.\n- Timeline: 1 week.\n3. UX/UI (Reader Experience)\n3.1. Content Structure\nThe module will be structured to guide the learner from individual components to a fully integrated system.\n- Chapter 1: Voice-to-Action: Whisper and Command Extraction\n- Introduction to Whisper for speech-to-text.\n- Tutorial: Setting up Whisper and running a simple voice transcription demo.\n- Guide: Extracting key intents and entities from the transcribed text (e.g., turning \"Can you please find the red block\" into\n{action: \"find\", object: \"red block\"}\n).\n- Chapter 2: Cognitive Planning with LLMs\n- The role of LLMs as a \"reasoning engine\" for robots.\n- Tutorial: Prompt engineering techniques to make an LLM translate a task into a machine-readable plan (a sequence of ROS 2 actions).\n- Example: Converting\n{action: \"find\", object: \"red block\"}\ninto a plan like[{navigate: \"table\"}, {detect: \"red_block\"}]\n.\n- Chapter 3: Mini-Capstone: The Full VLA Loop\n- An overview of the complete system architecture.\n- A step-by-step guide to launching all components (Whisper listener, LLM planner, ROS 2 action servers).\n- A full demonstration of an autonomous task: e.g., \"Go to the table, find the blue cup, and pick it up.\"\n4. Technical Considerations\n4.1. Key Decisions\n- LLM Abstraction: We will use a major LLM provider's API (e.g., OpenAI, Anthropic, or Google) to avoid the complexity of self-hosting. The focus is on the integration, not the model itself.\n- ROS 2 Actions: The module will use the standard ROS 2 action interface for all robot tasks (e.g.,\nNavigateToPose\n,DetectObject\n,PickAndPlace\n). This ensures a modular and scalable design. - Focus on Integration: The code provided will be primarily \"glue code\" written in Python, connecting the various components of the VLA pipeline.\n4.2. External Dependencies\n- Software: Python 3, ROS 2, and the OpenAI Whisper library.\n- APIs: An API key for an LLM provider will be required for the cognitive planning chapter.\n- Hardware: A microphone for voice commands.\n- Prerequisites: Completion of prior modules, including ROS 2 basics, simulation, and navigation.\n--------------------------------------------------------------------------------"
  },
  {
    "url": "https://Muham251.github.io/ai-hackathon-1/docs/module-4/#2-functional-requirements",
    "text": "Spec: Module 4: Vision-Language-Action (VLA)\n1. Business Understanding\n1.1. Goal\nThe primary goal of this module is to teach students how to build a complete Vision-Language-Action (VLA) pipeline. This involves integrating Large Language Models (LLMs) with robotics, using Whisper for voice command input, employing LLMs for cognitive planning, and executing tasks through ROS 2 actions.\n1.2. Justification\nVLA is a transformative paradigm in robotics, enabling more natural human-robot interaction and a higher degree of autonomy. This module provides a crucial bridge between theoretical AI concepts (like LLMs) and practical, physical robotics, equipping students with highly sought-after skills in creating intelligent, responsive robots.\n1.3. Target Audience\n- Primary: Beginner to intermediate students in robotics and AI.\n- Secondary: Developers and researchers interested in the practical application of LLMs in autonomous systems.\n2. Functional Requirements\n2.1. In Scope (Features)\n- VLA Pipeline Overview: A clear, conceptual explanation of the entire Vision-Language-Action pipeline, from voice input to robot action.\n- Voice Command Processing: A tutorial on using OpenAI Whisper to transcribe spoken language into text and extract actionable commands.\n- LLM-based Planning: A guide on how to prompt an LLM to translate a high-level natural language task (e.g., \"get me the apple\") into a sequence of concrete ROS 2 action goals.\n- Mini-Capstone Project: A complete, runnable example demonstrating the full VLA loop. A user will issue a voice command, and a simulated humanoid robot will plan and execute a simple multi-step task involving navigation, perception, and manipulation.\n2.2. Out of Scope\n- The development of a production-grade, highly reliable robotics stack.\n- Training or fine-tuning custom Large Language Models.\n- Complex, long-horizon tasks or navigation in dynamic, multi-room environments.\n2.3. Success Criteria\n- Understanding: Students can draw a diagram of the VLA pipeline and explain the role of each component (Voice, LLM, ROS 2).\n- Application (Whisper): Students are able to run a demo that successfully converts a spoken command into a text string.\n- Application (LLM Planning): Students can explain how a natural language goal is decomposed into a JSON or YAML sequence of ROS 2 actions by an LLM.\n- Completion: The mini-capstone example runs end-to-end: a voice command triggers a sequence of autonomous actions in the simulated robot.\n2.4. Constraints\n- Word Count: 2000\u20133000 words.\n- Format: Docusaurus Markdown, including code snippets for Python (for Whisper and LLM interaction) and configuration files (for ROS 2 actions).\n- Sources: Content will be based on official OpenAI Whisper documentation, ROS 2 action design patterns, and foundational VLA research.\n- Timeline: 1 week.\n3. UX/UI (Reader Experience)\n3.1. Content Structure\nThe module will be structured to guide the learner from individual components to a fully integrated system.\n- Chapter 1: Voice-to-Action: Whisper and Command Extraction\n- Introduction to Whisper for speech-to-text.\n- Tutorial: Setting up Whisper and running a simple voice transcription demo.\n- Guide: Extracting key intents and entities from the transcribed text (e.g., turning \"Can you please find the red block\" into\n{action: \"find\", object: \"red block\"}\n).\n- Chapter 2: Cognitive Planning with LLMs\n- The role of LLMs as a \"reasoning engine\" for robots.\n- Tutorial: Prompt engineering techniques to make an LLM translate a task into a machine-readable plan (a sequence of ROS 2 actions).\n- Example: Converting\n{action: \"find\", object: \"red block\"}\ninto a plan like[{navigate: \"table\"}, {detect: \"red_block\"}]\n.\n- Chapter 3: Mini-Capstone: The Full VLA Loop\n- An overview of the complete system architecture.\n- A step-by-step guide to launching all components (Whisper listener, LLM planner, ROS 2 action servers).\n- A full demonstration of an autonomous task: e.g., \"Go to the table, find the blue cup, and pick it up.\"\n4. Technical Considerations\n4.1. Key Decisions\n- LLM Abstraction: We will use a major LLM provider's API (e.g., OpenAI, Anthropic, or Google) to avoid the complexity of self-hosting. The focus is on the integration, not the model itself.\n- ROS 2 Actions: The module will use the standard ROS 2 action interface for all robot tasks (e.g.,\nNavigateToPose\n,DetectObject\n,PickAndPlace\n). This ensures a modular and scalable design. - Focus on Integration: The code provided will be primarily \"glue code\" written in Python, connecting the various components of the VLA pipeline.\n4.2. External Dependencies\n- Software: Python 3, ROS 2, and the OpenAI Whisper library.\n- APIs: An API key for an LLM provider will be required for the cognitive planning chapter.\n- Hardware: A microphone for voice commands.\n- Prerequisites: Completion of prior modules, including ROS 2 basics, simulation, and navigation.\n--------------------------------------------------------------------------------"
  },
  {
    "url": "https://Muham251.github.io/ai-hackathon-1/docs/module-4/#21-in-scope-features",
    "text": "Spec: Module 4: Vision-Language-Action (VLA)\n1. Business Understanding\n1.1. Goal\nThe primary goal of this module is to teach students how to build a complete Vision-Language-Action (VLA) pipeline. This involves integrating Large Language Models (LLMs) with robotics, using Whisper for voice command input, employing LLMs for cognitive planning, and executing tasks through ROS 2 actions.\n1.2. Justification\nVLA is a transformative paradigm in robotics, enabling more natural human-robot interaction and a higher degree of autonomy. This module provides a crucial bridge between theoretical AI concepts (like LLMs) and practical, physical robotics, equipping students with highly sought-after skills in creating intelligent, responsive robots.\n1.3. Target Audience\n- Primary: Beginner to intermediate students in robotics and AI.\n- Secondary: Developers and researchers interested in the practical application of LLMs in autonomous systems.\n2. Functional Requirements\n2.1. In Scope (Features)\n- VLA Pipeline Overview: A clear, conceptual explanation of the entire Vision-Language-Action pipeline, from voice input to robot action.\n- Voice Command Processing: A tutorial on using OpenAI Whisper to transcribe spoken language into text and extract actionable commands.\n- LLM-based Planning: A guide on how to prompt an LLM to translate a high-level natural language task (e.g., \"get me the apple\") into a sequence of concrete ROS 2 action goals.\n- Mini-Capstone Project: A complete, runnable example demonstrating the full VLA loop. A user will issue a voice command, and a simulated humanoid robot will plan and execute a simple multi-step task involving navigation, perception, and manipulation.\n2.2. Out of Scope\n- The development of a production-grade, highly reliable robotics stack.\n- Training or fine-tuning custom Large Language Models.\n- Complex, long-horizon tasks or navigation in dynamic, multi-room environments.\n2.3. Success Criteria\n- Understanding: Students can draw a diagram of the VLA pipeline and explain the role of each component (Voice, LLM, ROS 2).\n- Application (Whisper): Students are able to run a demo that successfully converts a spoken command into a text string.\n- Application (LLM Planning): Students can explain how a natural language goal is decomposed into a JSON or YAML sequence of ROS 2 actions by an LLM.\n- Completion: The mini-capstone example runs end-to-end: a voice command triggers a sequence of autonomous actions in the simulated robot.\n2.4. Constraints\n- Word Count: 2000\u20133000 words.\n- Format: Docusaurus Markdown, including code snippets for Python (for Whisper and LLM interaction) and configuration files (for ROS 2 actions).\n- Sources: Content will be based on official OpenAI Whisper documentation, ROS 2 action design patterns, and foundational VLA research.\n- Timeline: 1 week.\n3. UX/UI (Reader Experience)\n3.1. Content Structure\nThe module will be structured to guide the learner from individual components to a fully integrated system.\n- Chapter 1: Voice-to-Action: Whisper and Command Extraction\n- Introduction to Whisper for speech-to-text.\n- Tutorial: Setting up Whisper and running a simple voice transcription demo.\n- Guide: Extracting key intents and entities from the transcribed text (e.g., turning \"Can you please find the red block\" into\n{action: \"find\", object: \"red block\"}\n).\n- Chapter 2: Cognitive Planning with LLMs\n- The role of LLMs as a \"reasoning engine\" for robots.\n- Tutorial: Prompt engineering techniques to make an LLM translate a task into a machine-readable plan (a sequence of ROS 2 actions).\n- Example: Converting\n{action: \"find\", object: \"red block\"}\ninto a plan like[{navigate: \"table\"}, {detect: \"red_block\"}]\n.\n- Chapter 3: Mini-Capstone: The Full VLA Loop\n- An overview of the complete system architecture.\n- A step-by-step guide to launching all components (Whisper listener, LLM planner, ROS 2 action servers).\n- A full demonstration of an autonomous task: e.g., \"Go to the table, find the blue cup, and pick it up.\"\n4. Technical Considerations\n4.1. Key Decisions\n- LLM Abstraction: We will use a major LLM provider's API (e.g., OpenAI, Anthropic, or Google) to avoid the complexity of self-hosting. The focus is on the integration, not the model itself.\n- ROS 2 Actions: The module will use the standard ROS 2 action interface for all robot tasks (e.g.,\nNavigateToPose\n,DetectObject\n,PickAndPlace\n). This ensures a modular and scalable design. - Focus on Integration: The code provided will be primarily \"glue code\" written in Python, connecting the various components of the VLA pipeline.\n4.2. External Dependencies\n- Software: Python 3, ROS 2, and the OpenAI Whisper library.\n- APIs: An API key for an LLM provider will be required for the cognitive planning chapter.\n- Hardware: A microphone for voice commands.\n- Prerequisites: Completion of prior modules, including ROS 2 basics, simulation, and navigation.\n--------------------------------------------------------------------------------"
  },
  {
    "url": "https://Muham251.github.io/ai-hackathon-1/docs/module-4/#22-out-of-scope",
    "text": "Spec: Module 4: Vision-Language-Action (VLA)\n1. Business Understanding\n1.1. Goal\nThe primary goal of this module is to teach students how to build a complete Vision-Language-Action (VLA) pipeline. This involves integrating Large Language Models (LLMs) with robotics, using Whisper for voice command input, employing LLMs for cognitive planning, and executing tasks through ROS 2 actions.\n1.2. Justification\nVLA is a transformative paradigm in robotics, enabling more natural human-robot interaction and a higher degree of autonomy. This module provides a crucial bridge between theoretical AI concepts (like LLMs) and practical, physical robotics, equipping students with highly sought-after skills in creating intelligent, responsive robots.\n1.3. Target Audience\n- Primary: Beginner to intermediate students in robotics and AI.\n- Secondary: Developers and researchers interested in the practical application of LLMs in autonomous systems.\n2. Functional Requirements\n2.1. In Scope (Features)\n- VLA Pipeline Overview: A clear, conceptual explanation of the entire Vision-Language-Action pipeline, from voice input to robot action.\n- Voice Command Processing: A tutorial on using OpenAI Whisper to transcribe spoken language into text and extract actionable commands.\n- LLM-based Planning: A guide on how to prompt an LLM to translate a high-level natural language task (e.g., \"get me the apple\") into a sequence of concrete ROS 2 action goals.\n- Mini-Capstone Project: A complete, runnable example demonstrating the full VLA loop. A user will issue a voice command, and a simulated humanoid robot will plan and execute a simple multi-step task involving navigation, perception, and manipulation.\n2.2. Out of Scope\n- The development of a production-grade, highly reliable robotics stack.\n- Training or fine-tuning custom Large Language Models.\n- Complex, long-horizon tasks or navigation in dynamic, multi-room environments.\n2.3. Success Criteria\n- Understanding: Students can draw a diagram of the VLA pipeline and explain the role of each component (Voice, LLM, ROS 2).\n- Application (Whisper): Students are able to run a demo that successfully converts a spoken command into a text string.\n- Application (LLM Planning): Students can explain how a natural language goal is decomposed into a JSON or YAML sequence of ROS 2 actions by an LLM.\n- Completion: The mini-capstone example runs end-to-end: a voice command triggers a sequence of autonomous actions in the simulated robot.\n2.4. Constraints\n- Word Count: 2000\u20133000 words.\n- Format: Docusaurus Markdown, including code snippets for Python (for Whisper and LLM interaction) and configuration files (for ROS 2 actions).\n- Sources: Content will be based on official OpenAI Whisper documentation, ROS 2 action design patterns, and foundational VLA research.\n- Timeline: 1 week.\n3. UX/UI (Reader Experience)\n3.1. Content Structure\nThe module will be structured to guide the learner from individual components to a fully integrated system.\n- Chapter 1: Voice-to-Action: Whisper and Command Extraction\n- Introduction to Whisper for speech-to-text.\n- Tutorial: Setting up Whisper and running a simple voice transcription demo.\n- Guide: Extracting key intents and entities from the transcribed text (e.g., turning \"Can you please find the red block\" into\n{action: \"find\", object: \"red block\"}\n).\n- Chapter 2: Cognitive Planning with LLMs\n- The role of LLMs as a \"reasoning engine\" for robots.\n- Tutorial: Prompt engineering techniques to make an LLM translate a task into a machine-readable plan (a sequence of ROS 2 actions).\n- Example: Converting\n{action: \"find\", object: \"red block\"}\ninto a plan like[{navigate: \"table\"}, {detect: \"red_block\"}]\n.\n- Chapter 3: Mini-Capstone: The Full VLA Loop\n- An overview of the complete system architecture.\n- A step-by-step guide to launching all components (Whisper listener, LLM planner, ROS 2 action servers).\n- A full demonstration of an autonomous task: e.g., \"Go to the table, find the blue cup, and pick it up.\"\n4. Technical Considerations\n4.1. Key Decisions\n- LLM Abstraction: We will use a major LLM provider's API (e.g., OpenAI, Anthropic, or Google) to avoid the complexity of self-hosting. The focus is on the integration, not the model itself.\n- ROS 2 Actions: The module will use the standard ROS 2 action interface for all robot tasks (e.g.,\nNavigateToPose\n,DetectObject\n,PickAndPlace\n). This ensures a modular and scalable design. - Focus on Integration: The code provided will be primarily \"glue code\" written in Python, connecting the various components of the VLA pipeline.\n4.2. External Dependencies\n- Software: Python 3, ROS 2, and the OpenAI Whisper library.\n- APIs: An API key for an LLM provider will be required for the cognitive planning chapter.\n- Hardware: A microphone for voice commands.\n- Prerequisites: Completion of prior modules, including ROS 2 basics, simulation, and navigation.\n--------------------------------------------------------------------------------"
  },
  {
    "url": "https://Muham251.github.io/ai-hackathon-1/docs/module-4/#23-success-criteria",
    "text": "Spec: Module 4: Vision-Language-Action (VLA)\n1. Business Understanding\n1.1. Goal\nThe primary goal of this module is to teach students how to build a complete Vision-Language-Action (VLA) pipeline. This involves integrating Large Language Models (LLMs) with robotics, using Whisper for voice command input, employing LLMs for cognitive planning, and executing tasks through ROS 2 actions.\n1.2. Justification\nVLA is a transformative paradigm in robotics, enabling more natural human-robot interaction and a higher degree of autonomy. This module provides a crucial bridge between theoretical AI concepts (like LLMs) and practical, physical robotics, equipping students with highly sought-after skills in creating intelligent, responsive robots.\n1.3. Target Audience\n- Primary: Beginner to intermediate students in robotics and AI.\n- Secondary: Developers and researchers interested in the practical application of LLMs in autonomous systems.\n2. Functional Requirements\n2.1. In Scope (Features)\n- VLA Pipeline Overview: A clear, conceptual explanation of the entire Vision-Language-Action pipeline, from voice input to robot action.\n- Voice Command Processing: A tutorial on using OpenAI Whisper to transcribe spoken language into text and extract actionable commands.\n- LLM-based Planning: A guide on how to prompt an LLM to translate a high-level natural language task (e.g., \"get me the apple\") into a sequence of concrete ROS 2 action goals.\n- Mini-Capstone Project: A complete, runnable example demonstrating the full VLA loop. A user will issue a voice command, and a simulated humanoid robot will plan and execute a simple multi-step task involving navigation, perception, and manipulation.\n2.2. Out of Scope\n- The development of a production-grade, highly reliable robotics stack.\n- Training or fine-tuning custom Large Language Models.\n- Complex, long-horizon tasks or navigation in dynamic, multi-room environments.\n2.3. Success Criteria\n- Understanding: Students can draw a diagram of the VLA pipeline and explain the role of each component (Voice, LLM, ROS 2).\n- Application (Whisper): Students are able to run a demo that successfully converts a spoken command into a text string.\n- Application (LLM Planning): Students can explain how a natural language goal is decomposed into a JSON or YAML sequence of ROS 2 actions by an LLM.\n- Completion: The mini-capstone example runs end-to-end: a voice command triggers a sequence of autonomous actions in the simulated robot.\n2.4. Constraints\n- Word Count: 2000\u20133000 words.\n- Format: Docusaurus Markdown, including code snippets for Python (for Whisper and LLM interaction) and configuration files (for ROS 2 actions).\n- Sources: Content will be based on official OpenAI Whisper documentation, ROS 2 action design patterns, and foundational VLA research.\n- Timeline: 1 week.\n3. UX/UI (Reader Experience)\n3.1. Content Structure\nThe module will be structured to guide the learner from individual components to a fully integrated system.\n- Chapter 1: Voice-to-Action: Whisper and Command Extraction\n- Introduction to Whisper for speech-to-text.\n- Tutorial: Setting up Whisper and running a simple voice transcription demo.\n- Guide: Extracting key intents and entities from the transcribed text (e.g., turning \"Can you please find the red block\" into\n{action: \"find\", object: \"red block\"}\n).\n- Chapter 2: Cognitive Planning with LLMs\n- The role of LLMs as a \"reasoning engine\" for robots.\n- Tutorial: Prompt engineering techniques to make an LLM translate a task into a machine-readable plan (a sequence of ROS 2 actions).\n- Example: Converting\n{action: \"find\", object: \"red block\"}\ninto a plan like[{navigate: \"table\"}, {detect: \"red_block\"}]\n.\n- Chapter 3: Mini-Capstone: The Full VLA Loop\n- An overview of the complete system architecture.\n- A step-by-step guide to launching all components (Whisper listener, LLM planner, ROS 2 action servers).\n- A full demonstration of an autonomous task: e.g., \"Go to the table, find the blue cup, and pick it up.\"\n4. Technical Considerations\n4.1. Key Decisions\n- LLM Abstraction: We will use a major LLM provider's API (e.g., OpenAI, Anthropic, or Google) to avoid the complexity of self-hosting. The focus is on the integration, not the model itself.\n- ROS 2 Actions: The module will use the standard ROS 2 action interface for all robot tasks (e.g.,\nNavigateToPose\n,DetectObject\n,PickAndPlace\n). This ensures a modular and scalable design. - Focus on Integration: The code provided will be primarily \"glue code\" written in Python, connecting the various components of the VLA pipeline.\n4.2. External Dependencies\n- Software: Python 3, ROS 2, and the OpenAI Whisper library.\n- APIs: An API key for an LLM provider will be required for the cognitive planning chapter.\n- Hardware: A microphone for voice commands.\n- Prerequisites: Completion of prior modules, including ROS 2 basics, simulation, and navigation.\n--------------------------------------------------------------------------------"
  },
  {
    "url": "https://Muham251.github.io/ai-hackathon-1/docs/module-4/#24-constraints",
    "text": "Spec: Module 4: Vision-Language-Action (VLA)\n1. Business Understanding\n1.1. Goal\nThe primary goal of this module is to teach students how to build a complete Vision-Language-Action (VLA) pipeline. This involves integrating Large Language Models (LLMs) with robotics, using Whisper for voice command input, employing LLMs for cognitive planning, and executing tasks through ROS 2 actions.\n1.2. Justification\nVLA is a transformative paradigm in robotics, enabling more natural human-robot interaction and a higher degree of autonomy. This module provides a crucial bridge between theoretical AI concepts (like LLMs) and practical, physical robotics, equipping students with highly sought-after skills in creating intelligent, responsive robots.\n1.3. Target Audience\n- Primary: Beginner to intermediate students in robotics and AI.\n- Secondary: Developers and researchers interested in the practical application of LLMs in autonomous systems.\n2. Functional Requirements\n2.1. In Scope (Features)\n- VLA Pipeline Overview: A clear, conceptual explanation of the entire Vision-Language-Action pipeline, from voice input to robot action.\n- Voice Command Processing: A tutorial on using OpenAI Whisper to transcribe spoken language into text and extract actionable commands.\n- LLM-based Planning: A guide on how to prompt an LLM to translate a high-level natural language task (e.g., \"get me the apple\") into a sequence of concrete ROS 2 action goals.\n- Mini-Capstone Project: A complete, runnable example demonstrating the full VLA loop. A user will issue a voice command, and a simulated humanoid robot will plan and execute a simple multi-step task involving navigation, perception, and manipulation.\n2.2. Out of Scope\n- The development of a production-grade, highly reliable robotics stack.\n- Training or fine-tuning custom Large Language Models.\n- Complex, long-horizon tasks or navigation in dynamic, multi-room environments.\n2.3. Success Criteria\n- Understanding: Students can draw a diagram of the VLA pipeline and explain the role of each component (Voice, LLM, ROS 2).\n- Application (Whisper): Students are able to run a demo that successfully converts a spoken command into a text string.\n- Application (LLM Planning): Students can explain how a natural language goal is decomposed into a JSON or YAML sequence of ROS 2 actions by an LLM.\n- Completion: The mini-capstone example runs end-to-end: a voice command triggers a sequence of autonomous actions in the simulated robot.\n2.4. Constraints\n- Word Count: 2000\u20133000 words.\n- Format: Docusaurus Markdown, including code snippets for Python (for Whisper and LLM interaction) and configuration files (for ROS 2 actions).\n- Sources: Content will be based on official OpenAI Whisper documentation, ROS 2 action design patterns, and foundational VLA research.\n- Timeline: 1 week.\n3. UX/UI (Reader Experience)\n3.1. Content Structure\nThe module will be structured to guide the learner from individual components to a fully integrated system.\n- Chapter 1: Voice-to-Action: Whisper and Command Extraction\n- Introduction to Whisper for speech-to-text.\n- Tutorial: Setting up Whisper and running a simple voice transcription demo.\n- Guide: Extracting key intents and entities from the transcribed text (e.g., turning \"Can you please find the red block\" into\n{action: \"find\", object: \"red block\"}\n).\n- Chapter 2: Cognitive Planning with LLMs\n- The role of LLMs as a \"reasoning engine\" for robots.\n- Tutorial: Prompt engineering techniques to make an LLM translate a task into a machine-readable plan (a sequence of ROS 2 actions).\n- Example: Converting\n{action: \"find\", object: \"red block\"}\ninto a plan like[{navigate: \"table\"}, {detect: \"red_block\"}]\n.\n- Chapter 3: Mini-Capstone: The Full VLA Loop\n- An overview of the complete system architecture.\n- A step-by-step guide to launching all components (Whisper listener, LLM planner, ROS 2 action servers).\n- A full demonstration of an autonomous task: e.g., \"Go to the table, find the blue cup, and pick it up.\"\n4. Technical Considerations\n4.1. Key Decisions\n- LLM Abstraction: We will use a major LLM provider's API (e.g., OpenAI, Anthropic, or Google) to avoid the complexity of self-hosting. The focus is on the integration, not the model itself.\n- ROS 2 Actions: The module will use the standard ROS 2 action interface for all robot tasks (e.g.,\nNavigateToPose\n,DetectObject\n,PickAndPlace\n). This ensures a modular and scalable design. - Focus on Integration: The code provided will be primarily \"glue code\" written in Python, connecting the various components of the VLA pipeline.\n4.2. External Dependencies\n- Software: Python 3, ROS 2, and the OpenAI Whisper library.\n- APIs: An API key for an LLM provider will be required for the cognitive planning chapter.\n- Hardware: A microphone for voice commands.\n- Prerequisites: Completion of prior modules, including ROS 2 basics, simulation, and navigation.\n--------------------------------------------------------------------------------"
  },
  {
    "url": "https://Muham251.github.io/ai-hackathon-1/docs/module-4/#3-uxui-reader-experience",
    "text": "Spec: Module 4: Vision-Language-Action (VLA)\n1. Business Understanding\n1.1. Goal\nThe primary goal of this module is to teach students how to build a complete Vision-Language-Action (VLA) pipeline. This involves integrating Large Language Models (LLMs) with robotics, using Whisper for voice command input, employing LLMs for cognitive planning, and executing tasks through ROS 2 actions.\n1.2. Justification\nVLA is a transformative paradigm in robotics, enabling more natural human-robot interaction and a higher degree of autonomy. This module provides a crucial bridge between theoretical AI concepts (like LLMs) and practical, physical robotics, equipping students with highly sought-after skills in creating intelligent, responsive robots.\n1.3. Target Audience\n- Primary: Beginner to intermediate students in robotics and AI.\n- Secondary: Developers and researchers interested in the practical application of LLMs in autonomous systems.\n2. Functional Requirements\n2.1. In Scope (Features)\n- VLA Pipeline Overview: A clear, conceptual explanation of the entire Vision-Language-Action pipeline, from voice input to robot action.\n- Voice Command Processing: A tutorial on using OpenAI Whisper to transcribe spoken language into text and extract actionable commands.\n- LLM-based Planning: A guide on how to prompt an LLM to translate a high-level natural language task (e.g., \"get me the apple\") into a sequence of concrete ROS 2 action goals.\n- Mini-Capstone Project: A complete, runnable example demonstrating the full VLA loop. A user will issue a voice command, and a simulated humanoid robot will plan and execute a simple multi-step task involving navigation, perception, and manipulation.\n2.2. Out of Scope\n- The development of a production-grade, highly reliable robotics stack.\n- Training or fine-tuning custom Large Language Models.\n- Complex, long-horizon tasks or navigation in dynamic, multi-room environments.\n2.3. Success Criteria\n- Understanding: Students can draw a diagram of the VLA pipeline and explain the role of each component (Voice, LLM, ROS 2).\n- Application (Whisper): Students are able to run a demo that successfully converts a spoken command into a text string.\n- Application (LLM Planning): Students can explain how a natural language goal is decomposed into a JSON or YAML sequence of ROS 2 actions by an LLM.\n- Completion: The mini-capstone example runs end-to-end: a voice command triggers a sequence of autonomous actions in the simulated robot.\n2.4. Constraints\n- Word Count: 2000\u20133000 words.\n- Format: Docusaurus Markdown, including code snippets for Python (for Whisper and LLM interaction) and configuration files (for ROS 2 actions).\n- Sources: Content will be based on official OpenAI Whisper documentation, ROS 2 action design patterns, and foundational VLA research.\n- Timeline: 1 week.\n3. UX/UI (Reader Experience)\n3.1. Content Structure\nThe module will be structured to guide the learner from individual components to a fully integrated system.\n- Chapter 1: Voice-to-Action: Whisper and Command Extraction\n- Introduction to Whisper for speech-to-text.\n- Tutorial: Setting up Whisper and running a simple voice transcription demo.\n- Guide: Extracting key intents and entities from the transcribed text (e.g., turning \"Can you please find the red block\" into\n{action: \"find\", object: \"red block\"}\n).\n- Chapter 2: Cognitive Planning with LLMs\n- The role of LLMs as a \"reasoning engine\" for robots.\n- Tutorial: Prompt engineering techniques to make an LLM translate a task into a machine-readable plan (a sequence of ROS 2 actions).\n- Example: Converting\n{action: \"find\", object: \"red block\"}\ninto a plan like[{navigate: \"table\"}, {detect: \"red_block\"}]\n.\n- Chapter 3: Mini-Capstone: The Full VLA Loop\n- An overview of the complete system architecture.\n- A step-by-step guide to launching all components (Whisper listener, LLM planner, ROS 2 action servers).\n- A full demonstration of an autonomous task: e.g., \"Go to the table, find the blue cup, and pick it up.\"\n4. Technical Considerations\n4.1. Key Decisions\n- LLM Abstraction: We will use a major LLM provider's API (e.g., OpenAI, Anthropic, or Google) to avoid the complexity of self-hosting. The focus is on the integration, not the model itself.\n- ROS 2 Actions: The module will use the standard ROS 2 action interface for all robot tasks (e.g.,\nNavigateToPose\n,DetectObject\n,PickAndPlace\n). This ensures a modular and scalable design. - Focus on Integration: The code provided will be primarily \"glue code\" written in Python, connecting the various components of the VLA pipeline.\n4.2. External Dependencies\n- Software: Python 3, ROS 2, and the OpenAI Whisper library.\n- APIs: An API key for an LLM provider will be required for the cognitive planning chapter.\n- Hardware: A microphone for voice commands.\n- Prerequisites: Completion of prior modules, including ROS 2 basics, simulation, and navigation.\n--------------------------------------------------------------------------------"
  },
  {
    "url": "https://Muham251.github.io/ai-hackathon-1/docs/module-4/#31-content-structure",
    "text": "Spec: Module 4: Vision-Language-Action (VLA)\n1. Business Understanding\n1.1. Goal\nThe primary goal of this module is to teach students how to build a complete Vision-Language-Action (VLA) pipeline. This involves integrating Large Language Models (LLMs) with robotics, using Whisper for voice command input, employing LLMs for cognitive planning, and executing tasks through ROS 2 actions.\n1.2. Justification\nVLA is a transformative paradigm in robotics, enabling more natural human-robot interaction and a higher degree of autonomy. This module provides a crucial bridge between theoretical AI concepts (like LLMs) and practical, physical robotics, equipping students with highly sought-after skills in creating intelligent, responsive robots.\n1.3. Target Audience\n- Primary: Beginner to intermediate students in robotics and AI.\n- Secondary: Developers and researchers interested in the practical application of LLMs in autonomous systems.\n2. Functional Requirements\n2.1. In Scope (Features)\n- VLA Pipeline Overview: A clear, conceptual explanation of the entire Vision-Language-Action pipeline, from voice input to robot action.\n- Voice Command Processing: A tutorial on using OpenAI Whisper to transcribe spoken language into text and extract actionable commands.\n- LLM-based Planning: A guide on how to prompt an LLM to translate a high-level natural language task (e.g., \"get me the apple\") into a sequence of concrete ROS 2 action goals.\n- Mini-Capstone Project: A complete, runnable example demonstrating the full VLA loop. A user will issue a voice command, and a simulated humanoid robot will plan and execute a simple multi-step task involving navigation, perception, and manipulation.\n2.2. Out of Scope\n- The development of a production-grade, highly reliable robotics stack.\n- Training or fine-tuning custom Large Language Models.\n- Complex, long-horizon tasks or navigation in dynamic, multi-room environments.\n2.3. Success Criteria\n- Understanding: Students can draw a diagram of the VLA pipeline and explain the role of each component (Voice, LLM, ROS 2).\n- Application (Whisper): Students are able to run a demo that successfully converts a spoken command into a text string.\n- Application (LLM Planning): Students can explain how a natural language goal is decomposed into a JSON or YAML sequence of ROS 2 actions by an LLM.\n- Completion: The mini-capstone example runs end-to-end: a voice command triggers a sequence of autonomous actions in the simulated robot.\n2.4. Constraints\n- Word Count: 2000\u20133000 words.\n- Format: Docusaurus Markdown, including code snippets for Python (for Whisper and LLM interaction) and configuration files (for ROS 2 actions).\n- Sources: Content will be based on official OpenAI Whisper documentation, ROS 2 action design patterns, and foundational VLA research.\n- Timeline: 1 week.\n3. UX/UI (Reader Experience)\n3.1. Content Structure\nThe module will be structured to guide the learner from individual components to a fully integrated system.\n- Chapter 1: Voice-to-Action: Whisper and Command Extraction\n- Introduction to Whisper for speech-to-text.\n- Tutorial: Setting up Whisper and running a simple voice transcription demo.\n- Guide: Extracting key intents and entities from the transcribed text (e.g., turning \"Can you please find the red block\" into\n{action: \"find\", object: \"red block\"}\n).\n- Chapter 2: Cognitive Planning with LLMs\n- The role of LLMs as a \"reasoning engine\" for robots.\n- Tutorial: Prompt engineering techniques to make an LLM translate a task into a machine-readable plan (a sequence of ROS 2 actions).\n- Example: Converting\n{action: \"find\", object: \"red block\"}\ninto a plan like[{navigate: \"table\"}, {detect: \"red_block\"}]\n.\n- Chapter 3: Mini-Capstone: The Full VLA Loop\n- An overview of the complete system architecture.\n- A step-by-step guide to launching all components (Whisper listener, LLM planner, ROS 2 action servers).\n- A full demonstration of an autonomous task: e.g., \"Go to the table, find the blue cup, and pick it up.\"\n4. Technical Considerations\n4.1. Key Decisions\n- LLM Abstraction: We will use a major LLM provider's API (e.g., OpenAI, Anthropic, or Google) to avoid the complexity of self-hosting. The focus is on the integration, not the model itself.\n- ROS 2 Actions: The module will use the standard ROS 2 action interface for all robot tasks (e.g.,\nNavigateToPose\n,DetectObject\n,PickAndPlace\n). This ensures a modular and scalable design. - Focus on Integration: The code provided will be primarily \"glue code\" written in Python, connecting the various components of the VLA pipeline.\n4.2. External Dependencies\n- Software: Python 3, ROS 2, and the OpenAI Whisper library.\n- APIs: An API key for an LLM provider will be required for the cognitive planning chapter.\n- Hardware: A microphone for voice commands.\n- Prerequisites: Completion of prior modules, including ROS 2 basics, simulation, and navigation.\n--------------------------------------------------------------------------------"
  },
  {
    "url": "https://Muham251.github.io/ai-hackathon-1/docs/module-4/#4-technical-considerations",
    "text": "Spec: Module 4: Vision-Language-Action (VLA)\n1. Business Understanding\n1.1. Goal\nThe primary goal of this module is to teach students how to build a complete Vision-Language-Action (VLA) pipeline. This involves integrating Large Language Models (LLMs) with robotics, using Whisper for voice command input, employing LLMs for cognitive planning, and executing tasks through ROS 2 actions.\n1.2. Justification\nVLA is a transformative paradigm in robotics, enabling more natural human-robot interaction and a higher degree of autonomy. This module provides a crucial bridge between theoretical AI concepts (like LLMs) and practical, physical robotics, equipping students with highly sought-after skills in creating intelligent, responsive robots.\n1.3. Target Audience\n- Primary: Beginner to intermediate students in robotics and AI.\n- Secondary: Developers and researchers interested in the practical application of LLMs in autonomous systems.\n2. Functional Requirements\n2.1. In Scope (Features)\n- VLA Pipeline Overview: A clear, conceptual explanation of the entire Vision-Language-Action pipeline, from voice input to robot action.\n- Voice Command Processing: A tutorial on using OpenAI Whisper to transcribe spoken language into text and extract actionable commands.\n- LLM-based Planning: A guide on how to prompt an LLM to translate a high-level natural language task (e.g., \"get me the apple\") into a sequence of concrete ROS 2 action goals.\n- Mini-Capstone Project: A complete, runnable example demonstrating the full VLA loop. A user will issue a voice command, and a simulated humanoid robot will plan and execute a simple multi-step task involving navigation, perception, and manipulation.\n2.2. Out of Scope\n- The development of a production-grade, highly reliable robotics stack.\n- Training or fine-tuning custom Large Language Models.\n- Complex, long-horizon tasks or navigation in dynamic, multi-room environments.\n2.3. Success Criteria\n- Understanding: Students can draw a diagram of the VLA pipeline and explain the role of each component (Voice, LLM, ROS 2).\n- Application (Whisper): Students are able to run a demo that successfully converts a spoken command into a text string.\n- Application (LLM Planning): Students can explain how a natural language goal is decomposed into a JSON or YAML sequence of ROS 2 actions by an LLM.\n- Completion: The mini-capstone example runs end-to-end: a voice command triggers a sequence of autonomous actions in the simulated robot.\n2.4. Constraints\n- Word Count: 2000\u20133000 words.\n- Format: Docusaurus Markdown, including code snippets for Python (for Whisper and LLM interaction) and configuration files (for ROS 2 actions).\n- Sources: Content will be based on official OpenAI Whisper documentation, ROS 2 action design patterns, and foundational VLA research.\n- Timeline: 1 week.\n3. UX/UI (Reader Experience)\n3.1. Content Structure\nThe module will be structured to guide the learner from individual components to a fully integrated system.\n- Chapter 1: Voice-to-Action: Whisper and Command Extraction\n- Introduction to Whisper for speech-to-text.\n- Tutorial: Setting up Whisper and running a simple voice transcription demo.\n- Guide: Extracting key intents and entities from the transcribed text (e.g., turning \"Can you please find the red block\" into\n{action: \"find\", object: \"red block\"}\n).\n- Chapter 2: Cognitive Planning with LLMs\n- The role of LLMs as a \"reasoning engine\" for robots.\n- Tutorial: Prompt engineering techniques to make an LLM translate a task into a machine-readable plan (a sequence of ROS 2 actions).\n- Example: Converting\n{action: \"find\", object: \"red block\"}\ninto a plan like[{navigate: \"table\"}, {detect: \"red_block\"}]\n.\n- Chapter 3: Mini-Capstone: The Full VLA Loop\n- An overview of the complete system architecture.\n- A step-by-step guide to launching all components (Whisper listener, LLM planner, ROS 2 action servers).\n- A full demonstration of an autonomous task: e.g., \"Go to the table, find the blue cup, and pick it up.\"\n4. Technical Considerations\n4.1. Key Decisions\n- LLM Abstraction: We will use a major LLM provider's API (e.g., OpenAI, Anthropic, or Google) to avoid the complexity of self-hosting. The focus is on the integration, not the model itself.\n- ROS 2 Actions: The module will use the standard ROS 2 action interface for all robot tasks (e.g.,\nNavigateToPose\n,DetectObject\n,PickAndPlace\n). This ensures a modular and scalable design. - Focus on Integration: The code provided will be primarily \"glue code\" written in Python, connecting the various components of the VLA pipeline.\n4.2. External Dependencies\n- Software: Python 3, ROS 2, and the OpenAI Whisper library.\n- APIs: An API key for an LLM provider will be required for the cognitive planning chapter.\n- Hardware: A microphone for voice commands.\n- Prerequisites: Completion of prior modules, including ROS 2 basics, simulation, and navigation.\n--------------------------------------------------------------------------------"
  },
  {
    "url": "https://Muham251.github.io/ai-hackathon-1/docs/module-4/#41-key-decisions",
    "text": "Spec: Module 4: Vision-Language-Action (VLA)\n1. Business Understanding\n1.1. Goal\nThe primary goal of this module is to teach students how to build a complete Vision-Language-Action (VLA) pipeline. This involves integrating Large Language Models (LLMs) with robotics, using Whisper for voice command input, employing LLMs for cognitive planning, and executing tasks through ROS 2 actions.\n1.2. Justification\nVLA is a transformative paradigm in robotics, enabling more natural human-robot interaction and a higher degree of autonomy. This module provides a crucial bridge between theoretical AI concepts (like LLMs) and practical, physical robotics, equipping students with highly sought-after skills in creating intelligent, responsive robots.\n1.3. Target Audience\n- Primary: Beginner to intermediate students in robotics and AI.\n- Secondary: Developers and researchers interested in the practical application of LLMs in autonomous systems.\n2. Functional Requirements\n2.1. In Scope (Features)\n- VLA Pipeline Overview: A clear, conceptual explanation of the entire Vision-Language-Action pipeline, from voice input to robot action.\n- Voice Command Processing: A tutorial on using OpenAI Whisper to transcribe spoken language into text and extract actionable commands.\n- LLM-based Planning: A guide on how to prompt an LLM to translate a high-level natural language task (e.g., \"get me the apple\") into a sequence of concrete ROS 2 action goals.\n- Mini-Capstone Project: A complete, runnable example demonstrating the full VLA loop. A user will issue a voice command, and a simulated humanoid robot will plan and execute a simple multi-step task involving navigation, perception, and manipulation.\n2.2. Out of Scope\n- The development of a production-grade, highly reliable robotics stack.\n- Training or fine-tuning custom Large Language Models.\n- Complex, long-horizon tasks or navigation in dynamic, multi-room environments.\n2.3. Success Criteria\n- Understanding: Students can draw a diagram of the VLA pipeline and explain the role of each component (Voice, LLM, ROS 2).\n- Application (Whisper): Students are able to run a demo that successfully converts a spoken command into a text string.\n- Application (LLM Planning): Students can explain how a natural language goal is decomposed into a JSON or YAML sequence of ROS 2 actions by an LLM.\n- Completion: The mini-capstone example runs end-to-end: a voice command triggers a sequence of autonomous actions in the simulated robot.\n2.4. Constraints\n- Word Count: 2000\u20133000 words.\n- Format: Docusaurus Markdown, including code snippets for Python (for Whisper and LLM interaction) and configuration files (for ROS 2 actions).\n- Sources: Content will be based on official OpenAI Whisper documentation, ROS 2 action design patterns, and foundational VLA research.\n- Timeline: 1 week.\n3. UX/UI (Reader Experience)\n3.1. Content Structure\nThe module will be structured to guide the learner from individual components to a fully integrated system.\n- Chapter 1: Voice-to-Action: Whisper and Command Extraction\n- Introduction to Whisper for speech-to-text.\n- Tutorial: Setting up Whisper and running a simple voice transcription demo.\n- Guide: Extracting key intents and entities from the transcribed text (e.g., turning \"Can you please find the red block\" into\n{action: \"find\", object: \"red block\"}\n).\n- Chapter 2: Cognitive Planning with LLMs\n- The role of LLMs as a \"reasoning engine\" for robots.\n- Tutorial: Prompt engineering techniques to make an LLM translate a task into a machine-readable plan (a sequence of ROS 2 actions).\n- Example: Converting\n{action: \"find\", object: \"red block\"}\ninto a plan like[{navigate: \"table\"}, {detect: \"red_block\"}]\n.\n- Chapter 3: Mini-Capstone: The Full VLA Loop\n- An overview of the complete system architecture.\n- A step-by-step guide to launching all components (Whisper listener, LLM planner, ROS 2 action servers).\n- A full demonstration of an autonomous task: e.g., \"Go to the table, find the blue cup, and pick it up.\"\n4. Technical Considerations\n4.1. Key Decisions\n- LLM Abstraction: We will use a major LLM provider's API (e.g., OpenAI, Anthropic, or Google) to avoid the complexity of self-hosting. The focus is on the integration, not the model itself.\n- ROS 2 Actions: The module will use the standard ROS 2 action interface for all robot tasks (e.g.,\nNavigateToPose\n,DetectObject\n,PickAndPlace\n). This ensures a modular and scalable design. - Focus on Integration: The code provided will be primarily \"glue code\" written in Python, connecting the various components of the VLA pipeline.\n4.2. External Dependencies\n- Software: Python 3, ROS 2, and the OpenAI Whisper library.\n- APIs: An API key for an LLM provider will be required for the cognitive planning chapter.\n- Hardware: A microphone for voice commands.\n- Prerequisites: Completion of prior modules, including ROS 2 basics, simulation, and navigation.\n--------------------------------------------------------------------------------"
  },
  {
    "url": "https://Muham251.github.io/ai-hackathon-1/docs/module-4/#42-external-dependencies",
    "text": "Spec: Module 4: Vision-Language-Action (VLA)\n1. Business Understanding\n1.1. Goal\nThe primary goal of this module is to teach students how to build a complete Vision-Language-Action (VLA) pipeline. This involves integrating Large Language Models (LLMs) with robotics, using Whisper for voice command input, employing LLMs for cognitive planning, and executing tasks through ROS 2 actions.\n1.2. Justification\nVLA is a transformative paradigm in robotics, enabling more natural human-robot interaction and a higher degree of autonomy. This module provides a crucial bridge between theoretical AI concepts (like LLMs) and practical, physical robotics, equipping students with highly sought-after skills in creating intelligent, responsive robots.\n1.3. Target Audience\n- Primary: Beginner to intermediate students in robotics and AI.\n- Secondary: Developers and researchers interested in the practical application of LLMs in autonomous systems.\n2. Functional Requirements\n2.1. In Scope (Features)\n- VLA Pipeline Overview: A clear, conceptual explanation of the entire Vision-Language-Action pipeline, from voice input to robot action.\n- Voice Command Processing: A tutorial on using OpenAI Whisper to transcribe spoken language into text and extract actionable commands.\n- LLM-based Planning: A guide on how to prompt an LLM to translate a high-level natural language task (e.g., \"get me the apple\") into a sequence of concrete ROS 2 action goals.\n- Mini-Capstone Project: A complete, runnable example demonstrating the full VLA loop. A user will issue a voice command, and a simulated humanoid robot will plan and execute a simple multi-step task involving navigation, perception, and manipulation.\n2.2. Out of Scope\n- The development of a production-grade, highly reliable robotics stack.\n- Training or fine-tuning custom Large Language Models.\n- Complex, long-horizon tasks or navigation in dynamic, multi-room environments.\n2.3. Success Criteria\n- Understanding: Students can draw a diagram of the VLA pipeline and explain the role of each component (Voice, LLM, ROS 2).\n- Application (Whisper): Students are able to run a demo that successfully converts a spoken command into a text string.\n- Application (LLM Planning): Students can explain how a natural language goal is decomposed into a JSON or YAML sequence of ROS 2 actions by an LLM.\n- Completion: The mini-capstone example runs end-to-end: a voice command triggers a sequence of autonomous actions in the simulated robot.\n2.4. Constraints\n- Word Count: 2000\u20133000 words.\n- Format: Docusaurus Markdown, including code snippets for Python (for Whisper and LLM interaction) and configuration files (for ROS 2 actions).\n- Sources: Content will be based on official OpenAI Whisper documentation, ROS 2 action design patterns, and foundational VLA research.\n- Timeline: 1 week.\n3. UX/UI (Reader Experience)\n3.1. Content Structure\nThe module will be structured to guide the learner from individual components to a fully integrated system.\n- Chapter 1: Voice-to-Action: Whisper and Command Extraction\n- Introduction to Whisper for speech-to-text.\n- Tutorial: Setting up Whisper and running a simple voice transcription demo.\n- Guide: Extracting key intents and entities from the transcribed text (e.g., turning \"Can you please find the red block\" into\n{action: \"find\", object: \"red block\"}\n).\n- Chapter 2: Cognitive Planning with LLMs\n- The role of LLMs as a \"reasoning engine\" for robots.\n- Tutorial: Prompt engineering techniques to make an LLM translate a task into a machine-readable plan (a sequence of ROS 2 actions).\n- Example: Converting\n{action: \"find\", object: \"red block\"}\ninto a plan like[{navigate: \"table\"}, {detect: \"red_block\"}]\n.\n- Chapter 3: Mini-Capstone: The Full VLA Loop\n- An overview of the complete system architecture.\n- A step-by-step guide to launching all components (Whisper listener, LLM planner, ROS 2 action servers).\n- A full demonstration of an autonomous task: e.g., \"Go to the table, find the blue cup, and pick it up.\"\n4. Technical Considerations\n4.1. Key Decisions\n- LLM Abstraction: We will use a major LLM provider's API (e.g., OpenAI, Anthropic, or Google) to avoid the complexity of self-hosting. The focus is on the integration, not the model itself.\n- ROS 2 Actions: The module will use the standard ROS 2 action interface for all robot tasks (e.g.,\nNavigateToPose\n,DetectObject\n,PickAndPlace\n). This ensures a modular and scalable design. - Focus on Integration: The code provided will be primarily \"glue code\" written in Python, connecting the various components of the VLA pipeline.\n4.2. External Dependencies\n- Software: Python 3, ROS 2, and the OpenAI Whisper library.\n- APIs: An API key for an LLM provider will be required for the cognitive planning chapter.\n- Hardware: A microphone for voice commands.\n- Prerequisites: Completion of prior modules, including ROS 2 basics, simulation, and navigation.\n--------------------------------------------------------------------------------"
  },
  {
    "url": "https://Muham251.github.io/ai-hackathon-1/docs/module-3/#1-business-understanding",
    "text": "Spec: Module 3: The AI-Robot Brain (NVIDIA Isaac\u2122)\n1. Business Understanding\n1.1. Goal\nThe objective of this module is to educate beginner to intermediate robotics students on leveraging the NVIDIA Isaac\u2122 ecosystem to build a robot's core intelligence. The curriculum will focus on Isaac Sim for photorealistic simulation, Isaac ROS for perception and navigation (specifically VSLAM), and Nav2 for autonomous path planning.\n1.2. Justification\nThe NVIDIA Isaac\u2122 platform is at the forefront of AI-driven robotics, offering powerful tools for developing and testing intelligent robots in realistic virtual environments. This module provides learners with critical, industry-relevant skills in simulation, perception, and navigation, forming the foundation of a robot's \"brain.\"\n1.3. Target Audience\n- Primary: Beginner to intermediate students in robotics and AI who have a foundational understanding of ROS and simulation concepts.\n- Secondary: Developers and engineers looking to get started with the NVIDIA Isaac\u2122 toolchain for robotics projects.\n2. Functional Requirements\n2.1. In Scope (Features)\n- Isaac Sim Fundamentals:\n- An introduction to creating scenes in Isaac Sim.\n- A guide on generating synthetic data (e.g., camera images, LiDAR data) for training and testing.\n- Isaac ROS Workflow:\n- A conceptual overview of the Visual SLAM (VSLAM) and navigation pipeline provided by Isaac ROS.\n- A tutorial on setting up and running the Isaac ROS navigation stack on a simulated robot.\n- Nav2 Path Planning:\n- A practical example demonstrating how to use Nav2 for path planning with a humanoid robot in Isaac Sim.\n- Runnable Demos:\n- The module must include 2\u20133 complete, runnable demonstrations that users can execute.\n- Format: The final content will be delivered as a Docusaurus-compatible Markdown file.\n2.2. Out of Scope\n- A complete, end-to-end humanoid control pipeline (e.g., manipulation, complex behaviors).\n- The development or deep-dive analysis of custom SLAM or navigation algorithms.\n- Advanced Isaac Sim features like custom Python scripting or detailed RTX rendering settings.\n2.3. Success Criteria\n- Understanding: Learners can explain the significance of photorealistic simulation and synthetic data generation for robotics.\n- Explanation: Learners can describe the high-level workflow of the Isaac ROS VSLAM and navigation stack.\n- Application: Learners can successfully launch and run a basic Nav2 path-planning demonstration within Isaac Sim.\n- Completion: The module is published with 2\u20133 fully functional and validated demos.\n2.4. Constraints\n- Word Count: Approximately 2000\u20133000 words.\n- Format: Docusaurus Markdown with appropriate code blocks for commands and configurations.\n- Sources: All technical information must be derived from and cite official NVIDIA Isaac\u2122 documentation.\n- Timeline: The module is to be completed within a 1-week timeframe.\n3. UX/UI (Reader Experience)\n3.1. Content Structure\nThe module will be organized into a clear, three-chapter structure to guide the learner from environment creation to autonomous navigation.\n- Chapter 1: Isaac Sim Basics: Scenes & Synthetic Data\n- Introduction to the Isaac Sim interface and its core concepts.\n- Tutorial: Setting up a basic scene with a robot and obstacles.\n- Guide: Generating and exporting synthetic sensor data from the simulation.\n- Chapter 2: Isaac ROS: VSLAM & Navigation Workflow\n- Overview of the Isaac ROS ecosystem and its advantages.\n- Step-by-step guide to launching the Isaac ROS Docker container.\n- Tutorial: Running the VSLAM and navigation stack on a robot within Isaac Sim.\n- Chapter 3: Nav2: Path Planning for Humanoids\n- Introduction to Nav2 and its role in autonomous navigation.\n- Example: Sending a navigation goal to a humanoid robot and having it execute a path using Nav2.\n4. Technical Considerations\n4.1. Key Decisions\n- Ecosystem Focus: This module will exclusively use the NVIDIA Isaac\u2122 toolchain to provide a cohesive learning experience.\n- Runnable Demos: Demos will be provided as shell scripts or clear, step-by-step command sequences to ensure they are easy to run.\n- Configuration over Code: The focus will be on configuring and running existing Isaac ROS packages rather than writing new code.\n4.2. External Dependencies\n- Hardware: A computer with a compatible NVIDIA GPU is required.\n- Software: Users must have NVIDIA Isaac Sim, Docker, and NVIDIA Container Toolkit installed.\n- Prerequisites: A foundational knowledge of ROS 2 concepts is assumed (as taught in Module 1).\n--------------------------------------------------------------------------------"
  },
  {
    "url": "https://Muham251.github.io/ai-hackathon-1/docs/module-3/#11-goal",
    "text": "Spec: Module 3: The AI-Robot Brain (NVIDIA Isaac\u2122)\n1. Business Understanding\n1.1. Goal\nThe objective of this module is to educate beginner to intermediate robotics students on leveraging the NVIDIA Isaac\u2122 ecosystem to build a robot's core intelligence. The curriculum will focus on Isaac Sim for photorealistic simulation, Isaac ROS for perception and navigation (specifically VSLAM), and Nav2 for autonomous path planning.\n1.2. Justification\nThe NVIDIA Isaac\u2122 platform is at the forefront of AI-driven robotics, offering powerful tools for developing and testing intelligent robots in realistic virtual environments. This module provides learners with critical, industry-relevant skills in simulation, perception, and navigation, forming the foundation of a robot's \"brain.\"\n1.3. Target Audience\n- Primary: Beginner to intermediate students in robotics and AI who have a foundational understanding of ROS and simulation concepts.\n- Secondary: Developers and engineers looking to get started with the NVIDIA Isaac\u2122 toolchain for robotics projects.\n2. Functional Requirements\n2.1. In Scope (Features)\n- Isaac Sim Fundamentals:\n- An introduction to creating scenes in Isaac Sim.\n- A guide on generating synthetic data (e.g., camera images, LiDAR data) for training and testing.\n- Isaac ROS Workflow:\n- A conceptual overview of the Visual SLAM (VSLAM) and navigation pipeline provided by Isaac ROS.\n- A tutorial on setting up and running the Isaac ROS navigation stack on a simulated robot.\n- Nav2 Path Planning:\n- A practical example demonstrating how to use Nav2 for path planning with a humanoid robot in Isaac Sim.\n- Runnable Demos:\n- The module must include 2\u20133 complete, runnable demonstrations that users can execute.\n- Format: The final content will be delivered as a Docusaurus-compatible Markdown file.\n2.2. Out of Scope\n- A complete, end-to-end humanoid control pipeline (e.g., manipulation, complex behaviors).\n- The development or deep-dive analysis of custom SLAM or navigation algorithms.\n- Advanced Isaac Sim features like custom Python scripting or detailed RTX rendering settings.\n2.3. Success Criteria\n- Understanding: Learners can explain the significance of photorealistic simulation and synthetic data generation for robotics.\n- Explanation: Learners can describe the high-level workflow of the Isaac ROS VSLAM and navigation stack.\n- Application: Learners can successfully launch and run a basic Nav2 path-planning demonstration within Isaac Sim.\n- Completion: The module is published with 2\u20133 fully functional and validated demos.\n2.4. Constraints\n- Word Count: Approximately 2000\u20133000 words.\n- Format: Docusaurus Markdown with appropriate code blocks for commands and configurations.\n- Sources: All technical information must be derived from and cite official NVIDIA Isaac\u2122 documentation.\n- Timeline: The module is to be completed within a 1-week timeframe.\n3. UX/UI (Reader Experience)\n3.1. Content Structure\nThe module will be organized into a clear, three-chapter structure to guide the learner from environment creation to autonomous navigation.\n- Chapter 1: Isaac Sim Basics: Scenes & Synthetic Data\n- Introduction to the Isaac Sim interface and its core concepts.\n- Tutorial: Setting up a basic scene with a robot and obstacles.\n- Guide: Generating and exporting synthetic sensor data from the simulation.\n- Chapter 2: Isaac ROS: VSLAM & Navigation Workflow\n- Overview of the Isaac ROS ecosystem and its advantages.\n- Step-by-step guide to launching the Isaac ROS Docker container.\n- Tutorial: Running the VSLAM and navigation stack on a robot within Isaac Sim.\n- Chapter 3: Nav2: Path Planning for Humanoids\n- Introduction to Nav2 and its role in autonomous navigation.\n- Example: Sending a navigation goal to a humanoid robot and having it execute a path using Nav2.\n4. Technical Considerations\n4.1. Key Decisions\n- Ecosystem Focus: This module will exclusively use the NVIDIA Isaac\u2122 toolchain to provide a cohesive learning experience.\n- Runnable Demos: Demos will be provided as shell scripts or clear, step-by-step command sequences to ensure they are easy to run.\n- Configuration over Code: The focus will be on configuring and running existing Isaac ROS packages rather than writing new code.\n4.2. External Dependencies\n- Hardware: A computer with a compatible NVIDIA GPU is required.\n- Software: Users must have NVIDIA Isaac Sim, Docker, and NVIDIA Container Toolkit installed.\n- Prerequisites: A foundational knowledge of ROS 2 concepts is assumed (as taught in Module 1).\n--------------------------------------------------------------------------------"
  },
  {
    "url": "https://Muham251.github.io/ai-hackathon-1/docs/module-3/#12-justification",
    "text": "Spec: Module 3: The AI-Robot Brain (NVIDIA Isaac\u2122)\n1. Business Understanding\n1.1. Goal\nThe objective of this module is to educate beginner to intermediate robotics students on leveraging the NVIDIA Isaac\u2122 ecosystem to build a robot's core intelligence. The curriculum will focus on Isaac Sim for photorealistic simulation, Isaac ROS for perception and navigation (specifically VSLAM), and Nav2 for autonomous path planning.\n1.2. Justification\nThe NVIDIA Isaac\u2122 platform is at the forefront of AI-driven robotics, offering powerful tools for developing and testing intelligent robots in realistic virtual environments. This module provides learners with critical, industry-relevant skills in simulation, perception, and navigation, forming the foundation of a robot's \"brain.\"\n1.3. Target Audience\n- Primary: Beginner to intermediate students in robotics and AI who have a foundational understanding of ROS and simulation concepts.\n- Secondary: Developers and engineers looking to get started with the NVIDIA Isaac\u2122 toolchain for robotics projects.\n2. Functional Requirements\n2.1. In Scope (Features)\n- Isaac Sim Fundamentals:\n- An introduction to creating scenes in Isaac Sim.\n- A guide on generating synthetic data (e.g., camera images, LiDAR data) for training and testing.\n- Isaac ROS Workflow:\n- A conceptual overview of the Visual SLAM (VSLAM) and navigation pipeline provided by Isaac ROS.\n- A tutorial on setting up and running the Isaac ROS navigation stack on a simulated robot.\n- Nav2 Path Planning:\n- A practical example demonstrating how to use Nav2 for path planning with a humanoid robot in Isaac Sim.\n- Runnable Demos:\n- The module must include 2\u20133 complete, runnable demonstrations that users can execute.\n- Format: The final content will be delivered as a Docusaurus-compatible Markdown file.\n2.2. Out of Scope\n- A complete, end-to-end humanoid control pipeline (e.g., manipulation, complex behaviors).\n- The development or deep-dive analysis of custom SLAM or navigation algorithms.\n- Advanced Isaac Sim features like custom Python scripting or detailed RTX rendering settings.\n2.3. Success Criteria\n- Understanding: Learners can explain the significance of photorealistic simulation and synthetic data generation for robotics.\n- Explanation: Learners can describe the high-level workflow of the Isaac ROS VSLAM and navigation stack.\n- Application: Learners can successfully launch and run a basic Nav2 path-planning demonstration within Isaac Sim.\n- Completion: The module is published with 2\u20133 fully functional and validated demos.\n2.4. Constraints\n- Word Count: Approximately 2000\u20133000 words.\n- Format: Docusaurus Markdown with appropriate code blocks for commands and configurations.\n- Sources: All technical information must be derived from and cite official NVIDIA Isaac\u2122 documentation.\n- Timeline: The module is to be completed within a 1-week timeframe.\n3. UX/UI (Reader Experience)\n3.1. Content Structure\nThe module will be organized into a clear, three-chapter structure to guide the learner from environment creation to autonomous navigation.\n- Chapter 1: Isaac Sim Basics: Scenes & Synthetic Data\n- Introduction to the Isaac Sim interface and its core concepts.\n- Tutorial: Setting up a basic scene with a robot and obstacles.\n- Guide: Generating and exporting synthetic sensor data from the simulation.\n- Chapter 2: Isaac ROS: VSLAM & Navigation Workflow\n- Overview of the Isaac ROS ecosystem and its advantages.\n- Step-by-step guide to launching the Isaac ROS Docker container.\n- Tutorial: Running the VSLAM and navigation stack on a robot within Isaac Sim.\n- Chapter 3: Nav2: Path Planning for Humanoids\n- Introduction to Nav2 and its role in autonomous navigation.\n- Example: Sending a navigation goal to a humanoid robot and having it execute a path using Nav2.\n4. Technical Considerations\n4.1. Key Decisions\n- Ecosystem Focus: This module will exclusively use the NVIDIA Isaac\u2122 toolchain to provide a cohesive learning experience.\n- Runnable Demos: Demos will be provided as shell scripts or clear, step-by-step command sequences to ensure they are easy to run.\n- Configuration over Code: The focus will be on configuring and running existing Isaac ROS packages rather than writing new code.\n4.2. External Dependencies\n- Hardware: A computer with a compatible NVIDIA GPU is required.\n- Software: Users must have NVIDIA Isaac Sim, Docker, and NVIDIA Container Toolkit installed.\n- Prerequisites: A foundational knowledge of ROS 2 concepts is assumed (as taught in Module 1).\n--------------------------------------------------------------------------------"
  },
  {
    "url": "https://Muham251.github.io/ai-hackathon-1/docs/module-3/#13-target-audience",
    "text": "Spec: Module 3: The AI-Robot Brain (NVIDIA Isaac\u2122)\n1. Business Understanding\n1.1. Goal\nThe objective of this module is to educate beginner to intermediate robotics students on leveraging the NVIDIA Isaac\u2122 ecosystem to build a robot's core intelligence. The curriculum will focus on Isaac Sim for photorealistic simulation, Isaac ROS for perception and navigation (specifically VSLAM), and Nav2 for autonomous path planning.\n1.2. Justification\nThe NVIDIA Isaac\u2122 platform is at the forefront of AI-driven robotics, offering powerful tools for developing and testing intelligent robots in realistic virtual environments. This module provides learners with critical, industry-relevant skills in simulation, perception, and navigation, forming the foundation of a robot's \"brain.\"\n1.3. Target Audience\n- Primary: Beginner to intermediate students in robotics and AI who have a foundational understanding of ROS and simulation concepts.\n- Secondary: Developers and engineers looking to get started with the NVIDIA Isaac\u2122 toolchain for robotics projects.\n2. Functional Requirements\n2.1. In Scope (Features)\n- Isaac Sim Fundamentals:\n- An introduction to creating scenes in Isaac Sim.\n- A guide on generating synthetic data (e.g., camera images, LiDAR data) for training and testing.\n- Isaac ROS Workflow:\n- A conceptual overview of the Visual SLAM (VSLAM) and navigation pipeline provided by Isaac ROS.\n- A tutorial on setting up and running the Isaac ROS navigation stack on a simulated robot.\n- Nav2 Path Planning:\n- A practical example demonstrating how to use Nav2 for path planning with a humanoid robot in Isaac Sim.\n- Runnable Demos:\n- The module must include 2\u20133 complete, runnable demonstrations that users can execute.\n- Format: The final content will be delivered as a Docusaurus-compatible Markdown file.\n2.2. Out of Scope\n- A complete, end-to-end humanoid control pipeline (e.g., manipulation, complex behaviors).\n- The development or deep-dive analysis of custom SLAM or navigation algorithms.\n- Advanced Isaac Sim features like custom Python scripting or detailed RTX rendering settings.\n2.3. Success Criteria\n- Understanding: Learners can explain the significance of photorealistic simulation and synthetic data generation for robotics.\n- Explanation: Learners can describe the high-level workflow of the Isaac ROS VSLAM and navigation stack.\n- Application: Learners can successfully launch and run a basic Nav2 path-planning demonstration within Isaac Sim.\n- Completion: The module is published with 2\u20133 fully functional and validated demos.\n2.4. Constraints\n- Word Count: Approximately 2000\u20133000 words.\n- Format: Docusaurus Markdown with appropriate code blocks for commands and configurations.\n- Sources: All technical information must be derived from and cite official NVIDIA Isaac\u2122 documentation.\n- Timeline: The module is to be completed within a 1-week timeframe.\n3. UX/UI (Reader Experience)\n3.1. Content Structure\nThe module will be organized into a clear, three-chapter structure to guide the learner from environment creation to autonomous navigation.\n- Chapter 1: Isaac Sim Basics: Scenes & Synthetic Data\n- Introduction to the Isaac Sim interface and its core concepts.\n- Tutorial: Setting up a basic scene with a robot and obstacles.\n- Guide: Generating and exporting synthetic sensor data from the simulation.\n- Chapter 2: Isaac ROS: VSLAM & Navigation Workflow\n- Overview of the Isaac ROS ecosystem and its advantages.\n- Step-by-step guide to launching the Isaac ROS Docker container.\n- Tutorial: Running the VSLAM and navigation stack on a robot within Isaac Sim.\n- Chapter 3: Nav2: Path Planning for Humanoids\n- Introduction to Nav2 and its role in autonomous navigation.\n- Example: Sending a navigation goal to a humanoid robot and having it execute a path using Nav2.\n4. Technical Considerations\n4.1. Key Decisions\n- Ecosystem Focus: This module will exclusively use the NVIDIA Isaac\u2122 toolchain to provide a cohesive learning experience.\n- Runnable Demos: Demos will be provided as shell scripts or clear, step-by-step command sequences to ensure they are easy to run.\n- Configuration over Code: The focus will be on configuring and running existing Isaac ROS packages rather than writing new code.\n4.2. External Dependencies\n- Hardware: A computer with a compatible NVIDIA GPU is required.\n- Software: Users must have NVIDIA Isaac Sim, Docker, and NVIDIA Container Toolkit installed.\n- Prerequisites: A foundational knowledge of ROS 2 concepts is assumed (as taught in Module 1).\n--------------------------------------------------------------------------------"
  },
  {
    "url": "https://Muham251.github.io/ai-hackathon-1/docs/module-3/#2-functional-requirements",
    "text": "Spec: Module 3: The AI-Robot Brain (NVIDIA Isaac\u2122)\n1. Business Understanding\n1.1. Goal\nThe objective of this module is to educate beginner to intermediate robotics students on leveraging the NVIDIA Isaac\u2122 ecosystem to build a robot's core intelligence. The curriculum will focus on Isaac Sim for photorealistic simulation, Isaac ROS for perception and navigation (specifically VSLAM), and Nav2 for autonomous path planning.\n1.2. Justification\nThe NVIDIA Isaac\u2122 platform is at the forefront of AI-driven robotics, offering powerful tools for developing and testing intelligent robots in realistic virtual environments. This module provides learners with critical, industry-relevant skills in simulation, perception, and navigation, forming the foundation of a robot's \"brain.\"\n1.3. Target Audience\n- Primary: Beginner to intermediate students in robotics and AI who have a foundational understanding of ROS and simulation concepts.\n- Secondary: Developers and engineers looking to get started with the NVIDIA Isaac\u2122 toolchain for robotics projects.\n2. Functional Requirements\n2.1. In Scope (Features)\n- Isaac Sim Fundamentals:\n- An introduction to creating scenes in Isaac Sim.\n- A guide on generating synthetic data (e.g., camera images, LiDAR data) for training and testing.\n- Isaac ROS Workflow:\n- A conceptual overview of the Visual SLAM (VSLAM) and navigation pipeline provided by Isaac ROS.\n- A tutorial on setting up and running the Isaac ROS navigation stack on a simulated robot.\n- Nav2 Path Planning:\n- A practical example demonstrating how to use Nav2 for path planning with a humanoid robot in Isaac Sim.\n- Runnable Demos:\n- The module must include 2\u20133 complete, runnable demonstrations that users can execute.\n- Format: The final content will be delivered as a Docusaurus-compatible Markdown file.\n2.2. Out of Scope\n- A complete, end-to-end humanoid control pipeline (e.g., manipulation, complex behaviors).\n- The development or deep-dive analysis of custom SLAM or navigation algorithms.\n- Advanced Isaac Sim features like custom Python scripting or detailed RTX rendering settings.\n2.3. Success Criteria\n- Understanding: Learners can explain the significance of photorealistic simulation and synthetic data generation for robotics.\n- Explanation: Learners can describe the high-level workflow of the Isaac ROS VSLAM and navigation stack.\n- Application: Learners can successfully launch and run a basic Nav2 path-planning demonstration within Isaac Sim.\n- Completion: The module is published with 2\u20133 fully functional and validated demos.\n2.4. Constraints\n- Word Count: Approximately 2000\u20133000 words.\n- Format: Docusaurus Markdown with appropriate code blocks for commands and configurations.\n- Sources: All technical information must be derived from and cite official NVIDIA Isaac\u2122 documentation.\n- Timeline: The module is to be completed within a 1-week timeframe.\n3. UX/UI (Reader Experience)\n3.1. Content Structure\nThe module will be organized into a clear, three-chapter structure to guide the learner from environment creation to autonomous navigation.\n- Chapter 1: Isaac Sim Basics: Scenes & Synthetic Data\n- Introduction to the Isaac Sim interface and its core concepts.\n- Tutorial: Setting up a basic scene with a robot and obstacles.\n- Guide: Generating and exporting synthetic sensor data from the simulation.\n- Chapter 2: Isaac ROS: VSLAM & Navigation Workflow\n- Overview of the Isaac ROS ecosystem and its advantages.\n- Step-by-step guide to launching the Isaac ROS Docker container.\n- Tutorial: Running the VSLAM and navigation stack on a robot within Isaac Sim.\n- Chapter 3: Nav2: Path Planning for Humanoids\n- Introduction to Nav2 and its role in autonomous navigation.\n- Example: Sending a navigation goal to a humanoid robot and having it execute a path using Nav2.\n4. Technical Considerations\n4.1. Key Decisions\n- Ecosystem Focus: This module will exclusively use the NVIDIA Isaac\u2122 toolchain to provide a cohesive learning experience.\n- Runnable Demos: Demos will be provided as shell scripts or clear, step-by-step command sequences to ensure they are easy to run.\n- Configuration over Code: The focus will be on configuring and running existing Isaac ROS packages rather than writing new code.\n4.2. External Dependencies\n- Hardware: A computer with a compatible NVIDIA GPU is required.\n- Software: Users must have NVIDIA Isaac Sim, Docker, and NVIDIA Container Toolkit installed.\n- Prerequisites: A foundational knowledge of ROS 2 concepts is assumed (as taught in Module 1).\n--------------------------------------------------------------------------------"
  },
  {
    "url": "https://Muham251.github.io/ai-hackathon-1/docs/module-3/#21-in-scope-features",
    "text": "Spec: Module 3: The AI-Robot Brain (NVIDIA Isaac\u2122)\n1. Business Understanding\n1.1. Goal\nThe objective of this module is to educate beginner to intermediate robotics students on leveraging the NVIDIA Isaac\u2122 ecosystem to build a robot's core intelligence. The curriculum will focus on Isaac Sim for photorealistic simulation, Isaac ROS for perception and navigation (specifically VSLAM), and Nav2 for autonomous path planning.\n1.2. Justification\nThe NVIDIA Isaac\u2122 platform is at the forefront of AI-driven robotics, offering powerful tools for developing and testing intelligent robots in realistic virtual environments. This module provides learners with critical, industry-relevant skills in simulation, perception, and navigation, forming the foundation of a robot's \"brain.\"\n1.3. Target Audience\n- Primary: Beginner to intermediate students in robotics and AI who have a foundational understanding of ROS and simulation concepts.\n- Secondary: Developers and engineers looking to get started with the NVIDIA Isaac\u2122 toolchain for robotics projects.\n2. Functional Requirements\n2.1. In Scope (Features)\n- Isaac Sim Fundamentals:\n- An introduction to creating scenes in Isaac Sim.\n- A guide on generating synthetic data (e.g., camera images, LiDAR data) for training and testing.\n- Isaac ROS Workflow:\n- A conceptual overview of the Visual SLAM (VSLAM) and navigation pipeline provided by Isaac ROS.\n- A tutorial on setting up and running the Isaac ROS navigation stack on a simulated robot.\n- Nav2 Path Planning:\n- A practical example demonstrating how to use Nav2 for path planning with a humanoid robot in Isaac Sim.\n- Runnable Demos:\n- The module must include 2\u20133 complete, runnable demonstrations that users can execute.\n- Format: The final content will be delivered as a Docusaurus-compatible Markdown file.\n2.2. Out of Scope\n- A complete, end-to-end humanoid control pipeline (e.g., manipulation, complex behaviors).\n- The development or deep-dive analysis of custom SLAM or navigation algorithms.\n- Advanced Isaac Sim features like custom Python scripting or detailed RTX rendering settings.\n2.3. Success Criteria\n- Understanding: Learners can explain the significance of photorealistic simulation and synthetic data generation for robotics.\n- Explanation: Learners can describe the high-level workflow of the Isaac ROS VSLAM and navigation stack.\n- Application: Learners can successfully launch and run a basic Nav2 path-planning demonstration within Isaac Sim.\n- Completion: The module is published with 2\u20133 fully functional and validated demos.\n2.4. Constraints\n- Word Count: Approximately 2000\u20133000 words.\n- Format: Docusaurus Markdown with appropriate code blocks for commands and configurations.\n- Sources: All technical information must be derived from and cite official NVIDIA Isaac\u2122 documentation.\n- Timeline: The module is to be completed within a 1-week timeframe.\n3. UX/UI (Reader Experience)\n3.1. Content Structure\nThe module will be organized into a clear, three-chapter structure to guide the learner from environment creation to autonomous navigation.\n- Chapter 1: Isaac Sim Basics: Scenes & Synthetic Data\n- Introduction to the Isaac Sim interface and its core concepts.\n- Tutorial: Setting up a basic scene with a robot and obstacles.\n- Guide: Generating and exporting synthetic sensor data from the simulation.\n- Chapter 2: Isaac ROS: VSLAM & Navigation Workflow\n- Overview of the Isaac ROS ecosystem and its advantages.\n- Step-by-step guide to launching the Isaac ROS Docker container.\n- Tutorial: Running the VSLAM and navigation stack on a robot within Isaac Sim.\n- Chapter 3: Nav2: Path Planning for Humanoids\n- Introduction to Nav2 and its role in autonomous navigation.\n- Example: Sending a navigation goal to a humanoid robot and having it execute a path using Nav2.\n4. Technical Considerations\n4.1. Key Decisions\n- Ecosystem Focus: This module will exclusively use the NVIDIA Isaac\u2122 toolchain to provide a cohesive learning experience.\n- Runnable Demos: Demos will be provided as shell scripts or clear, step-by-step command sequences to ensure they are easy to run.\n- Configuration over Code: The focus will be on configuring and running existing Isaac ROS packages rather than writing new code.\n4.2. External Dependencies\n- Hardware: A computer with a compatible NVIDIA GPU is required.\n- Software: Users must have NVIDIA Isaac Sim, Docker, and NVIDIA Container Toolkit installed.\n- Prerequisites: A foundational knowledge of ROS 2 concepts is assumed (as taught in Module 1).\n--------------------------------------------------------------------------------"
  },
  {
    "url": "https://Muham251.github.io/ai-hackathon-1/docs/module-3/#22-out-of-scope",
    "text": "Spec: Module 3: The AI-Robot Brain (NVIDIA Isaac\u2122)\n1. Business Understanding\n1.1. Goal\nThe objective of this module is to educate beginner to intermediate robotics students on leveraging the NVIDIA Isaac\u2122 ecosystem to build a robot's core intelligence. The curriculum will focus on Isaac Sim for photorealistic simulation, Isaac ROS for perception and navigation (specifically VSLAM), and Nav2 for autonomous path planning.\n1.2. Justification\nThe NVIDIA Isaac\u2122 platform is at the forefront of AI-driven robotics, offering powerful tools for developing and testing intelligent robots in realistic virtual environments. This module provides learners with critical, industry-relevant skills in simulation, perception, and navigation, forming the foundation of a robot's \"brain.\"\n1.3. Target Audience\n- Primary: Beginner to intermediate students in robotics and AI who have a foundational understanding of ROS and simulation concepts.\n- Secondary: Developers and engineers looking to get started with the NVIDIA Isaac\u2122 toolchain for robotics projects.\n2. Functional Requirements\n2.1. In Scope (Features)\n- Isaac Sim Fundamentals:\n- An introduction to creating scenes in Isaac Sim.\n- A guide on generating synthetic data (e.g., camera images, LiDAR data) for training and testing.\n- Isaac ROS Workflow:\n- A conceptual overview of the Visual SLAM (VSLAM) and navigation pipeline provided by Isaac ROS.\n- A tutorial on setting up and running the Isaac ROS navigation stack on a simulated robot.\n- Nav2 Path Planning:\n- A practical example demonstrating how to use Nav2 for path planning with a humanoid robot in Isaac Sim.\n- Runnable Demos:\n- The module must include 2\u20133 complete, runnable demonstrations that users can execute.\n- Format: The final content will be delivered as a Docusaurus-compatible Markdown file.\n2.2. Out of Scope\n- A complete, end-to-end humanoid control pipeline (e.g., manipulation, complex behaviors).\n- The development or deep-dive analysis of custom SLAM or navigation algorithms.\n- Advanced Isaac Sim features like custom Python scripting or detailed RTX rendering settings.\n2.3. Success Criteria\n- Understanding: Learners can explain the significance of photorealistic simulation and synthetic data generation for robotics.\n- Explanation: Learners can describe the high-level workflow of the Isaac ROS VSLAM and navigation stack.\n- Application: Learners can successfully launch and run a basic Nav2 path-planning demonstration within Isaac Sim.\n- Completion: The module is published with 2\u20133 fully functional and validated demos.\n2.4. Constraints\n- Word Count: Approximately 2000\u20133000 words.\n- Format: Docusaurus Markdown with appropriate code blocks for commands and configurations.\n- Sources: All technical information must be derived from and cite official NVIDIA Isaac\u2122 documentation.\n- Timeline: The module is to be completed within a 1-week timeframe.\n3. UX/UI (Reader Experience)\n3.1. Content Structure\nThe module will be organized into a clear, three-chapter structure to guide the learner from environment creation to autonomous navigation.\n- Chapter 1: Isaac Sim Basics: Scenes & Synthetic Data\n- Introduction to the Isaac Sim interface and its core concepts.\n- Tutorial: Setting up a basic scene with a robot and obstacles.\n- Guide: Generating and exporting synthetic sensor data from the simulation.\n- Chapter 2: Isaac ROS: VSLAM & Navigation Workflow\n- Overview of the Isaac ROS ecosystem and its advantages.\n- Step-by-step guide to launching the Isaac ROS Docker container.\n- Tutorial: Running the VSLAM and navigation stack on a robot within Isaac Sim.\n- Chapter 3: Nav2: Path Planning for Humanoids\n- Introduction to Nav2 and its role in autonomous navigation.\n- Example: Sending a navigation goal to a humanoid robot and having it execute a path using Nav2.\n4. Technical Considerations\n4.1. Key Decisions\n- Ecosystem Focus: This module will exclusively use the NVIDIA Isaac\u2122 toolchain to provide a cohesive learning experience.\n- Runnable Demos: Demos will be provided as shell scripts or clear, step-by-step command sequences to ensure they are easy to run.\n- Configuration over Code: The focus will be on configuring and running existing Isaac ROS packages rather than writing new code.\n4.2. External Dependencies\n- Hardware: A computer with a compatible NVIDIA GPU is required.\n- Software: Users must have NVIDIA Isaac Sim, Docker, and NVIDIA Container Toolkit installed.\n- Prerequisites: A foundational knowledge of ROS 2 concepts is assumed (as taught in Module 1).\n--------------------------------------------------------------------------------"
  },
  {
    "url": "https://Muham251.github.io/ai-hackathon-1/docs/module-3/#23-success-criteria",
    "text": "Spec: Module 3: The AI-Robot Brain (NVIDIA Isaac\u2122)\n1. Business Understanding\n1.1. Goal\nThe objective of this module is to educate beginner to intermediate robotics students on leveraging the NVIDIA Isaac\u2122 ecosystem to build a robot's core intelligence. The curriculum will focus on Isaac Sim for photorealistic simulation, Isaac ROS for perception and navigation (specifically VSLAM), and Nav2 for autonomous path planning.\n1.2. Justification\nThe NVIDIA Isaac\u2122 platform is at the forefront of AI-driven robotics, offering powerful tools for developing and testing intelligent robots in realistic virtual environments. This module provides learners with critical, industry-relevant skills in simulation, perception, and navigation, forming the foundation of a robot's \"brain.\"\n1.3. Target Audience\n- Primary: Beginner to intermediate students in robotics and AI who have a foundational understanding of ROS and simulation concepts.\n- Secondary: Developers and engineers looking to get started with the NVIDIA Isaac\u2122 toolchain for robotics projects.\n2. Functional Requirements\n2.1. In Scope (Features)\n- Isaac Sim Fundamentals:\n- An introduction to creating scenes in Isaac Sim.\n- A guide on generating synthetic data (e.g., camera images, LiDAR data) for training and testing.\n- Isaac ROS Workflow:\n- A conceptual overview of the Visual SLAM (VSLAM) and navigation pipeline provided by Isaac ROS.\n- A tutorial on setting up and running the Isaac ROS navigation stack on a simulated robot.\n- Nav2 Path Planning:\n- A practical example demonstrating how to use Nav2 for path planning with a humanoid robot in Isaac Sim.\n- Runnable Demos:\n- The module must include 2\u20133 complete, runnable demonstrations that users can execute.\n- Format: The final content will be delivered as a Docusaurus-compatible Markdown file.\n2.2. Out of Scope\n- A complete, end-to-end humanoid control pipeline (e.g., manipulation, complex behaviors).\n- The development or deep-dive analysis of custom SLAM or navigation algorithms.\n- Advanced Isaac Sim features like custom Python scripting or detailed RTX rendering settings.\n2.3. Success Criteria\n- Understanding: Learners can explain the significance of photorealistic simulation and synthetic data generation for robotics.\n- Explanation: Learners can describe the high-level workflow of the Isaac ROS VSLAM and navigation stack.\n- Application: Learners can successfully launch and run a basic Nav2 path-planning demonstration within Isaac Sim.\n- Completion: The module is published with 2\u20133 fully functional and validated demos.\n2.4. Constraints\n- Word Count: Approximately 2000\u20133000 words.\n- Format: Docusaurus Markdown with appropriate code blocks for commands and configurations.\n- Sources: All technical information must be derived from and cite official NVIDIA Isaac\u2122 documentation.\n- Timeline: The module is to be completed within a 1-week timeframe.\n3. UX/UI (Reader Experience)\n3.1. Content Structure\nThe module will be organized into a clear, three-chapter structure to guide the learner from environment creation to autonomous navigation.\n- Chapter 1: Isaac Sim Basics: Scenes & Synthetic Data\n- Introduction to the Isaac Sim interface and its core concepts.\n- Tutorial: Setting up a basic scene with a robot and obstacles.\n- Guide: Generating and exporting synthetic sensor data from the simulation.\n- Chapter 2: Isaac ROS: VSLAM & Navigation Workflow\n- Overview of the Isaac ROS ecosystem and its advantages.\n- Step-by-step guide to launching the Isaac ROS Docker container.\n- Tutorial: Running the VSLAM and navigation stack on a robot within Isaac Sim.\n- Chapter 3: Nav2: Path Planning for Humanoids\n- Introduction to Nav2 and its role in autonomous navigation.\n- Example: Sending a navigation goal to a humanoid robot and having it execute a path using Nav2.\n4. Technical Considerations\n4.1. Key Decisions\n- Ecosystem Focus: This module will exclusively use the NVIDIA Isaac\u2122 toolchain to provide a cohesive learning experience.\n- Runnable Demos: Demos will be provided as shell scripts or clear, step-by-step command sequences to ensure they are easy to run.\n- Configuration over Code: The focus will be on configuring and running existing Isaac ROS packages rather than writing new code.\n4.2. External Dependencies\n- Hardware: A computer with a compatible NVIDIA GPU is required.\n- Software: Users must have NVIDIA Isaac Sim, Docker, and NVIDIA Container Toolkit installed.\n- Prerequisites: A foundational knowledge of ROS 2 concepts is assumed (as taught in Module 1).\n--------------------------------------------------------------------------------"
  },
  {
    "url": "https://Muham251.github.io/ai-hackathon-1/docs/module-3/#24-constraints",
    "text": "Spec: Module 3: The AI-Robot Brain (NVIDIA Isaac\u2122)\n1. Business Understanding\n1.1. Goal\nThe objective of this module is to educate beginner to intermediate robotics students on leveraging the NVIDIA Isaac\u2122 ecosystem to build a robot's core intelligence. The curriculum will focus on Isaac Sim for photorealistic simulation, Isaac ROS for perception and navigation (specifically VSLAM), and Nav2 for autonomous path planning.\n1.2. Justification\nThe NVIDIA Isaac\u2122 platform is at the forefront of AI-driven robotics, offering powerful tools for developing and testing intelligent robots in realistic virtual environments. This module provides learners with critical, industry-relevant skills in simulation, perception, and navigation, forming the foundation of a robot's \"brain.\"\n1.3. Target Audience\n- Primary: Beginner to intermediate students in robotics and AI who have a foundational understanding of ROS and simulation concepts.\n- Secondary: Developers and engineers looking to get started with the NVIDIA Isaac\u2122 toolchain for robotics projects.\n2. Functional Requirements\n2.1. In Scope (Features)\n- Isaac Sim Fundamentals:\n- An introduction to creating scenes in Isaac Sim.\n- A guide on generating synthetic data (e.g., camera images, LiDAR data) for training and testing.\n- Isaac ROS Workflow:\n- A conceptual overview of the Visual SLAM (VSLAM) and navigation pipeline provided by Isaac ROS.\n- A tutorial on setting up and running the Isaac ROS navigation stack on a simulated robot.\n- Nav2 Path Planning:\n- A practical example demonstrating how to use Nav2 for path planning with a humanoid robot in Isaac Sim.\n- Runnable Demos:\n- The module must include 2\u20133 complete, runnable demonstrations that users can execute.\n- Format: The final content will be delivered as a Docusaurus-compatible Markdown file.\n2.2. Out of Scope\n- A complete, end-to-end humanoid control pipeline (e.g., manipulation, complex behaviors).\n- The development or deep-dive analysis of custom SLAM or navigation algorithms.\n- Advanced Isaac Sim features like custom Python scripting or detailed RTX rendering settings.\n2.3. Success Criteria\n- Understanding: Learners can explain the significance of photorealistic simulation and synthetic data generation for robotics.\n- Explanation: Learners can describe the high-level workflow of the Isaac ROS VSLAM and navigation stack.\n- Application: Learners can successfully launch and run a basic Nav2 path-planning demonstration within Isaac Sim.\n- Completion: The module is published with 2\u20133 fully functional and validated demos.\n2.4. Constraints\n- Word Count: Approximately 2000\u20133000 words.\n- Format: Docusaurus Markdown with appropriate code blocks for commands and configurations.\n- Sources: All technical information must be derived from and cite official NVIDIA Isaac\u2122 documentation.\n- Timeline: The module is to be completed within a 1-week timeframe.\n3. UX/UI (Reader Experience)\n3.1. Content Structure\nThe module will be organized into a clear, three-chapter structure to guide the learner from environment creation to autonomous navigation.\n- Chapter 1: Isaac Sim Basics: Scenes & Synthetic Data\n- Introduction to the Isaac Sim interface and its core concepts.\n- Tutorial: Setting up a basic scene with a robot and obstacles.\n- Guide: Generating and exporting synthetic sensor data from the simulation.\n- Chapter 2: Isaac ROS: VSLAM & Navigation Workflow\n- Overview of the Isaac ROS ecosystem and its advantages.\n- Step-by-step guide to launching the Isaac ROS Docker container.\n- Tutorial: Running the VSLAM and navigation stack on a robot within Isaac Sim.\n- Chapter 3: Nav2: Path Planning for Humanoids\n- Introduction to Nav2 and its role in autonomous navigation.\n- Example: Sending a navigation goal to a humanoid robot and having it execute a path using Nav2.\n4. Technical Considerations\n4.1. Key Decisions\n- Ecosystem Focus: This module will exclusively use the NVIDIA Isaac\u2122 toolchain to provide a cohesive learning experience.\n- Runnable Demos: Demos will be provided as shell scripts or clear, step-by-step command sequences to ensure they are easy to run.\n- Configuration over Code: The focus will be on configuring and running existing Isaac ROS packages rather than writing new code.\n4.2. External Dependencies\n- Hardware: A computer with a compatible NVIDIA GPU is required.\n- Software: Users must have NVIDIA Isaac Sim, Docker, and NVIDIA Container Toolkit installed.\n- Prerequisites: A foundational knowledge of ROS 2 concepts is assumed (as taught in Module 1).\n--------------------------------------------------------------------------------"
  },
  {
    "url": "https://Muham251.github.io/ai-hackathon-1/docs/module-3/#3-uxui-reader-experience",
    "text": "Spec: Module 3: The AI-Robot Brain (NVIDIA Isaac\u2122)\n1. Business Understanding\n1.1. Goal\nThe objective of this module is to educate beginner to intermediate robotics students on leveraging the NVIDIA Isaac\u2122 ecosystem to build a robot's core intelligence. The curriculum will focus on Isaac Sim for photorealistic simulation, Isaac ROS for perception and navigation (specifically VSLAM), and Nav2 for autonomous path planning.\n1.2. Justification\nThe NVIDIA Isaac\u2122 platform is at the forefront of AI-driven robotics, offering powerful tools for developing and testing intelligent robots in realistic virtual environments. This module provides learners with critical, industry-relevant skills in simulation, perception, and navigation, forming the foundation of a robot's \"brain.\"\n1.3. Target Audience\n- Primary: Beginner to intermediate students in robotics and AI who have a foundational understanding of ROS and simulation concepts.\n- Secondary: Developers and engineers looking to get started with the NVIDIA Isaac\u2122 toolchain for robotics projects.\n2. Functional Requirements\n2.1. In Scope (Features)\n- Isaac Sim Fundamentals:\n- An introduction to creating scenes in Isaac Sim.\n- A guide on generating synthetic data (e.g., camera images, LiDAR data) for training and testing.\n- Isaac ROS Workflow:\n- A conceptual overview of the Visual SLAM (VSLAM) and navigation pipeline provided by Isaac ROS.\n- A tutorial on setting up and running the Isaac ROS navigation stack on a simulated robot.\n- Nav2 Path Planning:\n- A practical example demonstrating how to use Nav2 for path planning with a humanoid robot in Isaac Sim.\n- Runnable Demos:\n- The module must include 2\u20133 complete, runnable demonstrations that users can execute.\n- Format: The final content will be delivered as a Docusaurus-compatible Markdown file.\n2.2. Out of Scope\n- A complete, end-to-end humanoid control pipeline (e.g., manipulation, complex behaviors).\n- The development or deep-dive analysis of custom SLAM or navigation algorithms.\n- Advanced Isaac Sim features like custom Python scripting or detailed RTX rendering settings.\n2.3. Success Criteria\n- Understanding: Learners can explain the significance of photorealistic simulation and synthetic data generation for robotics.\n- Explanation: Learners can describe the high-level workflow of the Isaac ROS VSLAM and navigation stack.\n- Application: Learners can successfully launch and run a basic Nav2 path-planning demonstration within Isaac Sim.\n- Completion: The module is published with 2\u20133 fully functional and validated demos.\n2.4. Constraints\n- Word Count: Approximately 2000\u20133000 words.\n- Format: Docusaurus Markdown with appropriate code blocks for commands and configurations.\n- Sources: All technical information must be derived from and cite official NVIDIA Isaac\u2122 documentation.\n- Timeline: The module is to be completed within a 1-week timeframe.\n3. UX/UI (Reader Experience)\n3.1. Content Structure\nThe module will be organized into a clear, three-chapter structure to guide the learner from environment creation to autonomous navigation.\n- Chapter 1: Isaac Sim Basics: Scenes & Synthetic Data\n- Introduction to the Isaac Sim interface and its core concepts.\n- Tutorial: Setting up a basic scene with a robot and obstacles.\n- Guide: Generating and exporting synthetic sensor data from the simulation.\n- Chapter 2: Isaac ROS: VSLAM & Navigation Workflow\n- Overview of the Isaac ROS ecosystem and its advantages.\n- Step-by-step guide to launching the Isaac ROS Docker container.\n- Tutorial: Running the VSLAM and navigation stack on a robot within Isaac Sim.\n- Chapter 3: Nav2: Path Planning for Humanoids\n- Introduction to Nav2 and its role in autonomous navigation.\n- Example: Sending a navigation goal to a humanoid robot and having it execute a path using Nav2.\n4. Technical Considerations\n4.1. Key Decisions\n- Ecosystem Focus: This module will exclusively use the NVIDIA Isaac\u2122 toolchain to provide a cohesive learning experience.\n- Runnable Demos: Demos will be provided as shell scripts or clear, step-by-step command sequences to ensure they are easy to run.\n- Configuration over Code: The focus will be on configuring and running existing Isaac ROS packages rather than writing new code.\n4.2. External Dependencies\n- Hardware: A computer with a compatible NVIDIA GPU is required.\n- Software: Users must have NVIDIA Isaac Sim, Docker, and NVIDIA Container Toolkit installed.\n- Prerequisites: A foundational knowledge of ROS 2 concepts is assumed (as taught in Module 1).\n--------------------------------------------------------------------------------"
  },
  {
    "url": "https://Muham251.github.io/ai-hackathon-1/docs/module-3/#31-content-structure",
    "text": "Spec: Module 3: The AI-Robot Brain (NVIDIA Isaac\u2122)\n1. Business Understanding\n1.1. Goal\nThe objective of this module is to educate beginner to intermediate robotics students on leveraging the NVIDIA Isaac\u2122 ecosystem to build a robot's core intelligence. The curriculum will focus on Isaac Sim for photorealistic simulation, Isaac ROS for perception and navigation (specifically VSLAM), and Nav2 for autonomous path planning.\n1.2. Justification\nThe NVIDIA Isaac\u2122 platform is at the forefront of AI-driven robotics, offering powerful tools for developing and testing intelligent robots in realistic virtual environments. This module provides learners with critical, industry-relevant skills in simulation, perception, and navigation, forming the foundation of a robot's \"brain.\"\n1.3. Target Audience\n- Primary: Beginner to intermediate students in robotics and AI who have a foundational understanding of ROS and simulation concepts.\n- Secondary: Developers and engineers looking to get started with the NVIDIA Isaac\u2122 toolchain for robotics projects.\n2. Functional Requirements\n2.1. In Scope (Features)\n- Isaac Sim Fundamentals:\n- An introduction to creating scenes in Isaac Sim.\n- A guide on generating synthetic data (e.g., camera images, LiDAR data) for training and testing.\n- Isaac ROS Workflow:\n- A conceptual overview of the Visual SLAM (VSLAM) and navigation pipeline provided by Isaac ROS.\n- A tutorial on setting up and running the Isaac ROS navigation stack on a simulated robot.\n- Nav2 Path Planning:\n- A practical example demonstrating how to use Nav2 for path planning with a humanoid robot in Isaac Sim.\n- Runnable Demos:\n- The module must include 2\u20133 complete, runnable demonstrations that users can execute.\n- Format: The final content will be delivered as a Docusaurus-compatible Markdown file.\n2.2. Out of Scope\n- A complete, end-to-end humanoid control pipeline (e.g., manipulation, complex behaviors).\n- The development or deep-dive analysis of custom SLAM or navigation algorithms.\n- Advanced Isaac Sim features like custom Python scripting or detailed RTX rendering settings.\n2.3. Success Criteria\n- Understanding: Learners can explain the significance of photorealistic simulation and synthetic data generation for robotics.\n- Explanation: Learners can describe the high-level workflow of the Isaac ROS VSLAM and navigation stack.\n- Application: Learners can successfully launch and run a basic Nav2 path-planning demonstration within Isaac Sim.\n- Completion: The module is published with 2\u20133 fully functional and validated demos.\n2.4. Constraints\n- Word Count: Approximately 2000\u20133000 words.\n- Format: Docusaurus Markdown with appropriate code blocks for commands and configurations.\n- Sources: All technical information must be derived from and cite official NVIDIA Isaac\u2122 documentation.\n- Timeline: The module is to be completed within a 1-week timeframe.\n3. UX/UI (Reader Experience)\n3.1. Content Structure\nThe module will be organized into a clear, three-chapter structure to guide the learner from environment creation to autonomous navigation.\n- Chapter 1: Isaac Sim Basics: Scenes & Synthetic Data\n- Introduction to the Isaac Sim interface and its core concepts.\n- Tutorial: Setting up a basic scene with a robot and obstacles.\n- Guide: Generating and exporting synthetic sensor data from the simulation.\n- Chapter 2: Isaac ROS: VSLAM & Navigation Workflow\n- Overview of the Isaac ROS ecosystem and its advantages.\n- Step-by-step guide to launching the Isaac ROS Docker container.\n- Tutorial: Running the VSLAM and navigation stack on a robot within Isaac Sim.\n- Chapter 3: Nav2: Path Planning for Humanoids\n- Introduction to Nav2 and its role in autonomous navigation.\n- Example: Sending a navigation goal to a humanoid robot and having it execute a path using Nav2.\n4. Technical Considerations\n4.1. Key Decisions\n- Ecosystem Focus: This module will exclusively use the NVIDIA Isaac\u2122 toolchain to provide a cohesive learning experience.\n- Runnable Demos: Demos will be provided as shell scripts or clear, step-by-step command sequences to ensure they are easy to run.\n- Configuration over Code: The focus will be on configuring and running existing Isaac ROS packages rather than writing new code.\n4.2. External Dependencies\n- Hardware: A computer with a compatible NVIDIA GPU is required.\n- Software: Users must have NVIDIA Isaac Sim, Docker, and NVIDIA Container Toolkit installed.\n- Prerequisites: A foundational knowledge of ROS 2 concepts is assumed (as taught in Module 1).\n--------------------------------------------------------------------------------"
  },
  {
    "url": "https://Muham251.github.io/ai-hackathon-1/docs/module-3/#4-technical-considerations",
    "text": "Spec: Module 3: The AI-Robot Brain (NVIDIA Isaac\u2122)\n1. Business Understanding\n1.1. Goal\nThe objective of this module is to educate beginner to intermediate robotics students on leveraging the NVIDIA Isaac\u2122 ecosystem to build a robot's core intelligence. The curriculum will focus on Isaac Sim for photorealistic simulation, Isaac ROS for perception and navigation (specifically VSLAM), and Nav2 for autonomous path planning.\n1.2. Justification\nThe NVIDIA Isaac\u2122 platform is at the forefront of AI-driven robotics, offering powerful tools for developing and testing intelligent robots in realistic virtual environments. This module provides learners with critical, industry-relevant skills in simulation, perception, and navigation, forming the foundation of a robot's \"brain.\"\n1.3. Target Audience\n- Primary: Beginner to intermediate students in robotics and AI who have a foundational understanding of ROS and simulation concepts.\n- Secondary: Developers and engineers looking to get started with the NVIDIA Isaac\u2122 toolchain for robotics projects.\n2. Functional Requirements\n2.1. In Scope (Features)\n- Isaac Sim Fundamentals:\n- An introduction to creating scenes in Isaac Sim.\n- A guide on generating synthetic data (e.g., camera images, LiDAR data) for training and testing.\n- Isaac ROS Workflow:\n- A conceptual overview of the Visual SLAM (VSLAM) and navigation pipeline provided by Isaac ROS.\n- A tutorial on setting up and running the Isaac ROS navigation stack on a simulated robot.\n- Nav2 Path Planning:\n- A practical example demonstrating how to use Nav2 for path planning with a humanoid robot in Isaac Sim.\n- Runnable Demos:\n- The module must include 2\u20133 complete, runnable demonstrations that users can execute.\n- Format: The final content will be delivered as a Docusaurus-compatible Markdown file.\n2.2. Out of Scope\n- A complete, end-to-end humanoid control pipeline (e.g., manipulation, complex behaviors).\n- The development or deep-dive analysis of custom SLAM or navigation algorithms.\n- Advanced Isaac Sim features like custom Python scripting or detailed RTX rendering settings.\n2.3. Success Criteria\n- Understanding: Learners can explain the significance of photorealistic simulation and synthetic data generation for robotics.\n- Explanation: Learners can describe the high-level workflow of the Isaac ROS VSLAM and navigation stack.\n- Application: Learners can successfully launch and run a basic Nav2 path-planning demonstration within Isaac Sim.\n- Completion: The module is published with 2\u20133 fully functional and validated demos.\n2.4. Constraints\n- Word Count: Approximately 2000\u20133000 words.\n- Format: Docusaurus Markdown with appropriate code blocks for commands and configurations.\n- Sources: All technical information must be derived from and cite official NVIDIA Isaac\u2122 documentation.\n- Timeline: The module is to be completed within a 1-week timeframe.\n3. UX/UI (Reader Experience)\n3.1. Content Structure\nThe module will be organized into a clear, three-chapter structure to guide the learner from environment creation to autonomous navigation.\n- Chapter 1: Isaac Sim Basics: Scenes & Synthetic Data\n- Introduction to the Isaac Sim interface and its core concepts.\n- Tutorial: Setting up a basic scene with a robot and obstacles.\n- Guide: Generating and exporting synthetic sensor data from the simulation.\n- Chapter 2: Isaac ROS: VSLAM & Navigation Workflow\n- Overview of the Isaac ROS ecosystem and its advantages.\n- Step-by-step guide to launching the Isaac ROS Docker container.\n- Tutorial: Running the VSLAM and navigation stack on a robot within Isaac Sim.\n- Chapter 3: Nav2: Path Planning for Humanoids\n- Introduction to Nav2 and its role in autonomous navigation.\n- Example: Sending a navigation goal to a humanoid robot and having it execute a path using Nav2.\n4. Technical Considerations\n4.1. Key Decisions\n- Ecosystem Focus: This module will exclusively use the NVIDIA Isaac\u2122 toolchain to provide a cohesive learning experience.\n- Runnable Demos: Demos will be provided as shell scripts or clear, step-by-step command sequences to ensure they are easy to run.\n- Configuration over Code: The focus will be on configuring and running existing Isaac ROS packages rather than writing new code.\n4.2. External Dependencies\n- Hardware: A computer with a compatible NVIDIA GPU is required.\n- Software: Users must have NVIDIA Isaac Sim, Docker, and NVIDIA Container Toolkit installed.\n- Prerequisites: A foundational knowledge of ROS 2 concepts is assumed (as taught in Module 1).\n--------------------------------------------------------------------------------"
  },
  {
    "url": "https://Muham251.github.io/ai-hackathon-1/docs/module-3/#41-key-decisions",
    "text": "Spec: Module 3: The AI-Robot Brain (NVIDIA Isaac\u2122)\n1. Business Understanding\n1.1. Goal\nThe objective of this module is to educate beginner to intermediate robotics students on leveraging the NVIDIA Isaac\u2122 ecosystem to build a robot's core intelligence. The curriculum will focus on Isaac Sim for photorealistic simulation, Isaac ROS for perception and navigation (specifically VSLAM), and Nav2 for autonomous path planning.\n1.2. Justification\nThe NVIDIA Isaac\u2122 platform is at the forefront of AI-driven robotics, offering powerful tools for developing and testing intelligent robots in realistic virtual environments. This module provides learners with critical, industry-relevant skills in simulation, perception, and navigation, forming the foundation of a robot's \"brain.\"\n1.3. Target Audience\n- Primary: Beginner to intermediate students in robotics and AI who have a foundational understanding of ROS and simulation concepts.\n- Secondary: Developers and engineers looking to get started with the NVIDIA Isaac\u2122 toolchain for robotics projects.\n2. Functional Requirements\n2.1. In Scope (Features)\n- Isaac Sim Fundamentals:\n- An introduction to creating scenes in Isaac Sim.\n- A guide on generating synthetic data (e.g., camera images, LiDAR data) for training and testing.\n- Isaac ROS Workflow:\n- A conceptual overview of the Visual SLAM (VSLAM) and navigation pipeline provided by Isaac ROS.\n- A tutorial on setting up and running the Isaac ROS navigation stack on a simulated robot.\n- Nav2 Path Planning:\n- A practical example demonstrating how to use Nav2 for path planning with a humanoid robot in Isaac Sim.\n- Runnable Demos:\n- The module must include 2\u20133 complete, runnable demonstrations that users can execute.\n- Format: The final content will be delivered as a Docusaurus-compatible Markdown file.\n2.2. Out of Scope\n- A complete, end-to-end humanoid control pipeline (e.g., manipulation, complex behaviors).\n- The development or deep-dive analysis of custom SLAM or navigation algorithms.\n- Advanced Isaac Sim features like custom Python scripting or detailed RTX rendering settings.\n2.3. Success Criteria\n- Understanding: Learners can explain the significance of photorealistic simulation and synthetic data generation for robotics.\n- Explanation: Learners can describe the high-level workflow of the Isaac ROS VSLAM and navigation stack.\n- Application: Learners can successfully launch and run a basic Nav2 path-planning demonstration within Isaac Sim.\n- Completion: The module is published with 2\u20133 fully functional and validated demos.\n2.4. Constraints\n- Word Count: Approximately 2000\u20133000 words.\n- Format: Docusaurus Markdown with appropriate code blocks for commands and configurations.\n- Sources: All technical information must be derived from and cite official NVIDIA Isaac\u2122 documentation.\n- Timeline: The module is to be completed within a 1-week timeframe.\n3. UX/UI (Reader Experience)\n3.1. Content Structure\nThe module will be organized into a clear, three-chapter structure to guide the learner from environment creation to autonomous navigation.\n- Chapter 1: Isaac Sim Basics: Scenes & Synthetic Data\n- Introduction to the Isaac Sim interface and its core concepts.\n- Tutorial: Setting up a basic scene with a robot and obstacles.\n- Guide: Generating and exporting synthetic sensor data from the simulation.\n- Chapter 2: Isaac ROS: VSLAM & Navigation Workflow\n- Overview of the Isaac ROS ecosystem and its advantages.\n- Step-by-step guide to launching the Isaac ROS Docker container.\n- Tutorial: Running the VSLAM and navigation stack on a robot within Isaac Sim.\n- Chapter 3: Nav2: Path Planning for Humanoids\n- Introduction to Nav2 and its role in autonomous navigation.\n- Example: Sending a navigation goal to a humanoid robot and having it execute a path using Nav2.\n4. Technical Considerations\n4.1. Key Decisions\n- Ecosystem Focus: This module will exclusively use the NVIDIA Isaac\u2122 toolchain to provide a cohesive learning experience.\n- Runnable Demos: Demos will be provided as shell scripts or clear, step-by-step command sequences to ensure they are easy to run.\n- Configuration over Code: The focus will be on configuring and running existing Isaac ROS packages rather than writing new code.\n4.2. External Dependencies\n- Hardware: A computer with a compatible NVIDIA GPU is required.\n- Software: Users must have NVIDIA Isaac Sim, Docker, and NVIDIA Container Toolkit installed.\n- Prerequisites: A foundational knowledge of ROS 2 concepts is assumed (as taught in Module 1).\n--------------------------------------------------------------------------------"
  },
  {
    "url": "https://Muham251.github.io/ai-hackathon-1/docs/module-3/#42-external-dependencies",
    "text": "Spec: Module 3: The AI-Robot Brain (NVIDIA Isaac\u2122)\n1. Business Understanding\n1.1. Goal\nThe objective of this module is to educate beginner to intermediate robotics students on leveraging the NVIDIA Isaac\u2122 ecosystem to build a robot's core intelligence. The curriculum will focus on Isaac Sim for photorealistic simulation, Isaac ROS for perception and navigation (specifically VSLAM), and Nav2 for autonomous path planning.\n1.2. Justification\nThe NVIDIA Isaac\u2122 platform is at the forefront of AI-driven robotics, offering powerful tools for developing and testing intelligent robots in realistic virtual environments. This module provides learners with critical, industry-relevant skills in simulation, perception, and navigation, forming the foundation of a robot's \"brain.\"\n1.3. Target Audience\n- Primary: Beginner to intermediate students in robotics and AI who have a foundational understanding of ROS and simulation concepts.\n- Secondary: Developers and engineers looking to get started with the NVIDIA Isaac\u2122 toolchain for robotics projects.\n2. Functional Requirements\n2.1. In Scope (Features)\n- Isaac Sim Fundamentals:\n- An introduction to creating scenes in Isaac Sim.\n- A guide on generating synthetic data (e.g., camera images, LiDAR data) for training and testing.\n- Isaac ROS Workflow:\n- A conceptual overview of the Visual SLAM (VSLAM) and navigation pipeline provided by Isaac ROS.\n- A tutorial on setting up and running the Isaac ROS navigation stack on a simulated robot.\n- Nav2 Path Planning:\n- A practical example demonstrating how to use Nav2 for path planning with a humanoid robot in Isaac Sim.\n- Runnable Demos:\n- The module must include 2\u20133 complete, runnable demonstrations that users can execute.\n- Format: The final content will be delivered as a Docusaurus-compatible Markdown file.\n2.2. Out of Scope\n- A complete, end-to-end humanoid control pipeline (e.g., manipulation, complex behaviors).\n- The development or deep-dive analysis of custom SLAM or navigation algorithms.\n- Advanced Isaac Sim features like custom Python scripting or detailed RTX rendering settings.\n2.3. Success Criteria\n- Understanding: Learners can explain the significance of photorealistic simulation and synthetic data generation for robotics.\n- Explanation: Learners can describe the high-level workflow of the Isaac ROS VSLAM and navigation stack.\n- Application: Learners can successfully launch and run a basic Nav2 path-planning demonstration within Isaac Sim.\n- Completion: The module is published with 2\u20133 fully functional and validated demos.\n2.4. Constraints\n- Word Count: Approximately 2000\u20133000 words.\n- Format: Docusaurus Markdown with appropriate code blocks for commands and configurations.\n- Sources: All technical information must be derived from and cite official NVIDIA Isaac\u2122 documentation.\n- Timeline: The module is to be completed within a 1-week timeframe.\n3. UX/UI (Reader Experience)\n3.1. Content Structure\nThe module will be organized into a clear, three-chapter structure to guide the learner from environment creation to autonomous navigation.\n- Chapter 1: Isaac Sim Basics: Scenes & Synthetic Data\n- Introduction to the Isaac Sim interface and its core concepts.\n- Tutorial: Setting up a basic scene with a robot and obstacles.\n- Guide: Generating and exporting synthetic sensor data from the simulation.\n- Chapter 2: Isaac ROS: VSLAM & Navigation Workflow\n- Overview of the Isaac ROS ecosystem and its advantages.\n- Step-by-step guide to launching the Isaac ROS Docker container.\n- Tutorial: Running the VSLAM and navigation stack on a robot within Isaac Sim.\n- Chapter 3: Nav2: Path Planning for Humanoids\n- Introduction to Nav2 and its role in autonomous navigation.\n- Example: Sending a navigation goal to a humanoid robot and having it execute a path using Nav2.\n4. Technical Considerations\n4.1. Key Decisions\n- Ecosystem Focus: This module will exclusively use the NVIDIA Isaac\u2122 toolchain to provide a cohesive learning experience.\n- Runnable Demos: Demos will be provided as shell scripts or clear, step-by-step command sequences to ensure they are easy to run.\n- Configuration over Code: The focus will be on configuring and running existing Isaac ROS packages rather than writing new code.\n4.2. External Dependencies\n- Hardware: A computer with a compatible NVIDIA GPU is required.\n- Software: Users must have NVIDIA Isaac Sim, Docker, and NVIDIA Container Toolkit installed.\n- Prerequisites: A foundational knowledge of ROS 2 concepts is assumed (as taught in Module 1).\n--------------------------------------------------------------------------------"
  },
  {
    "url": "https://Muham251.github.io/ai-hackathon-1/docs/tutorial-extras/translate-your-site#configure-i18n",
    "text": "Translate your site\nLet's translate docs/intro.md\nto French.\nConfigure i18n\nModify docusaurus.config.js\nto add support for the fr\nlocale:\ndocusaurus.config.js\nexport default {\ni18n: {\ndefaultLocale: 'en',\nlocales: ['en', 'fr'],\n},\n};\nTranslate a doc\nCopy the docs/intro.md\nfile to the i18n/fr\nfolder:\nmkdir -p i18n/fr/docusaurus-plugin-content-docs/current/\ncp docs/intro.md i18n/fr/docusaurus-plugin-content-docs/current/intro.md\nTranslate i18n/fr/docusaurus-plugin-content-docs/current/intro.md\nin French.\nStart your localized site\nStart your site on the French locale:\nnpm run start -- --locale fr\nYour localized site is accessible at http://localhost:3000/fr/ and the Getting Started\npage is translated.\ncaution\nIn development, you can only use one locale at a time.\nAdd a Locale Dropdown\nTo navigate seamlessly across languages, add a locale dropdown.\nModify the docusaurus.config.js\nfile:\ndocusaurus.config.js\nexport default {\nthemeConfig: {\nnavbar: {\nitems: [\n{\ntype: 'localeDropdown',\n},\n],\n},\n},\n};\nThe locale dropdown now appears in your navbar:\nBuild your localized site\nBuild your site for a specific locale:\nnpm run build -- --locale fr\nOr build your site to include all the locales at once:\nnpm run build\n--------------------------------------------------------------------------------"
  },
  {
    "url": "https://Muham251.github.io/ai-hackathon-1/docs/tutorial-extras/translate-your-site#translate-a-doc",
    "text": "Translate your site\nLet's translate docs/intro.md\nto French.\nConfigure i18n\nModify docusaurus.config.js\nto add support for the fr\nlocale:\ndocusaurus.config.js\nexport default {\ni18n: {\ndefaultLocale: 'en',\nlocales: ['en', 'fr'],\n},\n};\nTranslate a doc\nCopy the docs/intro.md\nfile to the i18n/fr\nfolder:\nmkdir -p i18n/fr/docusaurus-plugin-content-docs/current/\ncp docs/intro.md i18n/fr/docusaurus-plugin-content-docs/current/intro.md\nTranslate i18n/fr/docusaurus-plugin-content-docs/current/intro.md\nin French.\nStart your localized site\nStart your site on the French locale:\nnpm run start -- --locale fr\nYour localized site is accessible at http://localhost:3000/fr/ and the Getting Started\npage is translated.\ncaution\nIn development, you can only use one locale at a time.\nAdd a Locale Dropdown\nTo navigate seamlessly across languages, add a locale dropdown.\nModify the docusaurus.config.js\nfile:\ndocusaurus.config.js\nexport default {\nthemeConfig: {\nnavbar: {\nitems: [\n{\ntype: 'localeDropdown',\n},\n],\n},\n},\n};\nThe locale dropdown now appears in your navbar:\nBuild your localized site\nBuild your site for a specific locale:\nnpm run build -- --locale fr\nOr build your site to include all the locales at once:\nnpm run build\n--------------------------------------------------------------------------------"
  },
  {
    "url": "https://Muham251.github.io/ai-hackathon-1/docs/tutorial-extras/translate-your-site#start-your-localized-site",
    "text": "Translate your site\nLet's translate docs/intro.md\nto French.\nConfigure i18n\nModify docusaurus.config.js\nto add support for the fr\nlocale:\ndocusaurus.config.js\nexport default {\ni18n: {\ndefaultLocale: 'en',\nlocales: ['en', 'fr'],\n},\n};\nTranslate a doc\nCopy the docs/intro.md\nfile to the i18n/fr\nfolder:\nmkdir -p i18n/fr/docusaurus-plugin-content-docs/current/\ncp docs/intro.md i18n/fr/docusaurus-plugin-content-docs/current/intro.md\nTranslate i18n/fr/docusaurus-plugin-content-docs/current/intro.md\nin French.\nStart your localized site\nStart your site on the French locale:\nnpm run start -- --locale fr\nYour localized site is accessible at http://localhost:3000/fr/ and the Getting Started\npage is translated.\ncaution\nIn development, you can only use one locale at a time.\nAdd a Locale Dropdown\nTo navigate seamlessly across languages, add a locale dropdown.\nModify the docusaurus.config.js\nfile:\ndocusaurus.config.js\nexport default {\nthemeConfig: {\nnavbar: {\nitems: [\n{\ntype: 'localeDropdown',\n},\n],\n},\n},\n};\nThe locale dropdown now appears in your navbar:\nBuild your localized site\nBuild your site for a specific locale:\nnpm run build -- --locale fr\nOr build your site to include all the locales at once:\nnpm run build\n--------------------------------------------------------------------------------"
  },
  {
    "url": "https://Muham251.github.io/ai-hackathon-1/docs/tutorial-extras/translate-your-site#add-a-locale-dropdown",
    "text": "Translate your site\nLet's translate docs/intro.md\nto French.\nConfigure i18n\nModify docusaurus.config.js\nto add support for the fr\nlocale:\ndocusaurus.config.js\nexport default {\ni18n: {\ndefaultLocale: 'en',\nlocales: ['en', 'fr'],\n},\n};\nTranslate a doc\nCopy the docs/intro.md\nfile to the i18n/fr\nfolder:\nmkdir -p i18n/fr/docusaurus-plugin-content-docs/current/\ncp docs/intro.md i18n/fr/docusaurus-plugin-content-docs/current/intro.md\nTranslate i18n/fr/docusaurus-plugin-content-docs/current/intro.md\nin French.\nStart your localized site\nStart your site on the French locale:\nnpm run start -- --locale fr\nYour localized site is accessible at http://localhost:3000/fr/ and the Getting Started\npage is translated.\ncaution\nIn development, you can only use one locale at a time.\nAdd a Locale Dropdown\nTo navigate seamlessly across languages, add a locale dropdown.\nModify the docusaurus.config.js\nfile:\ndocusaurus.config.js\nexport default {\nthemeConfig: {\nnavbar: {\nitems: [\n{\ntype: 'localeDropdown',\n},\n],\n},\n},\n};\nThe locale dropdown now appears in your navbar:\nBuild your localized site\nBuild your site for a specific locale:\nnpm run build -- --locale fr\nOr build your site to include all the locales at once:\nnpm run build\n--------------------------------------------------------------------------------"
  },
  {
    "url": "https://Muham251.github.io/ai-hackathon-1/docs/tutorial-extras/translate-your-site#build-your-localized-site",
    "text": "Translate your site\nLet's translate docs/intro.md\nto French.\nConfigure i18n\nModify docusaurus.config.js\nto add support for the fr\nlocale:\ndocusaurus.config.js\nexport default {\ni18n: {\ndefaultLocale: 'en',\nlocales: ['en', 'fr'],\n},\n};\nTranslate a doc\nCopy the docs/intro.md\nfile to the i18n/fr\nfolder:\nmkdir -p i18n/fr/docusaurus-plugin-content-docs/current/\ncp docs/intro.md i18n/fr/docusaurus-plugin-content-docs/current/intro.md\nTranslate i18n/fr/docusaurus-plugin-content-docs/current/intro.md\nin French.\nStart your localized site\nStart your site on the French locale:\nnpm run start -- --locale fr\nYour localized site is accessible at http://localhost:3000/fr/ and the Getting Started\npage is translated.\ncaution\nIn development, you can only use one locale at a time.\nAdd a Locale Dropdown\nTo navigate seamlessly across languages, add a locale dropdown.\nModify the docusaurus.config.js\nfile:\ndocusaurus.config.js\nexport default {\nthemeConfig: {\nnavbar: {\nitems: [\n{\ntype: 'localeDropdown',\n},\n],\n},\n},\n};\nThe locale dropdown now appears in your navbar:\nBuild your localized site\nBuild your site for a specific locale:\nnpm run build -- --locale fr\nOr build your site to include all the locales at once:\nnpm run build\n--------------------------------------------------------------------------------"
  },
  {
    "url": "https://Muham251.github.io/ai-hackathon-1/docs/tutorial-extras/manage-docs-versions#create-a-docs-version",
    "text": "Manage Docs Versions\nDocusaurus can manage multiple versions of your docs.\nCreate a docs version\nRelease a version 1.0 of your project:\nnpm run docusaurus docs:version 1.0\nThe docs\nfolder is copied into versioned_docs/version-1.0\nand versions.json\nis created.\nYour docs now have 2 versions:\n1.0\nathttp://localhost:3000/docs/\nfor the version 1.0 docscurrent\nathttp://localhost:3000/docs/next/\nfor the upcoming, unreleased docs\nAdd a Version Dropdown\nTo navigate seamlessly across versions, add a version dropdown.\nModify the docusaurus.config.js\nfile:\ndocusaurus.config.js\nexport default {\nthemeConfig: {\nnavbar: {\nitems: [\n{\ntype: 'docsVersionDropdown',\n},\n],\n},\n},\n};\nThe docs version dropdown appears in your navbar:\nUpdate an existing version\nIt is possible to edit versioned docs in their respective folder:\nversioned_docs/version-1.0/hello.md\nupdateshttp://localhost:3000/docs/hello\ndocs/hello.md\nupdateshttp://localhost:3000/docs/next/hello\n--------------------------------------------------------------------------------"
  },
  {
    "url": "https://Muham251.github.io/ai-hackathon-1/docs/tutorial-extras/manage-docs-versions#add-a-version-dropdown",
    "text": "Manage Docs Versions\nDocusaurus can manage multiple versions of your docs.\nCreate a docs version\nRelease a version 1.0 of your project:\nnpm run docusaurus docs:version 1.0\nThe docs\nfolder is copied into versioned_docs/version-1.0\nand versions.json\nis created.\nYour docs now have 2 versions:\n1.0\nathttp://localhost:3000/docs/\nfor the version 1.0 docscurrent\nathttp://localhost:3000/docs/next/\nfor the upcoming, unreleased docs\nAdd a Version Dropdown\nTo navigate seamlessly across versions, add a version dropdown.\nModify the docusaurus.config.js\nfile:\ndocusaurus.config.js\nexport default {\nthemeConfig: {\nnavbar: {\nitems: [\n{\ntype: 'docsVersionDropdown',\n},\n],\n},\n},\n};\nThe docs version dropdown appears in your navbar:\nUpdate an existing version\nIt is possible to edit versioned docs in their respective folder:\nversioned_docs/version-1.0/hello.md\nupdateshttp://localhost:3000/docs/hello\ndocs/hello.md\nupdateshttp://localhost:3000/docs/next/hello\n--------------------------------------------------------------------------------"
  },
  {
    "url": "https://Muham251.github.io/ai-hackathon-1/docs/tutorial-extras/manage-docs-versions#update-an-existing-version",
    "text": "Manage Docs Versions\nDocusaurus can manage multiple versions of your docs.\nCreate a docs version\nRelease a version 1.0 of your project:\nnpm run docusaurus docs:version 1.0\nThe docs\nfolder is copied into versioned_docs/version-1.0\nand versions.json\nis created.\nYour docs now have 2 versions:\n1.0\nathttp://localhost:3000/docs/\nfor the version 1.0 docscurrent\nathttp://localhost:3000/docs/next/\nfor the upcoming, unreleased docs\nAdd a Version Dropdown\nTo navigate seamlessly across versions, add a version dropdown.\nModify the docusaurus.config.js\nfile:\ndocusaurus.config.js\nexport default {\nthemeConfig: {\nnavbar: {\nitems: [\n{\ntype: 'docsVersionDropdown',\n},\n],\n},\n},\n};\nThe docs version dropdown appears in your navbar:\nUpdate an existing version\nIt is possible to edit versioned docs in their respective folder:\nversioned_docs/version-1.0/hello.md\nupdateshttp://localhost:3000/docs/hello\ndocs/hello.md\nupdateshttp://localhost:3000/docs/next/hello\n--------------------------------------------------------------------------------"
  },
  {
    "url": "https://Muham251.github.io/ai-hackathon-1/docs/module-2/#1-business-understanding",
    "text": "Spec: Module 2: The Digital Twin (Gazebo & Unity)\n1. Business Understanding\n1.1. Goal\nThe goal of this module is to educate students on creating and utilizing digital twins for humanoid robotics. The focus is on mastering physics simulation, environment construction, and sensor simulation within Gazebo and leveraging Unity for high-fidelity rendering.\n1.2. Justification\nDigital twins are fundamental to modern robotics development, enabling safe, cost-effective, and efficient testing of robot behaviors before deployment. This module provides the essential skills for simulating robots and their interactions with the environment, which is a cornerstone of Physical AI.\n1.3. Target Audience\n- Primary: Beginner to intermediate students in Physical AI and Humanoid Robotics.\n- Secondary: Hobbyists and developers looking to expand their skills into robotics simulation.\n2. Functional Requirements\n2.1. In Scope (Features)\n- Gazebo Physics Simulation:\n- Detailed explanation of core physics concepts: gravity, friction, and collision models.\n- A tutorial on building a simple environment (e.g., a room with obstacles) using SDF.\n- A demonstration of a humanoid robot interacting with the environment.\n- Unity for High-Fidelity Rendering:\n- An overview of Unity's rendering pipeline and its application in robotics.\n- A guide to setting up a simple Unity scene for robotic visualization.\n- Sensor Simulation:\n- A guide to simulating common robotic sensors: LiDAR, Depth Cameras, and IMUs in both Gazebo and Unity.\n- Configuration examples for each sensor.\n- Runnable Examples:\n- The module must include 2\u20133 complete, runnable code examples demonstrating the concepts.\n- Format: The final output must be a Docusaurus-compatible Markdown file.\n2.2. Out of Scope\n- Full-scale game development or advanced Unity features.\n- Complex humanoid control algorithms (this is reserved for Module 3).\n- Networked or multi-agent simulations.\n2.3. Success Criteria\n- Knowledge: Students can clearly explain the role of physics engines (Gazebo) and rendering engines (Unity) in creating a digital twin.\n- Practical Skill (Gazebo): Students can successfully build a simple Gazebo world and simulate a humanoid robot within it.\n- Practical Skill (Unity): Students can set up a Unity scene to render a robot model.\n- Practical Skill (Sensors): Students are able to add and configure virtual LiDAR, Depth Camera, and IMU sensors to a simulated robot.\n- Completion: The module contains 2-3 fully functional examples that run without errors.\n2.4. Constraints\n- Word Count: 2000\u20133000 words.\n- Format: Docusaurus Markdown, including formatted code blocks for SDF, C#, and Python/C++.\n- Sources: Content must be based on official documentation from Gazebo, Unity, and relevant sensor simulation libraries.\n- Timeline: 1 week for completion.\n3. UX/UI (Reader Experience)\n3.1. Content Structure\nThe module will be structured into clear, logical chapters to guide the reader progressively.\n- Chapter 1: Gazebo Physics & Environments\n- Introduction to Gazebo as a physics simulator.\n- Core concepts: gravity, collisions, and inertia.\n- Tutorial: Creating a\n.world\nfile with ground plane and basic shapes. - Example: Spawning a humanoid model and observing basic physical interactions.\n- Chapter 2: High-Fidelity Rendering with Unity\n- The role of Unity in robotics for high-quality visualization.\n- Setting up a Unity project for robotics (e.g., using ROS-Unity connectors).\n- Example: Importing a URDF model and creating a simple visualization scene.\n- Chapter 3: Simulating the Senses\n- Introduction to sensor simulation.\n- Simulating LiDAR and visualizing point clouds.\n- Simulating Depth Cameras and interpreting depth images.\n- Simulating an IMU to get orientation and acceleration data.\n- Example: A single robot model equipped with all three sensors, with configuration snippets for each.\n4. Technical Considerations\n4.1. Key Decisions\n- Simulation Tools: Gazebo will be used for robust physics simulation, while Unity will be highlighted for its superior rendering capabilities. The text will explain the trade-offs.\n- Code Examples: Examples will be provided as self-contained code blocks that can be easily copied and run.\n- Structure: The content will follow the chapter structure defined in section 3.1.\n4.2. External Dependencies\n- The module assumes readers have access to and a basic understanding of a Linux environment (for Gazebo/ROS).\n- Readers will need to have Gazebo and Unity Hub (with a recent Unity version) installed.\n- Code examples may rely on ROS 2,\nrclpy\n, orroscpp\nfor integration, building on concepts from Module 1.\n--------------------------------------------------------------------------------"
  },
  {
    "url": "https://Muham251.github.io/ai-hackathon-1/docs/module-2/#11-goal",
    "text": "Spec: Module 2: The Digital Twin (Gazebo & Unity)\n1. Business Understanding\n1.1. Goal\nThe goal of this module is to educate students on creating and utilizing digital twins for humanoid robotics. The focus is on mastering physics simulation, environment construction, and sensor simulation within Gazebo and leveraging Unity for high-fidelity rendering.\n1.2. Justification\nDigital twins are fundamental to modern robotics development, enabling safe, cost-effective, and efficient testing of robot behaviors before deployment. This module provides the essential skills for simulating robots and their interactions with the environment, which is a cornerstone of Physical AI.\n1.3. Target Audience\n- Primary: Beginner to intermediate students in Physical AI and Humanoid Robotics.\n- Secondary: Hobbyists and developers looking to expand their skills into robotics simulation.\n2. Functional Requirements\n2.1. In Scope (Features)\n- Gazebo Physics Simulation:\n- Detailed explanation of core physics concepts: gravity, friction, and collision models.\n- A tutorial on building a simple environment (e.g., a room with obstacles) using SDF.\n- A demonstration of a humanoid robot interacting with the environment.\n- Unity for High-Fidelity Rendering:\n- An overview of Unity's rendering pipeline and its application in robotics.\n- A guide to setting up a simple Unity scene for robotic visualization.\n- Sensor Simulation:\n- A guide to simulating common robotic sensors: LiDAR, Depth Cameras, and IMUs in both Gazebo and Unity.\n- Configuration examples for each sensor.\n- Runnable Examples:\n- The module must include 2\u20133 complete, runnable code examples demonstrating the concepts.\n- Format: The final output must be a Docusaurus-compatible Markdown file.\n2.2. Out of Scope\n- Full-scale game development or advanced Unity features.\n- Complex humanoid control algorithms (this is reserved for Module 3).\n- Networked or multi-agent simulations.\n2.3. Success Criteria\n- Knowledge: Students can clearly explain the role of physics engines (Gazebo) and rendering engines (Unity) in creating a digital twin.\n- Practical Skill (Gazebo): Students can successfully build a simple Gazebo world and simulate a humanoid robot within it.\n- Practical Skill (Unity): Students can set up a Unity scene to render a robot model.\n- Practical Skill (Sensors): Students are able to add and configure virtual LiDAR, Depth Camera, and IMU sensors to a simulated robot.\n- Completion: The module contains 2-3 fully functional examples that run without errors.\n2.4. Constraints\n- Word Count: 2000\u20133000 words.\n- Format: Docusaurus Markdown, including formatted code blocks for SDF, C#, and Python/C++.\n- Sources: Content must be based on official documentation from Gazebo, Unity, and relevant sensor simulation libraries.\n- Timeline: 1 week for completion.\n3. UX/UI (Reader Experience)\n3.1. Content Structure\nThe module will be structured into clear, logical chapters to guide the reader progressively.\n- Chapter 1: Gazebo Physics & Environments\n- Introduction to Gazebo as a physics simulator.\n- Core concepts: gravity, collisions, and inertia.\n- Tutorial: Creating a\n.world\nfile with ground plane and basic shapes. - Example: Spawning a humanoid model and observing basic physical interactions.\n- Chapter 2: High-Fidelity Rendering with Unity\n- The role of Unity in robotics for high-quality visualization.\n- Setting up a Unity project for robotics (e.g., using ROS-Unity connectors).\n- Example: Importing a URDF model and creating a simple visualization scene.\n- Chapter 3: Simulating the Senses\n- Introduction to sensor simulation.\n- Simulating LiDAR and visualizing point clouds.\n- Simulating Depth Cameras and interpreting depth images.\n- Simulating an IMU to get orientation and acceleration data.\n- Example: A single robot model equipped with all three sensors, with configuration snippets for each.\n4. Technical Considerations\n4.1. Key Decisions\n- Simulation Tools: Gazebo will be used for robust physics simulation, while Unity will be highlighted for its superior rendering capabilities. The text will explain the trade-offs.\n- Code Examples: Examples will be provided as self-contained code blocks that can be easily copied and run.\n- Structure: The content will follow the chapter structure defined in section 3.1.\n4.2. External Dependencies\n- The module assumes readers have access to and a basic understanding of a Linux environment (for Gazebo/ROS).\n- Readers will need to have Gazebo and Unity Hub (with a recent Unity version) installed.\n- Code examples may rely on ROS 2,\nrclpy\n, orroscpp\nfor integration, building on concepts from Module 1.\n--------------------------------------------------------------------------------"
  },
  {
    "url": "https://Muham251.github.io/ai-hackathon-1/docs/module-2/#12-justification",
    "text": "Spec: Module 2: The Digital Twin (Gazebo & Unity)\n1. Business Understanding\n1.1. Goal\nThe goal of this module is to educate students on creating and utilizing digital twins for humanoid robotics. The focus is on mastering physics simulation, environment construction, and sensor simulation within Gazebo and leveraging Unity for high-fidelity rendering.\n1.2. Justification\nDigital twins are fundamental to modern robotics development, enabling safe, cost-effective, and efficient testing of robot behaviors before deployment. This module provides the essential skills for simulating robots and their interactions with the environment, which is a cornerstone of Physical AI.\n1.3. Target Audience\n- Primary: Beginner to intermediate students in Physical AI and Humanoid Robotics.\n- Secondary: Hobbyists and developers looking to expand their skills into robotics simulation.\n2. Functional Requirements\n2.1. In Scope (Features)\n- Gazebo Physics Simulation:\n- Detailed explanation of core physics concepts: gravity, friction, and collision models.\n- A tutorial on building a simple environment (e.g., a room with obstacles) using SDF.\n- A demonstration of a humanoid robot interacting with the environment.\n- Unity for High-Fidelity Rendering:\n- An overview of Unity's rendering pipeline and its application in robotics.\n- A guide to setting up a simple Unity scene for robotic visualization.\n- Sensor Simulation:\n- A guide to simulating common robotic sensors: LiDAR, Depth Cameras, and IMUs in both Gazebo and Unity.\n- Configuration examples for each sensor.\n- Runnable Examples:\n- The module must include 2\u20133 complete, runnable code examples demonstrating the concepts.\n- Format: The final output must be a Docusaurus-compatible Markdown file.\n2.2. Out of Scope\n- Full-scale game development or advanced Unity features.\n- Complex humanoid control algorithms (this is reserved for Module 3).\n- Networked or multi-agent simulations.\n2.3. Success Criteria\n- Knowledge: Students can clearly explain the role of physics engines (Gazebo) and rendering engines (Unity) in creating a digital twin.\n- Practical Skill (Gazebo): Students can successfully build a simple Gazebo world and simulate a humanoid robot within it.\n- Practical Skill (Unity): Students can set up a Unity scene to render a robot model.\n- Practical Skill (Sensors): Students are able to add and configure virtual LiDAR, Depth Camera, and IMU sensors to a simulated robot.\n- Completion: The module contains 2-3 fully functional examples that run without errors.\n2.4. Constraints\n- Word Count: 2000\u20133000 words.\n- Format: Docusaurus Markdown, including formatted code blocks for SDF, C#, and Python/C++.\n- Sources: Content must be based on official documentation from Gazebo, Unity, and relevant sensor simulation libraries.\n- Timeline: 1 week for completion.\n3. UX/UI (Reader Experience)\n3.1. Content Structure\nThe module will be structured into clear, logical chapters to guide the reader progressively.\n- Chapter 1: Gazebo Physics & Environments\n- Introduction to Gazebo as a physics simulator.\n- Core concepts: gravity, collisions, and inertia.\n- Tutorial: Creating a\n.world\nfile with ground plane and basic shapes. - Example: Spawning a humanoid model and observing basic physical interactions.\n- Chapter 2: High-Fidelity Rendering with Unity\n- The role of Unity in robotics for high-quality visualization.\n- Setting up a Unity project for robotics (e.g., using ROS-Unity connectors).\n- Example: Importing a URDF model and creating a simple visualization scene.\n- Chapter 3: Simulating the Senses\n- Introduction to sensor simulation.\n- Simulating LiDAR and visualizing point clouds.\n- Simulating Depth Cameras and interpreting depth images.\n- Simulating an IMU to get orientation and acceleration data.\n- Example: A single robot model equipped with all three sensors, with configuration snippets for each.\n4. Technical Considerations\n4.1. Key Decisions\n- Simulation Tools: Gazebo will be used for robust physics simulation, while Unity will be highlighted for its superior rendering capabilities. The text will explain the trade-offs.\n- Code Examples: Examples will be provided as self-contained code blocks that can be easily copied and run.\n- Structure: The content will follow the chapter structure defined in section 3.1.\n4.2. External Dependencies\n- The module assumes readers have access to and a basic understanding of a Linux environment (for Gazebo/ROS).\n- Readers will need to have Gazebo and Unity Hub (with a recent Unity version) installed.\n- Code examples may rely on ROS 2,\nrclpy\n, orroscpp\nfor integration, building on concepts from Module 1.\n--------------------------------------------------------------------------------"
  },
  {
    "url": "https://Muham251.github.io/ai-hackathon-1/docs/module-2/#13-target-audience",
    "text": "Spec: Module 2: The Digital Twin (Gazebo & Unity)\n1. Business Understanding\n1.1. Goal\nThe goal of this module is to educate students on creating and utilizing digital twins for humanoid robotics. The focus is on mastering physics simulation, environment construction, and sensor simulation within Gazebo and leveraging Unity for high-fidelity rendering.\n1.2. Justification\nDigital twins are fundamental to modern robotics development, enabling safe, cost-effective, and efficient testing of robot behaviors before deployment. This module provides the essential skills for simulating robots and their interactions with the environment, which is a cornerstone of Physical AI.\n1.3. Target Audience\n- Primary: Beginner to intermediate students in Physical AI and Humanoid Robotics.\n- Secondary: Hobbyists and developers looking to expand their skills into robotics simulation.\n2. Functional Requirements\n2.1. In Scope (Features)\n- Gazebo Physics Simulation:\n- Detailed explanation of core physics concepts: gravity, friction, and collision models.\n- A tutorial on building a simple environment (e.g., a room with obstacles) using SDF.\n- A demonstration of a humanoid robot interacting with the environment.\n- Unity for High-Fidelity Rendering:\n- An overview of Unity's rendering pipeline and its application in robotics.\n- A guide to setting up a simple Unity scene for robotic visualization.\n- Sensor Simulation:\n- A guide to simulating common robotic sensors: LiDAR, Depth Cameras, and IMUs in both Gazebo and Unity.\n- Configuration examples for each sensor.\n- Runnable Examples:\n- The module must include 2\u20133 complete, runnable code examples demonstrating the concepts.\n- Format: The final output must be a Docusaurus-compatible Markdown file.\n2.2. Out of Scope\n- Full-scale game development or advanced Unity features.\n- Complex humanoid control algorithms (this is reserved for Module 3).\n- Networked or multi-agent simulations.\n2.3. Success Criteria\n- Knowledge: Students can clearly explain the role of physics engines (Gazebo) and rendering engines (Unity) in creating a digital twin.\n- Practical Skill (Gazebo): Students can successfully build a simple Gazebo world and simulate a humanoid robot within it.\n- Practical Skill (Unity): Students can set up a Unity scene to render a robot model.\n- Practical Skill (Sensors): Students are able to add and configure virtual LiDAR, Depth Camera, and IMU sensors to a simulated robot.\n- Completion: The module contains 2-3 fully functional examples that run without errors.\n2.4. Constraints\n- Word Count: 2000\u20133000 words.\n- Format: Docusaurus Markdown, including formatted code blocks for SDF, C#, and Python/C++.\n- Sources: Content must be based on official documentation from Gazebo, Unity, and relevant sensor simulation libraries.\n- Timeline: 1 week for completion.\n3. UX/UI (Reader Experience)\n3.1. Content Structure\nThe module will be structured into clear, logical chapters to guide the reader progressively.\n- Chapter 1: Gazebo Physics & Environments\n- Introduction to Gazebo as a physics simulator.\n- Core concepts: gravity, collisions, and inertia.\n- Tutorial: Creating a\n.world\nfile with ground plane and basic shapes. - Example: Spawning a humanoid model and observing basic physical interactions.\n- Chapter 2: High-Fidelity Rendering with Unity\n- The role of Unity in robotics for high-quality visualization.\n- Setting up a Unity project for robotics (e.g., using ROS-Unity connectors).\n- Example: Importing a URDF model and creating a simple visualization scene.\n- Chapter 3: Simulating the Senses\n- Introduction to sensor simulation.\n- Simulating LiDAR and visualizing point clouds.\n- Simulating Depth Cameras and interpreting depth images.\n- Simulating an IMU to get orientation and acceleration data.\n- Example: A single robot model equipped with all three sensors, with configuration snippets for each.\n4. Technical Considerations\n4.1. Key Decisions\n- Simulation Tools: Gazebo will be used for robust physics simulation, while Unity will be highlighted for its superior rendering capabilities. The text will explain the trade-offs.\n- Code Examples: Examples will be provided as self-contained code blocks that can be easily copied and run.\n- Structure: The content will follow the chapter structure defined in section 3.1.\n4.2. External Dependencies\n- The module assumes readers have access to and a basic understanding of a Linux environment (for Gazebo/ROS).\n- Readers will need to have Gazebo and Unity Hub (with a recent Unity version) installed.\n- Code examples may rely on ROS 2,\nrclpy\n, orroscpp\nfor integration, building on concepts from Module 1.\n--------------------------------------------------------------------------------"
  },
  {
    "url": "https://Muham251.github.io/ai-hackathon-1/docs/module-2/#2-functional-requirements",
    "text": "Spec: Module 2: The Digital Twin (Gazebo & Unity)\n1. Business Understanding\n1.1. Goal\nThe goal of this module is to educate students on creating and utilizing digital twins for humanoid robotics. The focus is on mastering physics simulation, environment construction, and sensor simulation within Gazebo and leveraging Unity for high-fidelity rendering.\n1.2. Justification\nDigital twins are fundamental to modern robotics development, enabling safe, cost-effective, and efficient testing of robot behaviors before deployment. This module provides the essential skills for simulating robots and their interactions with the environment, which is a cornerstone of Physical AI.\n1.3. Target Audience\n- Primary: Beginner to intermediate students in Physical AI and Humanoid Robotics.\n- Secondary: Hobbyists and developers looking to expand their skills into robotics simulation.\n2. Functional Requirements\n2.1. In Scope (Features)\n- Gazebo Physics Simulation:\n- Detailed explanation of core physics concepts: gravity, friction, and collision models.\n- A tutorial on building a simple environment (e.g., a room with obstacles) using SDF.\n- A demonstration of a humanoid robot interacting with the environment.\n- Unity for High-Fidelity Rendering:\n- An overview of Unity's rendering pipeline and its application in robotics.\n- A guide to setting up a simple Unity scene for robotic visualization.\n- Sensor Simulation:\n- A guide to simulating common robotic sensors: LiDAR, Depth Cameras, and IMUs in both Gazebo and Unity.\n- Configuration examples for each sensor.\n- Runnable Examples:\n- The module must include 2\u20133 complete, runnable code examples demonstrating the concepts.\n- Format: The final output must be a Docusaurus-compatible Markdown file.\n2.2. Out of Scope\n- Full-scale game development or advanced Unity features.\n- Complex humanoid control algorithms (this is reserved for Module 3).\n- Networked or multi-agent simulations.\n2.3. Success Criteria\n- Knowledge: Students can clearly explain the role of physics engines (Gazebo) and rendering engines (Unity) in creating a digital twin.\n- Practical Skill (Gazebo): Students can successfully build a simple Gazebo world and simulate a humanoid robot within it.\n- Practical Skill (Unity): Students can set up a Unity scene to render a robot model.\n- Practical Skill (Sensors): Students are able to add and configure virtual LiDAR, Depth Camera, and IMU sensors to a simulated robot.\n- Completion: The module contains 2-3 fully functional examples that run without errors.\n2.4. Constraints\n- Word Count: 2000\u20133000 words.\n- Format: Docusaurus Markdown, including formatted code blocks for SDF, C#, and Python/C++.\n- Sources: Content must be based on official documentation from Gazebo, Unity, and relevant sensor simulation libraries.\n- Timeline: 1 week for completion.\n3. UX/UI (Reader Experience)\n3.1. Content Structure\nThe module will be structured into clear, logical chapters to guide the reader progressively.\n- Chapter 1: Gazebo Physics & Environments\n- Introduction to Gazebo as a physics simulator.\n- Core concepts: gravity, collisions, and inertia.\n- Tutorial: Creating a\n.world\nfile with ground plane and basic shapes. - Example: Spawning a humanoid model and observing basic physical interactions.\n- Chapter 2: High-Fidelity Rendering with Unity\n- The role of Unity in robotics for high-quality visualization.\n- Setting up a Unity project for robotics (e.g., using ROS-Unity connectors).\n- Example: Importing a URDF model and creating a simple visualization scene.\n- Chapter 3: Simulating the Senses\n- Introduction to sensor simulation.\n- Simulating LiDAR and visualizing point clouds.\n- Simulating Depth Cameras and interpreting depth images.\n- Simulating an IMU to get orientation and acceleration data.\n- Example: A single robot model equipped with all three sensors, with configuration snippets for each.\n4. Technical Considerations\n4.1. Key Decisions\n- Simulation Tools: Gazebo will be used for robust physics simulation, while Unity will be highlighted for its superior rendering capabilities. The text will explain the trade-offs.\n- Code Examples: Examples will be provided as self-contained code blocks that can be easily copied and run.\n- Structure: The content will follow the chapter structure defined in section 3.1.\n4.2. External Dependencies\n- The module assumes readers have access to and a basic understanding of a Linux environment (for Gazebo/ROS).\n- Readers will need to have Gazebo and Unity Hub (with a recent Unity version) installed.\n- Code examples may rely on ROS 2,\nrclpy\n, orroscpp\nfor integration, building on concepts from Module 1.\n--------------------------------------------------------------------------------"
  },
  {
    "url": "https://Muham251.github.io/ai-hackathon-1/docs/module-2/#21-in-scope-features",
    "text": "Spec: Module 2: The Digital Twin (Gazebo & Unity)\n1. Business Understanding\n1.1. Goal\nThe goal of this module is to educate students on creating and utilizing digital twins for humanoid robotics. The focus is on mastering physics simulation, environment construction, and sensor simulation within Gazebo and leveraging Unity for high-fidelity rendering.\n1.2. Justification\nDigital twins are fundamental to modern robotics development, enabling safe, cost-effective, and efficient testing of robot behaviors before deployment. This module provides the essential skills for simulating robots and their interactions with the environment, which is a cornerstone of Physical AI.\n1.3. Target Audience\n- Primary: Beginner to intermediate students in Physical AI and Humanoid Robotics.\n- Secondary: Hobbyists and developers looking to expand their skills into robotics simulation.\n2. Functional Requirements\n2.1. In Scope (Features)\n- Gazebo Physics Simulation:\n- Detailed explanation of core physics concepts: gravity, friction, and collision models.\n- A tutorial on building a simple environment (e.g., a room with obstacles) using SDF.\n- A demonstration of a humanoid robot interacting with the environment.\n- Unity for High-Fidelity Rendering:\n- An overview of Unity's rendering pipeline and its application in robotics.\n- A guide to setting up a simple Unity scene for robotic visualization.\n- Sensor Simulation:\n- A guide to simulating common robotic sensors: LiDAR, Depth Cameras, and IMUs in both Gazebo and Unity.\n- Configuration examples for each sensor.\n- Runnable Examples:\n- The module must include 2\u20133 complete, runnable code examples demonstrating the concepts.\n- Format: The final output must be a Docusaurus-compatible Markdown file.\n2.2. Out of Scope\n- Full-scale game development or advanced Unity features.\n- Complex humanoid control algorithms (this is reserved for Module 3).\n- Networked or multi-agent simulations.\n2.3. Success Criteria\n- Knowledge: Students can clearly explain the role of physics engines (Gazebo) and rendering engines (Unity) in creating a digital twin.\n- Practical Skill (Gazebo): Students can successfully build a simple Gazebo world and simulate a humanoid robot within it.\n- Practical Skill (Unity): Students can set up a Unity scene to render a robot model.\n- Practical Skill (Sensors): Students are able to add and configure virtual LiDAR, Depth Camera, and IMU sensors to a simulated robot.\n- Completion: The module contains 2-3 fully functional examples that run without errors.\n2.4. Constraints\n- Word Count: 2000\u20133000 words.\n- Format: Docusaurus Markdown, including formatted code blocks for SDF, C#, and Python/C++.\n- Sources: Content must be based on official documentation from Gazebo, Unity, and relevant sensor simulation libraries.\n- Timeline: 1 week for completion.\n3. UX/UI (Reader Experience)\n3.1. Content Structure\nThe module will be structured into clear, logical chapters to guide the reader progressively.\n- Chapter 1: Gazebo Physics & Environments\n- Introduction to Gazebo as a physics simulator.\n- Core concepts: gravity, collisions, and inertia.\n- Tutorial: Creating a\n.world\nfile with ground plane and basic shapes. - Example: Spawning a humanoid model and observing basic physical interactions.\n- Chapter 2: High-Fidelity Rendering with Unity\n- The role of Unity in robotics for high-quality visualization.\n- Setting up a Unity project for robotics (e.g., using ROS-Unity connectors).\n- Example: Importing a URDF model and creating a simple visualization scene.\n- Chapter 3: Simulating the Senses\n- Introduction to sensor simulation.\n- Simulating LiDAR and visualizing point clouds.\n- Simulating Depth Cameras and interpreting depth images.\n- Simulating an IMU to get orientation and acceleration data.\n- Example: A single robot model equipped with all three sensors, with configuration snippets for each.\n4. Technical Considerations\n4.1. Key Decisions\n- Simulation Tools: Gazebo will be used for robust physics simulation, while Unity will be highlighted for its superior rendering capabilities. The text will explain the trade-offs.\n- Code Examples: Examples will be provided as self-contained code blocks that can be easily copied and run.\n- Structure: The content will follow the chapter structure defined in section 3.1.\n4.2. External Dependencies\n- The module assumes readers have access to and a basic understanding of a Linux environment (for Gazebo/ROS).\n- Readers will need to have Gazebo and Unity Hub (with a recent Unity version) installed.\n- Code examples may rely on ROS 2,\nrclpy\n, orroscpp\nfor integration, building on concepts from Module 1.\n--------------------------------------------------------------------------------"
  },
  {
    "url": "https://Muham251.github.io/ai-hackathon-1/docs/module-2/#22-out-of-scope",
    "text": "Spec: Module 2: The Digital Twin (Gazebo & Unity)\n1. Business Understanding\n1.1. Goal\nThe goal of this module is to educate students on creating and utilizing digital twins for humanoid robotics. The focus is on mastering physics simulation, environment construction, and sensor simulation within Gazebo and leveraging Unity for high-fidelity rendering.\n1.2. Justification\nDigital twins are fundamental to modern robotics development, enabling safe, cost-effective, and efficient testing of robot behaviors before deployment. This module provides the essential skills for simulating robots and their interactions with the environment, which is a cornerstone of Physical AI.\n1.3. Target Audience\n- Primary: Beginner to intermediate students in Physical AI and Humanoid Robotics.\n- Secondary: Hobbyists and developers looking to expand their skills into robotics simulation.\n2. Functional Requirements\n2.1. In Scope (Features)\n- Gazebo Physics Simulation:\n- Detailed explanation of core physics concepts: gravity, friction, and collision models.\n- A tutorial on building a simple environment (e.g., a room with obstacles) using SDF.\n- A demonstration of a humanoid robot interacting with the environment.\n- Unity for High-Fidelity Rendering:\n- An overview of Unity's rendering pipeline and its application in robotics.\n- A guide to setting up a simple Unity scene for robotic visualization.\n- Sensor Simulation:\n- A guide to simulating common robotic sensors: LiDAR, Depth Cameras, and IMUs in both Gazebo and Unity.\n- Configuration examples for each sensor.\n- Runnable Examples:\n- The module must include 2\u20133 complete, runnable code examples demonstrating the concepts.\n- Format: The final output must be a Docusaurus-compatible Markdown file.\n2.2. Out of Scope\n- Full-scale game development or advanced Unity features.\n- Complex humanoid control algorithms (this is reserved for Module 3).\n- Networked or multi-agent simulations.\n2.3. Success Criteria\n- Knowledge: Students can clearly explain the role of physics engines (Gazebo) and rendering engines (Unity) in creating a digital twin.\n- Practical Skill (Gazebo): Students can successfully build a simple Gazebo world and simulate a humanoid robot within it.\n- Practical Skill (Unity): Students can set up a Unity scene to render a robot model.\n- Practical Skill (Sensors): Students are able to add and configure virtual LiDAR, Depth Camera, and IMU sensors to a simulated robot.\n- Completion: The module contains 2-3 fully functional examples that run without errors.\n2.4. Constraints\n- Word Count: 2000\u20133000 words.\n- Format: Docusaurus Markdown, including formatted code blocks for SDF, C#, and Python/C++.\n- Sources: Content must be based on official documentation from Gazebo, Unity, and relevant sensor simulation libraries.\n- Timeline: 1 week for completion.\n3. UX/UI (Reader Experience)\n3.1. Content Structure\nThe module will be structured into clear, logical chapters to guide the reader progressively.\n- Chapter 1: Gazebo Physics & Environments\n- Introduction to Gazebo as a physics simulator.\n- Core concepts: gravity, collisions, and inertia.\n- Tutorial: Creating a\n.world\nfile with ground plane and basic shapes. - Example: Spawning a humanoid model and observing basic physical interactions.\n- Chapter 2: High-Fidelity Rendering with Unity\n- The role of Unity in robotics for high-quality visualization.\n- Setting up a Unity project for robotics (e.g., using ROS-Unity connectors).\n- Example: Importing a URDF model and creating a simple visualization scene.\n- Chapter 3: Simulating the Senses\n- Introduction to sensor simulation.\n- Simulating LiDAR and visualizing point clouds.\n- Simulating Depth Cameras and interpreting depth images.\n- Simulating an IMU to get orientation and acceleration data.\n- Example: A single robot model equipped with all three sensors, with configuration snippets for each.\n4. Technical Considerations\n4.1. Key Decisions\n- Simulation Tools: Gazebo will be used for robust physics simulation, while Unity will be highlighted for its superior rendering capabilities. The text will explain the trade-offs.\n- Code Examples: Examples will be provided as self-contained code blocks that can be easily copied and run.\n- Structure: The content will follow the chapter structure defined in section 3.1.\n4.2. External Dependencies\n- The module assumes readers have access to and a basic understanding of a Linux environment (for Gazebo/ROS).\n- Readers will need to have Gazebo and Unity Hub (with a recent Unity version) installed.\n- Code examples may rely on ROS 2,\nrclpy\n, orroscpp\nfor integration, building on concepts from Module 1.\n--------------------------------------------------------------------------------"
  },
  {
    "url": "https://Muham251.github.io/ai-hackathon-1/docs/module-2/#23-success-criteria",
    "text": "Spec: Module 2: The Digital Twin (Gazebo & Unity)\n1. Business Understanding\n1.1. Goal\nThe goal of this module is to educate students on creating and utilizing digital twins for humanoid robotics. The focus is on mastering physics simulation, environment construction, and sensor simulation within Gazebo and leveraging Unity for high-fidelity rendering.\n1.2. Justification\nDigital twins are fundamental to modern robotics development, enabling safe, cost-effective, and efficient testing of robot behaviors before deployment. This module provides the essential skills for simulating robots and their interactions with the environment, which is a cornerstone of Physical AI.\n1.3. Target Audience\n- Primary: Beginner to intermediate students in Physical AI and Humanoid Robotics.\n- Secondary: Hobbyists and developers looking to expand their skills into robotics simulation.\n2. Functional Requirements\n2.1. In Scope (Features)\n- Gazebo Physics Simulation:\n- Detailed explanation of core physics concepts: gravity, friction, and collision models.\n- A tutorial on building a simple environment (e.g., a room with obstacles) using SDF.\n- A demonstration of a humanoid robot interacting with the environment.\n- Unity for High-Fidelity Rendering:\n- An overview of Unity's rendering pipeline and its application in robotics.\n- A guide to setting up a simple Unity scene for robotic visualization.\n- Sensor Simulation:\n- A guide to simulating common robotic sensors: LiDAR, Depth Cameras, and IMUs in both Gazebo and Unity.\n- Configuration examples for each sensor.\n- Runnable Examples:\n- The module must include 2\u20133 complete, runnable code examples demonstrating the concepts.\n- Format: The final output must be a Docusaurus-compatible Markdown file.\n2.2. Out of Scope\n- Full-scale game development or advanced Unity features.\n- Complex humanoid control algorithms (this is reserved for Module 3).\n- Networked or multi-agent simulations.\n2.3. Success Criteria\n- Knowledge: Students can clearly explain the role of physics engines (Gazebo) and rendering engines (Unity) in creating a digital twin.\n- Practical Skill (Gazebo): Students can successfully build a simple Gazebo world and simulate a humanoid robot within it.\n- Practical Skill (Unity): Students can set up a Unity scene to render a robot model.\n- Practical Skill (Sensors): Students are able to add and configure virtual LiDAR, Depth Camera, and IMU sensors to a simulated robot.\n- Completion: The module contains 2-3 fully functional examples that run without errors.\n2.4. Constraints\n- Word Count: 2000\u20133000 words.\n- Format: Docusaurus Markdown, including formatted code blocks for SDF, C#, and Python/C++.\n- Sources: Content must be based on official documentation from Gazebo, Unity, and relevant sensor simulation libraries.\n- Timeline: 1 week for completion.\n3. UX/UI (Reader Experience)\n3.1. Content Structure\nThe module will be structured into clear, logical chapters to guide the reader progressively.\n- Chapter 1: Gazebo Physics & Environments\n- Introduction to Gazebo as a physics simulator.\n- Core concepts: gravity, collisions, and inertia.\n- Tutorial: Creating a\n.world\nfile with ground plane and basic shapes. - Example: Spawning a humanoid model and observing basic physical interactions.\n- Chapter 2: High-Fidelity Rendering with Unity\n- The role of Unity in robotics for high-quality visualization.\n- Setting up a Unity project for robotics (e.g., using ROS-Unity connectors).\n- Example: Importing a URDF model and creating a simple visualization scene.\n- Chapter 3: Simulating the Senses\n- Introduction to sensor simulation.\n- Simulating LiDAR and visualizing point clouds.\n- Simulating Depth Cameras and interpreting depth images.\n- Simulating an IMU to get orientation and acceleration data.\n- Example: A single robot model equipped with all three sensors, with configuration snippets for each.\n4. Technical Considerations\n4.1. Key Decisions\n- Simulation Tools: Gazebo will be used for robust physics simulation, while Unity will be highlighted for its superior rendering capabilities. The text will explain the trade-offs.\n- Code Examples: Examples will be provided as self-contained code blocks that can be easily copied and run.\n- Structure: The content will follow the chapter structure defined in section 3.1.\n4.2. External Dependencies\n- The module assumes readers have access to and a basic understanding of a Linux environment (for Gazebo/ROS).\n- Readers will need to have Gazebo and Unity Hub (with a recent Unity version) installed.\n- Code examples may rely on ROS 2,\nrclpy\n, orroscpp\nfor integration, building on concepts from Module 1.\n--------------------------------------------------------------------------------"
  },
  {
    "url": "https://Muham251.github.io/ai-hackathon-1/docs/module-2/#24-constraints",
    "text": "Spec: Module 2: The Digital Twin (Gazebo & Unity)\n1. Business Understanding\n1.1. Goal\nThe goal of this module is to educate students on creating and utilizing digital twins for humanoid robotics. The focus is on mastering physics simulation, environment construction, and sensor simulation within Gazebo and leveraging Unity for high-fidelity rendering.\n1.2. Justification\nDigital twins are fundamental to modern robotics development, enabling safe, cost-effective, and efficient testing of robot behaviors before deployment. This module provides the essential skills for simulating robots and their interactions with the environment, which is a cornerstone of Physical AI.\n1.3. Target Audience\n- Primary: Beginner to intermediate students in Physical AI and Humanoid Robotics.\n- Secondary: Hobbyists and developers looking to expand their skills into robotics simulation.\n2. Functional Requirements\n2.1. In Scope (Features)\n- Gazebo Physics Simulation:\n- Detailed explanation of core physics concepts: gravity, friction, and collision models.\n- A tutorial on building a simple environment (e.g., a room with obstacles) using SDF.\n- A demonstration of a humanoid robot interacting with the environment.\n- Unity for High-Fidelity Rendering:\n- An overview of Unity's rendering pipeline and its application in robotics.\n- A guide to setting up a simple Unity scene for robotic visualization.\n- Sensor Simulation:\n- A guide to simulating common robotic sensors: LiDAR, Depth Cameras, and IMUs in both Gazebo and Unity.\n- Configuration examples for each sensor.\n- Runnable Examples:\n- The module must include 2\u20133 complete, runnable code examples demonstrating the concepts.\n- Format: The final output must be a Docusaurus-compatible Markdown file.\n2.2. Out of Scope\n- Full-scale game development or advanced Unity features.\n- Complex humanoid control algorithms (this is reserved for Module 3).\n- Networked or multi-agent simulations.\n2.3. Success Criteria\n- Knowledge: Students can clearly explain the role of physics engines (Gazebo) and rendering engines (Unity) in creating a digital twin.\n- Practical Skill (Gazebo): Students can successfully build a simple Gazebo world and simulate a humanoid robot within it.\n- Practical Skill (Unity): Students can set up a Unity scene to render a robot model.\n- Practical Skill (Sensors): Students are able to add and configure virtual LiDAR, Depth Camera, and IMU sensors to a simulated robot.\n- Completion: The module contains 2-3 fully functional examples that run without errors.\n2.4. Constraints\n- Word Count: 2000\u20133000 words.\n- Format: Docusaurus Markdown, including formatted code blocks for SDF, C#, and Python/C++.\n- Sources: Content must be based on official documentation from Gazebo, Unity, and relevant sensor simulation libraries.\n- Timeline: 1 week for completion.\n3. UX/UI (Reader Experience)\n3.1. Content Structure\nThe module will be structured into clear, logical chapters to guide the reader progressively.\n- Chapter 1: Gazebo Physics & Environments\n- Introduction to Gazebo as a physics simulator.\n- Core concepts: gravity, collisions, and inertia.\n- Tutorial: Creating a\n.world\nfile with ground plane and basic shapes. - Example: Spawning a humanoid model and observing basic physical interactions.\n- Chapter 2: High-Fidelity Rendering with Unity\n- The role of Unity in robotics for high-quality visualization.\n- Setting up a Unity project for robotics (e.g., using ROS-Unity connectors).\n- Example: Importing a URDF model and creating a simple visualization scene.\n- Chapter 3: Simulating the Senses\n- Introduction to sensor simulation.\n- Simulating LiDAR and visualizing point clouds.\n- Simulating Depth Cameras and interpreting depth images.\n- Simulating an IMU to get orientation and acceleration data.\n- Example: A single robot model equipped with all three sensors, with configuration snippets for each.\n4. Technical Considerations\n4.1. Key Decisions\n- Simulation Tools: Gazebo will be used for robust physics simulation, while Unity will be highlighted for its superior rendering capabilities. The text will explain the trade-offs.\n- Code Examples: Examples will be provided as self-contained code blocks that can be easily copied and run.\n- Structure: The content will follow the chapter structure defined in section 3.1.\n4.2. External Dependencies\n- The module assumes readers have access to and a basic understanding of a Linux environment (for Gazebo/ROS).\n- Readers will need to have Gazebo and Unity Hub (with a recent Unity version) installed.\n- Code examples may rely on ROS 2,\nrclpy\n, orroscpp\nfor integration, building on concepts from Module 1.\n--------------------------------------------------------------------------------"
  },
  {
    "url": "https://Muham251.github.io/ai-hackathon-1/docs/module-2/#3-uxui-reader-experience",
    "text": "Spec: Module 2: The Digital Twin (Gazebo & Unity)\n1. Business Understanding\n1.1. Goal\nThe goal of this module is to educate students on creating and utilizing digital twins for humanoid robotics. The focus is on mastering physics simulation, environment construction, and sensor simulation within Gazebo and leveraging Unity for high-fidelity rendering.\n1.2. Justification\nDigital twins are fundamental to modern robotics development, enabling safe, cost-effective, and efficient testing of robot behaviors before deployment. This module provides the essential skills for simulating robots and their interactions with the environment, which is a cornerstone of Physical AI.\n1.3. Target Audience\n- Primary: Beginner to intermediate students in Physical AI and Humanoid Robotics.\n- Secondary: Hobbyists and developers looking to expand their skills into robotics simulation.\n2. Functional Requirements\n2.1. In Scope (Features)\n- Gazebo Physics Simulation:\n- Detailed explanation of core physics concepts: gravity, friction, and collision models.\n- A tutorial on building a simple environment (e.g., a room with obstacles) using SDF.\n- A demonstration of a humanoid robot interacting with the environment.\n- Unity for High-Fidelity Rendering:\n- An overview of Unity's rendering pipeline and its application in robotics.\n- A guide to setting up a simple Unity scene for robotic visualization.\n- Sensor Simulation:\n- A guide to simulating common robotic sensors: LiDAR, Depth Cameras, and IMUs in both Gazebo and Unity.\n- Configuration examples for each sensor.\n- Runnable Examples:\n- The module must include 2\u20133 complete, runnable code examples demonstrating the concepts.\n- Format: The final output must be a Docusaurus-compatible Markdown file.\n2.2. Out of Scope\n- Full-scale game development or advanced Unity features.\n- Complex humanoid control algorithms (this is reserved for Module 3).\n- Networked or multi-agent simulations.\n2.3. Success Criteria\n- Knowledge: Students can clearly explain the role of physics engines (Gazebo) and rendering engines (Unity) in creating a digital twin.\n- Practical Skill (Gazebo): Students can successfully build a simple Gazebo world and simulate a humanoid robot within it.\n- Practical Skill (Unity): Students can set up a Unity scene to render a robot model.\n- Practical Skill (Sensors): Students are able to add and configure virtual LiDAR, Depth Camera, and IMU sensors to a simulated robot.\n- Completion: The module contains 2-3 fully functional examples that run without errors.\n2.4. Constraints\n- Word Count: 2000\u20133000 words.\n- Format: Docusaurus Markdown, including formatted code blocks for SDF, C#, and Python/C++.\n- Sources: Content must be based on official documentation from Gazebo, Unity, and relevant sensor simulation libraries.\n- Timeline: 1 week for completion.\n3. UX/UI (Reader Experience)\n3.1. Content Structure\nThe module will be structured into clear, logical chapters to guide the reader progressively.\n- Chapter 1: Gazebo Physics & Environments\n- Introduction to Gazebo as a physics simulator.\n- Core concepts: gravity, collisions, and inertia.\n- Tutorial: Creating a\n.world\nfile with ground plane and basic shapes. - Example: Spawning a humanoid model and observing basic physical interactions.\n- Chapter 2: High-Fidelity Rendering with Unity\n- The role of Unity in robotics for high-quality visualization.\n- Setting up a Unity project for robotics (e.g., using ROS-Unity connectors).\n- Example: Importing a URDF model and creating a simple visualization scene.\n- Chapter 3: Simulating the Senses\n- Introduction to sensor simulation.\n- Simulating LiDAR and visualizing point clouds.\n- Simulating Depth Cameras and interpreting depth images.\n- Simulating an IMU to get orientation and acceleration data.\n- Example: A single robot model equipped with all three sensors, with configuration snippets for each.\n4. Technical Considerations\n4.1. Key Decisions\n- Simulation Tools: Gazebo will be used for robust physics simulation, while Unity will be highlighted for its superior rendering capabilities. The text will explain the trade-offs.\n- Code Examples: Examples will be provided as self-contained code blocks that can be easily copied and run.\n- Structure: The content will follow the chapter structure defined in section 3.1.\n4.2. External Dependencies\n- The module assumes readers have access to and a basic understanding of a Linux environment (for Gazebo/ROS).\n- Readers will need to have Gazebo and Unity Hub (with a recent Unity version) installed.\n- Code examples may rely on ROS 2,\nrclpy\n, orroscpp\nfor integration, building on concepts from Module 1.\n--------------------------------------------------------------------------------"
  },
  {
    "url": "https://Muham251.github.io/ai-hackathon-1/docs/module-2/#31-content-structure",
    "text": "Spec: Module 2: The Digital Twin (Gazebo & Unity)\n1. Business Understanding\n1.1. Goal\nThe goal of this module is to educate students on creating and utilizing digital twins for humanoid robotics. The focus is on mastering physics simulation, environment construction, and sensor simulation within Gazebo and leveraging Unity for high-fidelity rendering.\n1.2. Justification\nDigital twins are fundamental to modern robotics development, enabling safe, cost-effective, and efficient testing of robot behaviors before deployment. This module provides the essential skills for simulating robots and their interactions with the environment, which is a cornerstone of Physical AI.\n1.3. Target Audience\n- Primary: Beginner to intermediate students in Physical AI and Humanoid Robotics.\n- Secondary: Hobbyists and developers looking to expand their skills into robotics simulation.\n2. Functional Requirements\n2.1. In Scope (Features)\n- Gazebo Physics Simulation:\n- Detailed explanation of core physics concepts: gravity, friction, and collision models.\n- A tutorial on building a simple environment (e.g., a room with obstacles) using SDF.\n- A demonstration of a humanoid robot interacting with the environment.\n- Unity for High-Fidelity Rendering:\n- An overview of Unity's rendering pipeline and its application in robotics.\n- A guide to setting up a simple Unity scene for robotic visualization.\n- Sensor Simulation:\n- A guide to simulating common robotic sensors: LiDAR, Depth Cameras, and IMUs in both Gazebo and Unity.\n- Configuration examples for each sensor.\n- Runnable Examples:\n- The module must include 2\u20133 complete, runnable code examples demonstrating the concepts.\n- Format: The final output must be a Docusaurus-compatible Markdown file.\n2.2. Out of Scope\n- Full-scale game development or advanced Unity features.\n- Complex humanoid control algorithms (this is reserved for Module 3).\n- Networked or multi-agent simulations.\n2.3. Success Criteria\n- Knowledge: Students can clearly explain the role of physics engines (Gazebo) and rendering engines (Unity) in creating a digital twin.\n- Practical Skill (Gazebo): Students can successfully build a simple Gazebo world and simulate a humanoid robot within it.\n- Practical Skill (Unity): Students can set up a Unity scene to render a robot model.\n- Practical Skill (Sensors): Students are able to add and configure virtual LiDAR, Depth Camera, and IMU sensors to a simulated robot.\n- Completion: The module contains 2-3 fully functional examples that run without errors.\n2.4. Constraints\n- Word Count: 2000\u20133000 words.\n- Format: Docusaurus Markdown, including formatted code blocks for SDF, C#, and Python/C++.\n- Sources: Content must be based on official documentation from Gazebo, Unity, and relevant sensor simulation libraries.\n- Timeline: 1 week for completion.\n3. UX/UI (Reader Experience)\n3.1. Content Structure\nThe module will be structured into clear, logical chapters to guide the reader progressively.\n- Chapter 1: Gazebo Physics & Environments\n- Introduction to Gazebo as a physics simulator.\n- Core concepts: gravity, collisions, and inertia.\n- Tutorial: Creating a\n.world\nfile with ground plane and basic shapes. - Example: Spawning a humanoid model and observing basic physical interactions.\n- Chapter 2: High-Fidelity Rendering with Unity\n- The role of Unity in robotics for high-quality visualization.\n- Setting up a Unity project for robotics (e.g., using ROS-Unity connectors).\n- Example: Importing a URDF model and creating a simple visualization scene.\n- Chapter 3: Simulating the Senses\n- Introduction to sensor simulation.\n- Simulating LiDAR and visualizing point clouds.\n- Simulating Depth Cameras and interpreting depth images.\n- Simulating an IMU to get orientation and acceleration data.\n- Example: A single robot model equipped with all three sensors, with configuration snippets for each.\n4. Technical Considerations\n4.1. Key Decisions\n- Simulation Tools: Gazebo will be used for robust physics simulation, while Unity will be highlighted for its superior rendering capabilities. The text will explain the trade-offs.\n- Code Examples: Examples will be provided as self-contained code blocks that can be easily copied and run.\n- Structure: The content will follow the chapter structure defined in section 3.1.\n4.2. External Dependencies\n- The module assumes readers have access to and a basic understanding of a Linux environment (for Gazebo/ROS).\n- Readers will need to have Gazebo and Unity Hub (with a recent Unity version) installed.\n- Code examples may rely on ROS 2,\nrclpy\n, orroscpp\nfor integration, building on concepts from Module 1.\n--------------------------------------------------------------------------------"
  },
  {
    "url": "https://Muham251.github.io/ai-hackathon-1/docs/module-2/#4-technical-considerations",
    "text": "Spec: Module 2: The Digital Twin (Gazebo & Unity)\n1. Business Understanding\n1.1. Goal\nThe goal of this module is to educate students on creating and utilizing digital twins for humanoid robotics. The focus is on mastering physics simulation, environment construction, and sensor simulation within Gazebo and leveraging Unity for high-fidelity rendering.\n1.2. Justification\nDigital twins are fundamental to modern robotics development, enabling safe, cost-effective, and efficient testing of robot behaviors before deployment. This module provides the essential skills for simulating robots and their interactions with the environment, which is a cornerstone of Physical AI.\n1.3. Target Audience\n- Primary: Beginner to intermediate students in Physical AI and Humanoid Robotics.\n- Secondary: Hobbyists and developers looking to expand their skills into robotics simulation.\n2. Functional Requirements\n2.1. In Scope (Features)\n- Gazebo Physics Simulation:\n- Detailed explanation of core physics concepts: gravity, friction, and collision models.\n- A tutorial on building a simple environment (e.g., a room with obstacles) using SDF.\n- A demonstration of a humanoid robot interacting with the environment.\n- Unity for High-Fidelity Rendering:\n- An overview of Unity's rendering pipeline and its application in robotics.\n- A guide to setting up a simple Unity scene for robotic visualization.\n- Sensor Simulation:\n- A guide to simulating common robotic sensors: LiDAR, Depth Cameras, and IMUs in both Gazebo and Unity.\n- Configuration examples for each sensor.\n- Runnable Examples:\n- The module must include 2\u20133 complete, runnable code examples demonstrating the concepts.\n- Format: The final output must be a Docusaurus-compatible Markdown file.\n2.2. Out of Scope\n- Full-scale game development or advanced Unity features.\n- Complex humanoid control algorithms (this is reserved for Module 3).\n- Networked or multi-agent simulations.\n2.3. Success Criteria\n- Knowledge: Students can clearly explain the role of physics engines (Gazebo) and rendering engines (Unity) in creating a digital twin.\n- Practical Skill (Gazebo): Students can successfully build a simple Gazebo world and simulate a humanoid robot within it.\n- Practical Skill (Unity): Students can set up a Unity scene to render a robot model.\n- Practical Skill (Sensors): Students are able to add and configure virtual LiDAR, Depth Camera, and IMU sensors to a simulated robot.\n- Completion: The module contains 2-3 fully functional examples that run without errors.\n2.4. Constraints\n- Word Count: 2000\u20133000 words.\n- Format: Docusaurus Markdown, including formatted code blocks for SDF, C#, and Python/C++.\n- Sources: Content must be based on official documentation from Gazebo, Unity, and relevant sensor simulation libraries.\n- Timeline: 1 week for completion.\n3. UX/UI (Reader Experience)\n3.1. Content Structure\nThe module will be structured into clear, logical chapters to guide the reader progressively.\n- Chapter 1: Gazebo Physics & Environments\n- Introduction to Gazebo as a physics simulator.\n- Core concepts: gravity, collisions, and inertia.\n- Tutorial: Creating a\n.world\nfile with ground plane and basic shapes. - Example: Spawning a humanoid model and observing basic physical interactions.\n- Chapter 2: High-Fidelity Rendering with Unity\n- The role of Unity in robotics for high-quality visualization.\n- Setting up a Unity project for robotics (e.g., using ROS-Unity connectors).\n- Example: Importing a URDF model and creating a simple visualization scene.\n- Chapter 3: Simulating the Senses\n- Introduction to sensor simulation.\n- Simulating LiDAR and visualizing point clouds.\n- Simulating Depth Cameras and interpreting depth images.\n- Simulating an IMU to get orientation and acceleration data.\n- Example: A single robot model equipped with all three sensors, with configuration snippets for each.\n4. Technical Considerations\n4.1. Key Decisions\n- Simulation Tools: Gazebo will be used for robust physics simulation, while Unity will be highlighted for its superior rendering capabilities. The text will explain the trade-offs.\n- Code Examples: Examples will be provided as self-contained code blocks that can be easily copied and run.\n- Structure: The content will follow the chapter structure defined in section 3.1.\n4.2. External Dependencies\n- The module assumes readers have access to and a basic understanding of a Linux environment (for Gazebo/ROS).\n- Readers will need to have Gazebo and Unity Hub (with a recent Unity version) installed.\n- Code examples may rely on ROS 2,\nrclpy\n, orroscpp\nfor integration, building on concepts from Module 1.\n--------------------------------------------------------------------------------"
  },
  {
    "url": "https://Muham251.github.io/ai-hackathon-1/docs/module-2/#41-key-decisions",
    "text": "Spec: Module 2: The Digital Twin (Gazebo & Unity)\n1. Business Understanding\n1.1. Goal\nThe goal of this module is to educate students on creating and utilizing digital twins for humanoid robotics. The focus is on mastering physics simulation, environment construction, and sensor simulation within Gazebo and leveraging Unity for high-fidelity rendering.\n1.2. Justification\nDigital twins are fundamental to modern robotics development, enabling safe, cost-effective, and efficient testing of robot behaviors before deployment. This module provides the essential skills for simulating robots and their interactions with the environment, which is a cornerstone of Physical AI.\n1.3. Target Audience\n- Primary: Beginner to intermediate students in Physical AI and Humanoid Robotics.\n- Secondary: Hobbyists and developers looking to expand their skills into robotics simulation.\n2. Functional Requirements\n2.1. In Scope (Features)\n- Gazebo Physics Simulation:\n- Detailed explanation of core physics concepts: gravity, friction, and collision models.\n- A tutorial on building a simple environment (e.g., a room with obstacles) using SDF.\n- A demonstration of a humanoid robot interacting with the environment.\n- Unity for High-Fidelity Rendering:\n- An overview of Unity's rendering pipeline and its application in robotics.\n- A guide to setting up a simple Unity scene for robotic visualization.\n- Sensor Simulation:\n- A guide to simulating common robotic sensors: LiDAR, Depth Cameras, and IMUs in both Gazebo and Unity.\n- Configuration examples for each sensor.\n- Runnable Examples:\n- The module must include 2\u20133 complete, runnable code examples demonstrating the concepts.\n- Format: The final output must be a Docusaurus-compatible Markdown file.\n2.2. Out of Scope\n- Full-scale game development or advanced Unity features.\n- Complex humanoid control algorithms (this is reserved for Module 3).\n- Networked or multi-agent simulations.\n2.3. Success Criteria\n- Knowledge: Students can clearly explain the role of physics engines (Gazebo) and rendering engines (Unity) in creating a digital twin.\n- Practical Skill (Gazebo): Students can successfully build a simple Gazebo world and simulate a humanoid robot within it.\n- Practical Skill (Unity): Students can set up a Unity scene to render a robot model.\n- Practical Skill (Sensors): Students are able to add and configure virtual LiDAR, Depth Camera, and IMU sensors to a simulated robot.\n- Completion: The module contains 2-3 fully functional examples that run without errors.\n2.4. Constraints\n- Word Count: 2000\u20133000 words.\n- Format: Docusaurus Markdown, including formatted code blocks for SDF, C#, and Python/C++.\n- Sources: Content must be based on official documentation from Gazebo, Unity, and relevant sensor simulation libraries.\n- Timeline: 1 week for completion.\n3. UX/UI (Reader Experience)\n3.1. Content Structure\nThe module will be structured into clear, logical chapters to guide the reader progressively.\n- Chapter 1: Gazebo Physics & Environments\n- Introduction to Gazebo as a physics simulator.\n- Core concepts: gravity, collisions, and inertia.\n- Tutorial: Creating a\n.world\nfile with ground plane and basic shapes. - Example: Spawning a humanoid model and observing basic physical interactions.\n- Chapter 2: High-Fidelity Rendering with Unity\n- The role of Unity in robotics for high-quality visualization.\n- Setting up a Unity project for robotics (e.g., using ROS-Unity connectors).\n- Example: Importing a URDF model and creating a simple visualization scene.\n- Chapter 3: Simulating the Senses\n- Introduction to sensor simulation.\n- Simulating LiDAR and visualizing point clouds.\n- Simulating Depth Cameras and interpreting depth images.\n- Simulating an IMU to get orientation and acceleration data.\n- Example: A single robot model equipped with all three sensors, with configuration snippets for each.\n4. Technical Considerations\n4.1. Key Decisions\n- Simulation Tools: Gazebo will be used for robust physics simulation, while Unity will be highlighted for its superior rendering capabilities. The text will explain the trade-offs.\n- Code Examples: Examples will be provided as self-contained code blocks that can be easily copied and run.\n- Structure: The content will follow the chapter structure defined in section 3.1.\n4.2. External Dependencies\n- The module assumes readers have access to and a basic understanding of a Linux environment (for Gazebo/ROS).\n- Readers will need to have Gazebo and Unity Hub (with a recent Unity version) installed.\n- Code examples may rely on ROS 2,\nrclpy\n, orroscpp\nfor integration, building on concepts from Module 1.\n--------------------------------------------------------------------------------"
  },
  {
    "url": "https://Muham251.github.io/ai-hackathon-1/docs/module-2/#42-external-dependencies",
    "text": "Spec: Module 2: The Digital Twin (Gazebo & Unity)\n1. Business Understanding\n1.1. Goal\nThe goal of this module is to educate students on creating and utilizing digital twins for humanoid robotics. The focus is on mastering physics simulation, environment construction, and sensor simulation within Gazebo and leveraging Unity for high-fidelity rendering.\n1.2. Justification\nDigital twins are fundamental to modern robotics development, enabling safe, cost-effective, and efficient testing of robot behaviors before deployment. This module provides the essential skills for simulating robots and their interactions with the environment, which is a cornerstone of Physical AI.\n1.3. Target Audience\n- Primary: Beginner to intermediate students in Physical AI and Humanoid Robotics.\n- Secondary: Hobbyists and developers looking to expand their skills into robotics simulation.\n2. Functional Requirements\n2.1. In Scope (Features)\n- Gazebo Physics Simulation:\n- Detailed explanation of core physics concepts: gravity, friction, and collision models.\n- A tutorial on building a simple environment (e.g., a room with obstacles) using SDF.\n- A demonstration of a humanoid robot interacting with the environment.\n- Unity for High-Fidelity Rendering:\n- An overview of Unity's rendering pipeline and its application in robotics.\n- A guide to setting up a simple Unity scene for robotic visualization.\n- Sensor Simulation:\n- A guide to simulating common robotic sensors: LiDAR, Depth Cameras, and IMUs in both Gazebo and Unity.\n- Configuration examples for each sensor.\n- Runnable Examples:\n- The module must include 2\u20133 complete, runnable code examples demonstrating the concepts.\n- Format: The final output must be a Docusaurus-compatible Markdown file.\n2.2. Out of Scope\n- Full-scale game development or advanced Unity features.\n- Complex humanoid control algorithms (this is reserved for Module 3).\n- Networked or multi-agent simulations.\n2.3. Success Criteria\n- Knowledge: Students can clearly explain the role of physics engines (Gazebo) and rendering engines (Unity) in creating a digital twin.\n- Practical Skill (Gazebo): Students can successfully build a simple Gazebo world and simulate a humanoid robot within it.\n- Practical Skill (Unity): Students can set up a Unity scene to render a robot model.\n- Practical Skill (Sensors): Students are able to add and configure virtual LiDAR, Depth Camera, and IMU sensors to a simulated robot.\n- Completion: The module contains 2-3 fully functional examples that run without errors.\n2.4. Constraints\n- Word Count: 2000\u20133000 words.\n- Format: Docusaurus Markdown, including formatted code blocks for SDF, C#, and Python/C++.\n- Sources: Content must be based on official documentation from Gazebo, Unity, and relevant sensor simulation libraries.\n- Timeline: 1 week for completion.\n3. UX/UI (Reader Experience)\n3.1. Content Structure\nThe module will be structured into clear, logical chapters to guide the reader progressively.\n- Chapter 1: Gazebo Physics & Environments\n- Introduction to Gazebo as a physics simulator.\n- Core concepts: gravity, collisions, and inertia.\n- Tutorial: Creating a\n.world\nfile with ground plane and basic shapes. - Example: Spawning a humanoid model and observing basic physical interactions.\n- Chapter 2: High-Fidelity Rendering with Unity\n- The role of Unity in robotics for high-quality visualization.\n- Setting up a Unity project for robotics (e.g., using ROS-Unity connectors).\n- Example: Importing a URDF model and creating a simple visualization scene.\n- Chapter 3: Simulating the Senses\n- Introduction to sensor simulation.\n- Simulating LiDAR and visualizing point clouds.\n- Simulating Depth Cameras and interpreting depth images.\n- Simulating an IMU to get orientation and acceleration data.\n- Example: A single robot model equipped with all three sensors, with configuration snippets for each.\n4. Technical Considerations\n4.1. Key Decisions\n- Simulation Tools: Gazebo will be used for robust physics simulation, while Unity will be highlighted for its superior rendering capabilities. The text will explain the trade-offs.\n- Code Examples: Examples will be provided as self-contained code blocks that can be easily copied and run.\n- Structure: The content will follow the chapter structure defined in section 3.1.\n4.2. External Dependencies\n- The module assumes readers have access to and a basic understanding of a Linux environment (for Gazebo/ROS).\n- Readers will need to have Gazebo and Unity Hub (with a recent Unity version) installed.\n- Code examples may rely on ROS 2,\nrclpy\n, orroscpp\nfor integration, building on concepts from Module 1.\n--------------------------------------------------------------------------------"
  },
  {
    "url": "https://Muham251.github.io/ai-hackathon-1/docs/tutorial-basics/congratulations#whats-next",
    "text": "Congratulations!\nYou have just learned the basics of Docusaurus and made some changes to the initial template.\nDocusaurus has much more to offer!\nHave 5 more minutes? Take a look at versioning and i18n.\nAnything unclear or buggy in this tutorial? Please report it!\nWhat's next?\n- Read the official documentation\n- Modify your site configuration with\ndocusaurus.config.js\n- Add navbar and footer items with\nthemeConfig\n- Add a custom Design and Layout\n- Add a search bar\n- Find inspirations in the Docusaurus showcase\n- Get involved in the Docusaurus Community\n--------------------------------------------------------------------------------"
  },
  {
    "url": "https://Muham251.github.io/ai-hackathon-1/docs/tutorial-basics/deploy-your-site#build-your-site",
    "text": "Deploy your site\nDocusaurus is a static-site-generator (also called Jamstack).\nIt builds your site as simple static HTML, JavaScript and CSS files.\nBuild your site\nBuild your site for production:\nnpm run build\nThe static files are generated in the build\nfolder.\nDeploy your site\nTest your production build locally:\nnpm run serve\nThe build\nfolder is now served at http://localhost:3000/.\nYou can now deploy the build\nfolder almost anywhere easily, for free or very small cost (read the Deployment Guide).\n--------------------------------------------------------------------------------"
  },
  {
    "url": "https://Muham251.github.io/ai-hackathon-1/docs/tutorial-basics/deploy-your-site#deploy-your-site-1",
    "text": "Deploy your site\nDocusaurus is a static-site-generator (also called Jamstack).\nIt builds your site as simple static HTML, JavaScript and CSS files.\nBuild your site\nBuild your site for production:\nnpm run build\nThe static files are generated in the build\nfolder.\nDeploy your site\nTest your production build locally:\nnpm run serve\nThe build\nfolder is now served at http://localhost:3000/.\nYou can now deploy the build\nfolder almost anywhere easily, for free or very small cost (read the Deployment Guide).\n--------------------------------------------------------------------------------"
  },
  {
    "url": "https://Muham251.github.io/ai-hackathon-1/docs/tutorial-basics/markdown-features#front-matter",
    "text": "Markdown Features\nDocusaurus supports Markdown and a few additional features.\nFront Matter\nMarkdown documents have metadata at the top called Front Matter:\n---\nid: my-doc-id\ntitle: My document title\ndescription: My document description\nslug: /my-custom-url\n---\n## Markdown heading\nMarkdown text with [links](./hello.md)\nLinks\nRegular Markdown links are supported, using url paths or relative file paths.\nLet's see how to [Create a page](/create-a-page).\nLet's see how to [Create a page](./create-a-page.md).\nResult: Let's see how to Create a page.\nImages\nRegular Markdown images are supported.\nYou can use absolute paths to reference images in the static directory (static/img/docusaurus.png\n):\n![Docusaurus logo](/img/docusaurus.png)\nYou can reference images relative to the current file as well. This is particularly useful to colocate images close to the Markdown files using them:\n![Docusaurus logo](./img/docusaurus.png)\nCode Blocks\nMarkdown code blocks are supported with Syntax highlighting.\n```jsx title=\"src/components/HelloDocusaurus.js\"\nfunction HelloDocusaurus() {\nreturn <h1>Hello, Docusaurus!</h1>;\n}\n```\nfunction HelloDocusaurus() {\nreturn <h1>Hello, Docusaurus!</h1>;\n}\nAdmonitions\nDocusaurus has a special syntax to create admonitions and callouts:\n:::tip[My tip]\nUse this awesome feature option\n:::\n:::danger[Take care]\nThis action is dangerous\n:::\nUse this awesome feature option\nThis action is dangerous\nMDX and React Components\nMDX can make your documentation more interactive and allows using any React components inside Markdown:\nexport const Highlight = ({children, color}) => (\n<span\nstyle={{\nbackgroundColor: color,\nborderRadius: '20px',\ncolor: '#fff',\npadding: '10px',\ncursor: 'pointer',\n}}\nonClick={() => {\nalert(`You clicked the color ${color} with label ${children}`)\n}}>\n{children}\n</span>\n);\nThis is <Highlight color=\"#25c2a0\">Docusaurus green</Highlight> !\nThis is <Highlight color=\"#1877F2\">Facebook blue</Highlight> !\nThis is Docusaurus green !\nThis is Facebook blue !\n--------------------------------------------------------------------------------"
  },
  {
    "url": "https://Muham251.github.io/ai-hackathon-1/docs/tutorial-basics/markdown-features#links",
    "text": "Markdown Features\nDocusaurus supports Markdown and a few additional features.\nFront Matter\nMarkdown documents have metadata at the top called Front Matter:\n---\nid: my-doc-id\ntitle: My document title\ndescription: My document description\nslug: /my-custom-url\n---\n## Markdown heading\nMarkdown text with [links](./hello.md)\nLinks\nRegular Markdown links are supported, using url paths or relative file paths.\nLet's see how to [Create a page](/create-a-page).\nLet's see how to [Create a page](./create-a-page.md).\nResult: Let's see how to Create a page.\nImages\nRegular Markdown images are supported.\nYou can use absolute paths to reference images in the static directory (static/img/docusaurus.png\n):\n![Docusaurus logo](/img/docusaurus.png)\nYou can reference images relative to the current file as well. This is particularly useful to colocate images close to the Markdown files using them:\n![Docusaurus logo](./img/docusaurus.png)\nCode Blocks\nMarkdown code blocks are supported with Syntax highlighting.\n```jsx title=\"src/components/HelloDocusaurus.js\"\nfunction HelloDocusaurus() {\nreturn <h1>Hello, Docusaurus!</h1>;\n}\n```\nfunction HelloDocusaurus() {\nreturn <h1>Hello, Docusaurus!</h1>;\n}\nAdmonitions\nDocusaurus has a special syntax to create admonitions and callouts:\n:::tip[My tip]\nUse this awesome feature option\n:::\n:::danger[Take care]\nThis action is dangerous\n:::\nUse this awesome feature option\nThis action is dangerous\nMDX and React Components\nMDX can make your documentation more interactive and allows using any React components inside Markdown:\nexport const Highlight = ({children, color}) => (\n<span\nstyle={{\nbackgroundColor: color,\nborderRadius: '20px',\ncolor: '#fff',\npadding: '10px',\ncursor: 'pointer',\n}}\nonClick={() => {\nalert(`You clicked the color ${color} with label ${children}`)\n}}>\n{children}\n</span>\n);\nThis is <Highlight color=\"#25c2a0\">Docusaurus green</Highlight> !\nThis is <Highlight color=\"#1877F2\">Facebook blue</Highlight> !\nThis is Docusaurus green !\nThis is Facebook blue !\n--------------------------------------------------------------------------------"
  },
  {
    "url": "https://Muham251.github.io/ai-hackathon-1/docs/tutorial-basics/markdown-features#images",
    "text": "Markdown Features\nDocusaurus supports Markdown and a few additional features.\nFront Matter\nMarkdown documents have metadata at the top called Front Matter:\n---\nid: my-doc-id\ntitle: My document title\ndescription: My document description\nslug: /my-custom-url\n---\n## Markdown heading\nMarkdown text with [links](./hello.md)\nLinks\nRegular Markdown links are supported, using url paths or relative file paths.\nLet's see how to [Create a page](/create-a-page).\nLet's see how to [Create a page](./create-a-page.md).\nResult: Let's see how to Create a page.\nImages\nRegular Markdown images are supported.\nYou can use absolute paths to reference images in the static directory (static/img/docusaurus.png\n):\n![Docusaurus logo](/img/docusaurus.png)\nYou can reference images relative to the current file as well. This is particularly useful to colocate images close to the Markdown files using them:\n![Docusaurus logo](./img/docusaurus.png)\nCode Blocks\nMarkdown code blocks are supported with Syntax highlighting.\n```jsx title=\"src/components/HelloDocusaurus.js\"\nfunction HelloDocusaurus() {\nreturn <h1>Hello, Docusaurus!</h1>;\n}\n```\nfunction HelloDocusaurus() {\nreturn <h1>Hello, Docusaurus!</h1>;\n}\nAdmonitions\nDocusaurus has a special syntax to create admonitions and callouts:\n:::tip[My tip]\nUse this awesome feature option\n:::\n:::danger[Take care]\nThis action is dangerous\n:::\nUse this awesome feature option\nThis action is dangerous\nMDX and React Components\nMDX can make your documentation more interactive and allows using any React components inside Markdown:\nexport const Highlight = ({children, color}) => (\n<span\nstyle={{\nbackgroundColor: color,\nborderRadius: '20px',\ncolor: '#fff',\npadding: '10px',\ncursor: 'pointer',\n}}\nonClick={() => {\nalert(`You clicked the color ${color} with label ${children}`)\n}}>\n{children}\n</span>\n);\nThis is <Highlight color=\"#25c2a0\">Docusaurus green</Highlight> !\nThis is <Highlight color=\"#1877F2\">Facebook blue</Highlight> !\nThis is Docusaurus green !\nThis is Facebook blue !\n--------------------------------------------------------------------------------"
  },
  {
    "url": "https://Muham251.github.io/ai-hackathon-1/docs/tutorial-basics/markdown-features#code-blocks",
    "text": "Markdown Features\nDocusaurus supports Markdown and a few additional features.\nFront Matter\nMarkdown documents have metadata at the top called Front Matter:\n---\nid: my-doc-id\ntitle: My document title\ndescription: My document description\nslug: /my-custom-url\n---\n## Markdown heading\nMarkdown text with [links](./hello.md)\nLinks\nRegular Markdown links are supported, using url paths or relative file paths.\nLet's see how to [Create a page](/create-a-page).\nLet's see how to [Create a page](./create-a-page.md).\nResult: Let's see how to Create a page.\nImages\nRegular Markdown images are supported.\nYou can use absolute paths to reference images in the static directory (static/img/docusaurus.png\n):\n![Docusaurus logo](/img/docusaurus.png)\nYou can reference images relative to the current file as well. This is particularly useful to colocate images close to the Markdown files using them:\n![Docusaurus logo](./img/docusaurus.png)\nCode Blocks\nMarkdown code blocks are supported with Syntax highlighting.\n```jsx title=\"src/components/HelloDocusaurus.js\"\nfunction HelloDocusaurus() {\nreturn <h1>Hello, Docusaurus!</h1>;\n}\n```\nfunction HelloDocusaurus() {\nreturn <h1>Hello, Docusaurus!</h1>;\n}\nAdmonitions\nDocusaurus has a special syntax to create admonitions and callouts:\n:::tip[My tip]\nUse this awesome feature option\n:::\n:::danger[Take care]\nThis action is dangerous\n:::\nUse this awesome feature option\nThis action is dangerous\nMDX and React Components\nMDX can make your documentation more interactive and allows using any React components inside Markdown:\nexport const Highlight = ({children, color}) => (\n<span\nstyle={{\nbackgroundColor: color,\nborderRadius: '20px',\ncolor: '#fff',\npadding: '10px',\ncursor: 'pointer',\n}}\nonClick={() => {\nalert(`You clicked the color ${color} with label ${children}`)\n}}>\n{children}\n</span>\n);\nThis is <Highlight color=\"#25c2a0\">Docusaurus green</Highlight> !\nThis is <Highlight color=\"#1877F2\">Facebook blue</Highlight> !\nThis is Docusaurus green !\nThis is Facebook blue !\n--------------------------------------------------------------------------------"
  },
  {
    "url": "https://Muham251.github.io/ai-hackathon-1/docs/tutorial-basics/markdown-features#admonitions",
    "text": "Markdown Features\nDocusaurus supports Markdown and a few additional features.\nFront Matter\nMarkdown documents have metadata at the top called Front Matter:\n---\nid: my-doc-id\ntitle: My document title\ndescription: My document description\nslug: /my-custom-url\n---\n## Markdown heading\nMarkdown text with [links](./hello.md)\nLinks\nRegular Markdown links are supported, using url paths or relative file paths.\nLet's see how to [Create a page](/create-a-page).\nLet's see how to [Create a page](./create-a-page.md).\nResult: Let's see how to Create a page.\nImages\nRegular Markdown images are supported.\nYou can use absolute paths to reference images in the static directory (static/img/docusaurus.png\n):\n![Docusaurus logo](/img/docusaurus.png)\nYou can reference images relative to the current file as well. This is particularly useful to colocate images close to the Markdown files using them:\n![Docusaurus logo](./img/docusaurus.png)\nCode Blocks\nMarkdown code blocks are supported with Syntax highlighting.\n```jsx title=\"src/components/HelloDocusaurus.js\"\nfunction HelloDocusaurus() {\nreturn <h1>Hello, Docusaurus!</h1>;\n}\n```\nfunction HelloDocusaurus() {\nreturn <h1>Hello, Docusaurus!</h1>;\n}\nAdmonitions\nDocusaurus has a special syntax to create admonitions and callouts:\n:::tip[My tip]\nUse this awesome feature option\n:::\n:::danger[Take care]\nThis action is dangerous\n:::\nUse this awesome feature option\nThis action is dangerous\nMDX and React Components\nMDX can make your documentation more interactive and allows using any React components inside Markdown:\nexport const Highlight = ({children, color}) => (\n<span\nstyle={{\nbackgroundColor: color,\nborderRadius: '20px',\ncolor: '#fff',\npadding: '10px',\ncursor: 'pointer',\n}}\nonClick={() => {\nalert(`You clicked the color ${color} with label ${children}`)\n}}>\n{children}\n</span>\n);\nThis is <Highlight color=\"#25c2a0\">Docusaurus green</Highlight> !\nThis is <Highlight color=\"#1877F2\">Facebook blue</Highlight> !\nThis is Docusaurus green !\nThis is Facebook blue !\n--------------------------------------------------------------------------------"
  },
  {
    "url": "https://Muham251.github.io/ai-hackathon-1/docs/tutorial-basics/markdown-features#mdx-and-react-components",
    "text": "Markdown Features\nDocusaurus supports Markdown and a few additional features.\nFront Matter\nMarkdown documents have metadata at the top called Front Matter:\n---\nid: my-doc-id\ntitle: My document title\ndescription: My document description\nslug: /my-custom-url\n---\n## Markdown heading\nMarkdown text with [links](./hello.md)\nLinks\nRegular Markdown links are supported, using url paths or relative file paths.\nLet's see how to [Create a page](/create-a-page).\nLet's see how to [Create a page](./create-a-page.md).\nResult: Let's see how to Create a page.\nImages\nRegular Markdown images are supported.\nYou can use absolute paths to reference images in the static directory (static/img/docusaurus.png\n):\n![Docusaurus logo](/img/docusaurus.png)\nYou can reference images relative to the current file as well. This is particularly useful to colocate images close to the Markdown files using them:\n![Docusaurus logo](./img/docusaurus.png)\nCode Blocks\nMarkdown code blocks are supported with Syntax highlighting.\n```jsx title=\"src/components/HelloDocusaurus.js\"\nfunction HelloDocusaurus() {\nreturn <h1>Hello, Docusaurus!</h1>;\n}\n```\nfunction HelloDocusaurus() {\nreturn <h1>Hello, Docusaurus!</h1>;\n}\nAdmonitions\nDocusaurus has a special syntax to create admonitions and callouts:\n:::tip[My tip]\nUse this awesome feature option\n:::\n:::danger[Take care]\nThis action is dangerous\n:::\nUse this awesome feature option\nThis action is dangerous\nMDX and React Components\nMDX can make your documentation more interactive and allows using any React components inside Markdown:\nexport const Highlight = ({children, color}) => (\n<span\nstyle={{\nbackgroundColor: color,\nborderRadius: '20px',\ncolor: '#fff',\npadding: '10px',\ncursor: 'pointer',\n}}\nonClick={() => {\nalert(`You clicked the color ${color} with label ${children}`)\n}}>\n{children}\n</span>\n);\nThis is <Highlight color=\"#25c2a0\">Docusaurus green</Highlight> !\nThis is <Highlight color=\"#1877F2\">Facebook blue</Highlight> !\nThis is Docusaurus green !\nThis is Facebook blue !\n--------------------------------------------------------------------------------"
  },
  {
    "url": "https://Muham251.github.io/ai-hackathon-1/docs/tutorial-basics/create-a-blog-post#create-your-first-post",
    "text": "Create a Blog Post\nDocusaurus creates a page for each blog post, but also a blog index page, a tag system, an RSS feed...\nCreate your first Post\nCreate a file at blog/2021-02-28-greetings.md\n:\nblog/2021-02-28-greetings.md\n---\nslug: greetings\ntitle: Greetings!\nauthors:\n- name: Joel Marcey\ntitle: Co-creator of Docusaurus 1\nurl: https://github.com/JoelMarcey\nimage_url: https://github.com/JoelMarcey.png\n- name: S\u00e9bastien Lorber\ntitle: Docusaurus maintainer\nurl: https://sebastienlorber.com\nimage_url: https://github.com/slorber.png\ntags: [greetings]\n---\nCongratulations, you have made your first post!\nFeel free to play around and edit this post as much as you like.\nA new blog post is now available at http://localhost:3000/blog/greetings.\n--------------------------------------------------------------------------------"
  },
  {
    "url": "https://Muham251.github.io/ai-hackathon-1/docs/tutorial-basics/create-a-document#create-your-first-doc",
    "text": "Create a Document\nDocuments are groups of pages connected through:\n- a sidebar\n- previous/next navigation\n- versioning\nCreate your first Doc\nCreate a Markdown file at docs/hello.md\n:\ndocs/hello.md\n# Hello\nThis is my **first Docusaurus document**!\nA new document is now available at http://localhost:3000/docs/hello.\nConfigure the Sidebar\nDocusaurus automatically creates a sidebar from the docs\nfolder.\nAdd metadata to customize the sidebar label and position:\ndocs/hello.md\n---\nsidebar_label: 'Hi!'\nsidebar_position: 3\n---\n# Hello\nThis is my **first Docusaurus document**!\nIt is also possible to create your sidebar explicitly in sidebars.js\n:\nsidebars.js\nexport default {\ntutorialSidebar: [\n'intro',\n'hello',\n{\ntype: 'category',\nlabel: 'Tutorial',\nitems: ['tutorial-basics/create-a-document'],\n},\n],\n};\n--------------------------------------------------------------------------------"
  },
  {
    "url": "https://Muham251.github.io/ai-hackathon-1/docs/tutorial-basics/create-a-document#configure-the-sidebar",
    "text": "Create a Document\nDocuments are groups of pages connected through:\n- a sidebar\n- previous/next navigation\n- versioning\nCreate your first Doc\nCreate a Markdown file at docs/hello.md\n:\ndocs/hello.md\n# Hello\nThis is my **first Docusaurus document**!\nA new document is now available at http://localhost:3000/docs/hello.\nConfigure the Sidebar\nDocusaurus automatically creates a sidebar from the docs\nfolder.\nAdd metadata to customize the sidebar label and position:\ndocs/hello.md\n---\nsidebar_label: 'Hi!'\nsidebar_position: 3\n---\n# Hello\nThis is my **first Docusaurus document**!\nIt is also possible to create your sidebar explicitly in sidebars.js\n:\nsidebars.js\nexport default {\ntutorialSidebar: [\n'intro',\n'hello',\n{\ntype: 'category',\nlabel: 'Tutorial',\nitems: ['tutorial-basics/create-a-document'],\n},\n],\n};\n--------------------------------------------------------------------------------"
  },
  {
    "url": "https://Muham251.github.io/ai-hackathon-1/docs/tutorial-basics/create-a-page#create-your-first-react-page",
    "text": "Create a Page\nAdd Markdown or React files to src/pages\nto create a standalone page:\nsrc/pages/index.js\n\u2192localhost:3000/\nsrc/pages/foo.md\n\u2192localhost:3000/foo\nsrc/pages/foo/bar.js\n\u2192localhost:3000/foo/bar\nCreate your first React Page\nCreate a file at src/pages/my-react-page.js\n:\nsrc/pages/my-react-page.js\nimport React from 'react';\nimport Layout from '@theme/Layout';\nexport default function MyReactPage() {\nreturn (\n<Layout>\n<h1>My React page</h1>\n<p>This is a React page</p>\n</Layout>\n);\n}\nA new page is now available at http://localhost:3000/my-react-page.\nCreate your first Markdown Page\nCreate a file at src/pages/my-markdown-page.md\n:\nsrc/pages/my-markdown-page.md\n# My Markdown page\nThis is a Markdown page\nA new page is now available at http://localhost:3000/my-markdown-page.\n--------------------------------------------------------------------------------"
  },
  {
    "url": "https://Muham251.github.io/ai-hackathon-1/docs/tutorial-basics/create-a-page#create-your-first-markdown-page",
    "text": "Create a Page\nAdd Markdown or React files to src/pages\nto create a standalone page:\nsrc/pages/index.js\n\u2192localhost:3000/\nsrc/pages/foo.md\n\u2192localhost:3000/foo\nsrc/pages/foo/bar.js\n\u2192localhost:3000/foo/bar\nCreate your first React Page\nCreate a file at src/pages/my-react-page.js\n:\nsrc/pages/my-react-page.js\nimport React from 'react';\nimport Layout from '@theme/Layout';\nexport default function MyReactPage() {\nreturn (\n<Layout>\n<h1>My React page</h1>\n<p>This is a React page</p>\n</Layout>\n);\n}\nA new page is now available at http://localhost:3000/my-react-page.\nCreate your first Markdown Page\nCreate a file at src/pages/my-markdown-page.md\n:\nsrc/pages/my-markdown-page.md\n# My Markdown page\nThis is a Markdown page\nA new page is now available at http://localhost:3000/my-markdown-page.\n--------------------------------------------------------------------------------"
  },
  {
    "url": "https://Muham251.github.io/ai-hackathon-1/docs/module-1/urdf-essentials#key-urdf-tags",
    "text": "URDF Basics for Humanoid Models\nThe Unified Robot Description Format (URDF) is an XML format used in ROS to describe all the physical elements of a robot model. For a humanoid, the URDF file is its digital blueprint. It defines the robot's body parts, how they are connected, and their physical properties.\nA URDF file describes the robot as a tree of links connected by joints.\n<link>\n: A physical part of the robot (e.g., a forearm, a foot, a torso).<joint>\n: The connection between two links. It defines how one link moves relative to another (e.g., rotating like an elbow, sliding, or being fixed).\nKey URDF Tags\nThe <robot>\nTag\nEvery URDF file must start and end with the <robot>\ntag. It's the root element, and you give your robot a name here.\n<robot name=\"simple_humanoid\">\n... all your links and joints go here ...\n</robot>\nThe <link>\nTag\nEach link has a name and can contain three important sub-tags:\n<visual>\n: Defines the appearance of the link (shape, size, color, texture). This is what you see in simulation tools like RViz.<collision>\n: Defines the collision geometry of the link. This is what the physics engine uses to calculate collisions with other objects. It's often a simpler shape than the visual one for performance reasons.<inertial>\n: Defines the dynamic properties of the link: its mass, center of mass, and inertia matrix. This is crucial for realistic physics simulation.\n<link name=\"torso\">\n<visual>\n<geometry>\n<box size=\"0.2 0.3 0.5\"/>\n</geometry>\n<material name=\"grey\">\n<color rgba=\"0.5 0.5 0.5 1\"/>\n</material>\n</visual>\n<collision>\n<geometry>\n<box size=\"0.2 0.3 0.5\"/>\n</geometry>\n</collision>\n<inertial>\n<mass value=\"10\"/>\n<inertia ixx=\"0.4\" ixy=\"0.0\" ixz=\"0.0\" iyy=\"0.4\" iyz=\"0.0\" izz=\"0.2\"/>\n</inertial>\n</link>\nThe <joint>\nTag\nEach joint connects two links, called the parent and the child. It also defines the motion allowed between them.\nname\n: The name of the joint (e.g., \"left_shoulder_joint\").type\n: The type of motion. Common types are:revolute\n: Rotates around an axis (like an elbow). Requires<limit>\ntags for upper and lower angles.continuous\n: Rotates around an axis with no angle limits.prismatic\n: Slides along an axis (like a piston).fixed\n: No motion is allowed between the two links.\n<parent link=\"...\" />\n: The name of the parent link.<child link=\"...\" />\n: The name of the child link.<origin xyz=\"...\" rpy=\"...\" />\n: The transform (position and orientation) from the parent link's origin to the joint's origin.<axis xyz=\"...\" />\n: The axis of rotation or translation forrevolute\nandprismatic\njoints.\n<joint name=\"neck_joint\" type=\"revolute\">\n<parent link=\"torso\"/>\n<child link=\"head\"/>\n<origin xyz=\"0 0 0.25\" rpy=\"0 0 0\"/>\n<axis xyz=\"0 0 1\"/>\n<limit lower=\"-0.5\" upper=\"0.5\" effort=\"10\" velocity=\"1.0\"/>\n</joint>\nSimple Humanoid URDF Example\nThis example describes a very simple humanoid robot. It has a torso, a head, and a single arm with a shoulder and elbow joint. This demonstrates the parent-child tree structure. The \"torso\" is the root link of our tree.\n<!-- simple_humanoid.urdf -->\n<robot name=\"simple_humanoid\">\n<!-- A. Materials (for color) -->\n<material name=\"blue\">\n<color rgba=\"0.0 0.0 0.8 1.0\"/>\n</material>\n<material name=\"white\">\n<color rgba=\"1.0 1.0 1.0 1.0\"/>\n</material>\n<!-- B. Base Link (Torso) -->\n<link name=\"torso\">\n<visual>\n<geometry><box size=\"0.2 0.3 0.5\"/></geometry>\n<origin xyz=\"0 0 0\" rpy=\"0 0 0\"/>\n<material name=\"blue\"/>\n</visual>\n<collision><geometry><box size=\"0.2 0.3 0.5\"/></geometry></collision>\n<inertial>\n<mass value=\"10\"/>\n<inertia ixx=\"1.0\" ixy=\"0\" ixz=\"0\" iyy=\"1.0\" iyz=\"0\" izz=\"1.0\"/>\n</inertial>\n</link>\n<!-- C. Head Link -->\n<link name=\"head\">\n<visual>\n<geometry><sphere radius=\"0.1\"/></geometry>\n<origin xyz=\"0 0 0\" rpy=\"0 0 0\"/>\n<material name=\"white\"/>\n</visual>\n<collision><geometry><sphere radius=\"0.1\"/></geometry></collision>\n<inertial>\n<mass value=\"1\"/>\n<inertia ixx=\"0.1\" ixy=\"0\" ixz=\"0\" iyy=\"0.1\" iyz=\"0\" izz=\"0.1\"/>\n</inertial>\n</link>\n<!-- D. Neck Joint (Connects Torso to Head) -->\n<joint name=\"neck_joint\" type=\"revolute\">\n<parent link=\"torso\"/>\n<child link=\"head\"/>\n<origin xyz=\"0 0 0.30\" rpy=\"0 0 0\"/> <!-- Position of head relative to torso -->\n<axis xyz=\"0 0 1\"/>\n<limit lower=\"-0.7\" upper=\"0.7\" effort=\"5\" velocity=\"0.5\"/>\n</joint>\n<!-- E. Right Arm -->\n<link name=\"right_upper_arm\">\n<visual>\n<geometry><cylinder length=\"0.3\" radius=\"0.05\"/></geometry>\n<origin xyz=\"0 0 -0.15\" rpy=\"0 0 0\"/>\n<material name=\"white\"/>\n</visual>\n<collision><geometry><cylinder length=\"0.3\" radius=\"0.05\"/></geometry></collision>\n<inertial>\n<mass value=\"2\"/>\n<inertia ixx=\"0.1\" ixy=\"0\" ixz=\"0\" iyy=\"0.1\" iyz=\"0\" izz=\"0.1\"/>\n</inertial>\n</link>\n<!-- F. Right Shoulder Joint -->\n<joint name=\"right_shoulder_joint\" type=\"revolute\">\n<parent link=\"torso\"/>\n<child link=\"right_upper_arm\"/>\n<origin xyz=\"0 -0.20 0.15\" rpy=\"1.5707 0 0\"/> <!-- Position of arm relative to torso -->\n<axis xyz=\"0 0 1\"/>\n<limit lower=\"-1.57\" upper=\"1.57\" effort=\"10\" velocity=\"1.0\"/>\n</joint>\n</robot>\nHow to Use This URDF\n- Save the code above as\nsimple_humanoid.urdf\n. - You can check its validity using the\ncheck_urdf\ncommand:check_urdf simple_humanoid.urdf\n. - To visualize it, you can use ROS tools like RViz. This typically requires a small launch file to publish the robot's state, which is a topic for a later module.\nThis simple example provides the foundation. A full humanoid model would extend this tree with more links and joints for the legs, a more complex arm, and hands.\n--------------------------------------------------------------------------------"
  },
  {
    "url": "https://Muham251.github.io/ai-hackathon-1/docs/module-1/urdf-essentials#the-robot-tag",
    "text": "URDF Basics for Humanoid Models\nThe Unified Robot Description Format (URDF) is an XML format used in ROS to describe all the physical elements of a robot model. For a humanoid, the URDF file is its digital blueprint. It defines the robot's body parts, how they are connected, and their physical properties.\nA URDF file describes the robot as a tree of links connected by joints.\n<link>\n: A physical part of the robot (e.g., a forearm, a foot, a torso).<joint>\n: The connection between two links. It defines how one link moves relative to another (e.g., rotating like an elbow, sliding, or being fixed).\nKey URDF Tags\nThe <robot>\nTag\nEvery URDF file must start and end with the <robot>\ntag. It's the root element, and you give your robot a name here.\n<robot name=\"simple_humanoid\">\n... all your links and joints go here ...\n</robot>\nThe <link>\nTag\nEach link has a name and can contain three important sub-tags:\n<visual>\n: Defines the appearance of the link (shape, size, color, texture). This is what you see in simulation tools like RViz.<collision>\n: Defines the collision geometry of the link. This is what the physics engine uses to calculate collisions with other objects. It's often a simpler shape than the visual one for performance reasons.<inertial>\n: Defines the dynamic properties of the link: its mass, center of mass, and inertia matrix. This is crucial for realistic physics simulation.\n<link name=\"torso\">\n<visual>\n<geometry>\n<box size=\"0.2 0.3 0.5\"/>\n</geometry>\n<material name=\"grey\">\n<color rgba=\"0.5 0.5 0.5 1\"/>\n</material>\n</visual>\n<collision>\n<geometry>\n<box size=\"0.2 0.3 0.5\"/>\n</geometry>\n</collision>\n<inertial>\n<mass value=\"10\"/>\n<inertia ixx=\"0.4\" ixy=\"0.0\" ixz=\"0.0\" iyy=\"0.4\" iyz=\"0.0\" izz=\"0.2\"/>\n</inertial>\n</link>\nThe <joint>\nTag\nEach joint connects two links, called the parent and the child. It also defines the motion allowed between them.\nname\n: The name of the joint (e.g., \"left_shoulder_joint\").type\n: The type of motion. Common types are:revolute\n: Rotates around an axis (like an elbow). Requires<limit>\ntags for upper and lower angles.continuous\n: Rotates around an axis with no angle limits.prismatic\n: Slides along an axis (like a piston).fixed\n: No motion is allowed between the two links.\n<parent link=\"...\" />\n: The name of the parent link.<child link=\"...\" />\n: The name of the child link.<origin xyz=\"...\" rpy=\"...\" />\n: The transform (position and orientation) from the parent link's origin to the joint's origin.<axis xyz=\"...\" />\n: The axis of rotation or translation forrevolute\nandprismatic\njoints.\n<joint name=\"neck_joint\" type=\"revolute\">\n<parent link=\"torso\"/>\n<child link=\"head\"/>\n<origin xyz=\"0 0 0.25\" rpy=\"0 0 0\"/>\n<axis xyz=\"0 0 1\"/>\n<limit lower=\"-0.5\" upper=\"0.5\" effort=\"10\" velocity=\"1.0\"/>\n</joint>\nSimple Humanoid URDF Example\nThis example describes a very simple humanoid robot. It has a torso, a head, and a single arm with a shoulder and elbow joint. This demonstrates the parent-child tree structure. The \"torso\" is the root link of our tree.\n<!-- simple_humanoid.urdf -->\n<robot name=\"simple_humanoid\">\n<!-- A. Materials (for color) -->\n<material name=\"blue\">\n<color rgba=\"0.0 0.0 0.8 1.0\"/>\n</material>\n<material name=\"white\">\n<color rgba=\"1.0 1.0 1.0 1.0\"/>\n</material>\n<!-- B. Base Link (Torso) -->\n<link name=\"torso\">\n<visual>\n<geometry><box size=\"0.2 0.3 0.5\"/></geometry>\n<origin xyz=\"0 0 0\" rpy=\"0 0 0\"/>\n<material name=\"blue\"/>\n</visual>\n<collision><geometry><box size=\"0.2 0.3 0.5\"/></geometry></collision>\n<inertial>\n<mass value=\"10\"/>\n<inertia ixx=\"1.0\" ixy=\"0\" ixz=\"0\" iyy=\"1.0\" iyz=\"0\" izz=\"1.0\"/>\n</inertial>\n</link>\n<!-- C. Head Link -->\n<link name=\"head\">\n<visual>\n<geometry><sphere radius=\"0.1\"/></geometry>\n<origin xyz=\"0 0 0\" rpy=\"0 0 0\"/>\n<material name=\"white\"/>\n</visual>\n<collision><geometry><sphere radius=\"0.1\"/></geometry></collision>\n<inertial>\n<mass value=\"1\"/>\n<inertia ixx=\"0.1\" ixy=\"0\" ixz=\"0\" iyy=\"0.1\" iyz=\"0\" izz=\"0.1\"/>\n</inertial>\n</link>\n<!-- D. Neck Joint (Connects Torso to Head) -->\n<joint name=\"neck_joint\" type=\"revolute\">\n<parent link=\"torso\"/>\n<child link=\"head\"/>\n<origin xyz=\"0 0 0.30\" rpy=\"0 0 0\"/> <!-- Position of head relative to torso -->\n<axis xyz=\"0 0 1\"/>\n<limit lower=\"-0.7\" upper=\"0.7\" effort=\"5\" velocity=\"0.5\"/>\n</joint>\n<!-- E. Right Arm -->\n<link name=\"right_upper_arm\">\n<visual>\n<geometry><cylinder length=\"0.3\" radius=\"0.05\"/></geometry>\n<origin xyz=\"0 0 -0.15\" rpy=\"0 0 0\"/>\n<material name=\"white\"/>\n</visual>\n<collision><geometry><cylinder length=\"0.3\" radius=\"0.05\"/></geometry></collision>\n<inertial>\n<mass value=\"2\"/>\n<inertia ixx=\"0.1\" ixy=\"0\" ixz=\"0\" iyy=\"0.1\" iyz=\"0\" izz=\"0.1\"/>\n</inertial>\n</link>\n<!-- F. Right Shoulder Joint -->\n<joint name=\"right_shoulder_joint\" type=\"revolute\">\n<parent link=\"torso\"/>\n<child link=\"right_upper_arm\"/>\n<origin xyz=\"0 -0.20 0.15\" rpy=\"1.5707 0 0\"/> <!-- Position of arm relative to torso -->\n<axis xyz=\"0 0 1\"/>\n<limit lower=\"-1.57\" upper=\"1.57\" effort=\"10\" velocity=\"1.0\"/>\n</joint>\n</robot>\nHow to Use This URDF\n- Save the code above as\nsimple_humanoid.urdf\n. - You can check its validity using the\ncheck_urdf\ncommand:check_urdf simple_humanoid.urdf\n. - To visualize it, you can use ROS tools like RViz. This typically requires a small launch file to publish the robot's state, which is a topic for a later module.\nThis simple example provides the foundation. A full humanoid model would extend this tree with more links and joints for the legs, a more complex arm, and hands.\n--------------------------------------------------------------------------------"
  },
  {
    "url": "https://Muham251.github.io/ai-hackathon-1/docs/module-1/urdf-essentials#the-link-tag",
    "text": "URDF Basics for Humanoid Models\nThe Unified Robot Description Format (URDF) is an XML format used in ROS to describe all the physical elements of a robot model. For a humanoid, the URDF file is its digital blueprint. It defines the robot's body parts, how they are connected, and their physical properties.\nA URDF file describes the robot as a tree of links connected by joints.\n<link>\n: A physical part of the robot (e.g., a forearm, a foot, a torso).<joint>\n: The connection between two links. It defines how one link moves relative to another (e.g., rotating like an elbow, sliding, or being fixed).\nKey URDF Tags\nThe <robot>\nTag\nEvery URDF file must start and end with the <robot>\ntag. It's the root element, and you give your robot a name here.\n<robot name=\"simple_humanoid\">\n... all your links and joints go here ...\n</robot>\nThe <link>\nTag\nEach link has a name and can contain three important sub-tags:\n<visual>\n: Defines the appearance of the link (shape, size, color, texture). This is what you see in simulation tools like RViz.<collision>\n: Defines the collision geometry of the link. This is what the physics engine uses to calculate collisions with other objects. It's often a simpler shape than the visual one for performance reasons.<inertial>\n: Defines the dynamic properties of the link: its mass, center of mass, and inertia matrix. This is crucial for realistic physics simulation.\n<link name=\"torso\">\n<visual>\n<geometry>\n<box size=\"0.2 0.3 0.5\"/>\n</geometry>\n<material name=\"grey\">\n<color rgba=\"0.5 0.5 0.5 1\"/>\n</material>\n</visual>\n<collision>\n<geometry>\n<box size=\"0.2 0.3 0.5\"/>\n</geometry>\n</collision>\n<inertial>\n<mass value=\"10\"/>\n<inertia ixx=\"0.4\" ixy=\"0.0\" ixz=\"0.0\" iyy=\"0.4\" iyz=\"0.0\" izz=\"0.2\"/>\n</inertial>\n</link>\nThe <joint>\nTag\nEach joint connects two links, called the parent and the child. It also defines the motion allowed between them.\nname\n: The name of the joint (e.g., \"left_shoulder_joint\").type\n: The type of motion. Common types are:revolute\n: Rotates around an axis (like an elbow). Requires<limit>\ntags for upper and lower angles.continuous\n: Rotates around an axis with no angle limits.prismatic\n: Slides along an axis (like a piston).fixed\n: No motion is allowed between the two links.\n<parent link=\"...\" />\n: The name of the parent link.<child link=\"...\" />\n: The name of the child link.<origin xyz=\"...\" rpy=\"...\" />\n: The transform (position and orientation) from the parent link's origin to the joint's origin.<axis xyz=\"...\" />\n: The axis of rotation or translation forrevolute\nandprismatic\njoints.\n<joint name=\"neck_joint\" type=\"revolute\">\n<parent link=\"torso\"/>\n<child link=\"head\"/>\n<origin xyz=\"0 0 0.25\" rpy=\"0 0 0\"/>\n<axis xyz=\"0 0 1\"/>\n<limit lower=\"-0.5\" upper=\"0.5\" effort=\"10\" velocity=\"1.0\"/>\n</joint>\nSimple Humanoid URDF Example\nThis example describes a very simple humanoid robot. It has a torso, a head, and a single arm with a shoulder and elbow joint. This demonstrates the parent-child tree structure. The \"torso\" is the root link of our tree.\n<!-- simple_humanoid.urdf -->\n<robot name=\"simple_humanoid\">\n<!-- A. Materials (for color) -->\n<material name=\"blue\">\n<color rgba=\"0.0 0.0 0.8 1.0\"/>\n</material>\n<material name=\"white\">\n<color rgba=\"1.0 1.0 1.0 1.0\"/>\n</material>\n<!-- B. Base Link (Torso) -->\n<link name=\"torso\">\n<visual>\n<geometry><box size=\"0.2 0.3 0.5\"/></geometry>\n<origin xyz=\"0 0 0\" rpy=\"0 0 0\"/>\n<material name=\"blue\"/>\n</visual>\n<collision><geometry><box size=\"0.2 0.3 0.5\"/></geometry></collision>\n<inertial>\n<mass value=\"10\"/>\n<inertia ixx=\"1.0\" ixy=\"0\" ixz=\"0\" iyy=\"1.0\" iyz=\"0\" izz=\"1.0\"/>\n</inertial>\n</link>\n<!-- C. Head Link -->\n<link name=\"head\">\n<visual>\n<geometry><sphere radius=\"0.1\"/></geometry>\n<origin xyz=\"0 0 0\" rpy=\"0 0 0\"/>\n<material name=\"white\"/>\n</visual>\n<collision><geometry><sphere radius=\"0.1\"/></geometry></collision>\n<inertial>\n<mass value=\"1\"/>\n<inertia ixx=\"0.1\" ixy=\"0\" ixz=\"0\" iyy=\"0.1\" iyz=\"0\" izz=\"0.1\"/>\n</inertial>\n</link>\n<!-- D. Neck Joint (Connects Torso to Head) -->\n<joint name=\"neck_joint\" type=\"revolute\">\n<parent link=\"torso\"/>\n<child link=\"head\"/>\n<origin xyz=\"0 0 0.30\" rpy=\"0 0 0\"/> <!-- Position of head relative to torso -->\n<axis xyz=\"0 0 1\"/>\n<limit lower=\"-0.7\" upper=\"0.7\" effort=\"5\" velocity=\"0.5\"/>\n</joint>\n<!-- E. Right Arm -->\n<link name=\"right_upper_arm\">\n<visual>\n<geometry><cylinder length=\"0.3\" radius=\"0.05\"/></geometry>\n<origin xyz=\"0 0 -0.15\" rpy=\"0 0 0\"/>\n<material name=\"white\"/>\n</visual>\n<collision><geometry><cylinder length=\"0.3\" radius=\"0.05\"/></geometry></collision>\n<inertial>\n<mass value=\"2\"/>\n<inertia ixx=\"0.1\" ixy=\"0\" ixz=\"0\" iyy=\"0.1\" iyz=\"0\" izz=\"0.1\"/>\n</inertial>\n</link>\n<!-- F. Right Shoulder Joint -->\n<joint name=\"right_shoulder_joint\" type=\"revolute\">\n<parent link=\"torso\"/>\n<child link=\"right_upper_arm\"/>\n<origin xyz=\"0 -0.20 0.15\" rpy=\"1.5707 0 0\"/> <!-- Position of arm relative to torso -->\n<axis xyz=\"0 0 1\"/>\n<limit lower=\"-1.57\" upper=\"1.57\" effort=\"10\" velocity=\"1.0\"/>\n</joint>\n</robot>\nHow to Use This URDF\n- Save the code above as\nsimple_humanoid.urdf\n. - You can check its validity using the\ncheck_urdf\ncommand:check_urdf simple_humanoid.urdf\n. - To visualize it, you can use ROS tools like RViz. This typically requires a small launch file to publish the robot's state, which is a topic for a later module.\nThis simple example provides the foundation. A full humanoid model would extend this tree with more links and joints for the legs, a more complex arm, and hands.\n--------------------------------------------------------------------------------"
  },
  {
    "url": "https://Muham251.github.io/ai-hackathon-1/docs/module-1/urdf-essentials#the-joint-tag",
    "text": "URDF Basics for Humanoid Models\nThe Unified Robot Description Format (URDF) is an XML format used in ROS to describe all the physical elements of a robot model. For a humanoid, the URDF file is its digital blueprint. It defines the robot's body parts, how they are connected, and their physical properties.\nA URDF file describes the robot as a tree of links connected by joints.\n<link>\n: A physical part of the robot (e.g., a forearm, a foot, a torso).<joint>\n: The connection between two links. It defines how one link moves relative to another (e.g., rotating like an elbow, sliding, or being fixed).\nKey URDF Tags\nThe <robot>\nTag\nEvery URDF file must start and end with the <robot>\ntag. It's the root element, and you give your robot a name here.\n<robot name=\"simple_humanoid\">\n... all your links and joints go here ...\n</robot>\nThe <link>\nTag\nEach link has a name and can contain three important sub-tags:\n<visual>\n: Defines the appearance of the link (shape, size, color, texture). This is what you see in simulation tools like RViz.<collision>\n: Defines the collision geometry of the link. This is what the physics engine uses to calculate collisions with other objects. It's often a simpler shape than the visual one for performance reasons.<inertial>\n: Defines the dynamic properties of the link: its mass, center of mass, and inertia matrix. This is crucial for realistic physics simulation.\n<link name=\"torso\">\n<visual>\n<geometry>\n<box size=\"0.2 0.3 0.5\"/>\n</geometry>\n<material name=\"grey\">\n<color rgba=\"0.5 0.5 0.5 1\"/>\n</material>\n</visual>\n<collision>\n<geometry>\n<box size=\"0.2 0.3 0.5\"/>\n</geometry>\n</collision>\n<inertial>\n<mass value=\"10\"/>\n<inertia ixx=\"0.4\" ixy=\"0.0\" ixz=\"0.0\" iyy=\"0.4\" iyz=\"0.0\" izz=\"0.2\"/>\n</inertial>\n</link>\nThe <joint>\nTag\nEach joint connects two links, called the parent and the child. It also defines the motion allowed between them.\nname\n: The name of the joint (e.g., \"left_shoulder_joint\").type\n: The type of motion. Common types are:revolute\n: Rotates around an axis (like an elbow). Requires<limit>\ntags for upper and lower angles.continuous\n: Rotates around an axis with no angle limits.prismatic\n: Slides along an axis (like a piston).fixed\n: No motion is allowed between the two links.\n<parent link=\"...\" />\n: The name of the parent link.<child link=\"...\" />\n: The name of the child link.<origin xyz=\"...\" rpy=\"...\" />\n: The transform (position and orientation) from the parent link's origin to the joint's origin.<axis xyz=\"...\" />\n: The axis of rotation or translation forrevolute\nandprismatic\njoints.\n<joint name=\"neck_joint\" type=\"revolute\">\n<parent link=\"torso\"/>\n<child link=\"head\"/>\n<origin xyz=\"0 0 0.25\" rpy=\"0 0 0\"/>\n<axis xyz=\"0 0 1\"/>\n<limit lower=\"-0.5\" upper=\"0.5\" effort=\"10\" velocity=\"1.0\"/>\n</joint>\nSimple Humanoid URDF Example\nThis example describes a very simple humanoid robot. It has a torso, a head, and a single arm with a shoulder and elbow joint. This demonstrates the parent-child tree structure. The \"torso\" is the root link of our tree.\n<!-- simple_humanoid.urdf -->\n<robot name=\"simple_humanoid\">\n<!-- A. Materials (for color) -->\n<material name=\"blue\">\n<color rgba=\"0.0 0.0 0.8 1.0\"/>\n</material>\n<material name=\"white\">\n<color rgba=\"1.0 1.0 1.0 1.0\"/>\n</material>\n<!-- B. Base Link (Torso) -->\n<link name=\"torso\">\n<visual>\n<geometry><box size=\"0.2 0.3 0.5\"/></geometry>\n<origin xyz=\"0 0 0\" rpy=\"0 0 0\"/>\n<material name=\"blue\"/>\n</visual>\n<collision><geometry><box size=\"0.2 0.3 0.5\"/></geometry></collision>\n<inertial>\n<mass value=\"10\"/>\n<inertia ixx=\"1.0\" ixy=\"0\" ixz=\"0\" iyy=\"1.0\" iyz=\"0\" izz=\"1.0\"/>\n</inertial>\n</link>\n<!-- C. Head Link -->\n<link name=\"head\">\n<visual>\n<geometry><sphere radius=\"0.1\"/></geometry>\n<origin xyz=\"0 0 0\" rpy=\"0 0 0\"/>\n<material name=\"white\"/>\n</visual>\n<collision><geometry><sphere radius=\"0.1\"/></geometry></collision>\n<inertial>\n<mass value=\"1\"/>\n<inertia ixx=\"0.1\" ixy=\"0\" ixz=\"0\" iyy=\"0.1\" iyz=\"0\" izz=\"0.1\"/>\n</inertial>\n</link>\n<!-- D. Neck Joint (Connects Torso to Head) -->\n<joint name=\"neck_joint\" type=\"revolute\">\n<parent link=\"torso\"/>\n<child link=\"head\"/>\n<origin xyz=\"0 0 0.30\" rpy=\"0 0 0\"/> <!-- Position of head relative to torso -->\n<axis xyz=\"0 0 1\"/>\n<limit lower=\"-0.7\" upper=\"0.7\" effort=\"5\" velocity=\"0.5\"/>\n</joint>\n<!-- E. Right Arm -->\n<link name=\"right_upper_arm\">\n<visual>\n<geometry><cylinder length=\"0.3\" radius=\"0.05\"/></geometry>\n<origin xyz=\"0 0 -0.15\" rpy=\"0 0 0\"/>\n<material name=\"white\"/>\n</visual>\n<collision><geometry><cylinder length=\"0.3\" radius=\"0.05\"/></geometry></collision>\n<inertial>\n<mass value=\"2\"/>\n<inertia ixx=\"0.1\" ixy=\"0\" ixz=\"0\" iyy=\"0.1\" iyz=\"0\" izz=\"0.1\"/>\n</inertial>\n</link>\n<!-- F. Right Shoulder Joint -->\n<joint name=\"right_shoulder_joint\" type=\"revolute\">\n<parent link=\"torso\"/>\n<child link=\"right_upper_arm\"/>\n<origin xyz=\"0 -0.20 0.15\" rpy=\"1.5707 0 0\"/> <!-- Position of arm relative to torso -->\n<axis xyz=\"0 0 1\"/>\n<limit lower=\"-1.57\" upper=\"1.57\" effort=\"10\" velocity=\"1.0\"/>\n</joint>\n</robot>\nHow to Use This URDF\n- Save the code above as\nsimple_humanoid.urdf\n. - You can check its validity using the\ncheck_urdf\ncommand:check_urdf simple_humanoid.urdf\n. - To visualize it, you can use ROS tools like RViz. This typically requires a small launch file to publish the robot's state, which is a topic for a later module.\nThis simple example provides the foundation. A full humanoid model would extend this tree with more links and joints for the legs, a more complex arm, and hands.\n--------------------------------------------------------------------------------"
  },
  {
    "url": "https://Muham251.github.io/ai-hackathon-1/docs/module-1/urdf-essentials#simple-humanoid-urdf-example",
    "text": "URDF Basics for Humanoid Models\nThe Unified Robot Description Format (URDF) is an XML format used in ROS to describe all the physical elements of a robot model. For a humanoid, the URDF file is its digital blueprint. It defines the robot's body parts, how they are connected, and their physical properties.\nA URDF file describes the robot as a tree of links connected by joints.\n<link>\n: A physical part of the robot (e.g., a forearm, a foot, a torso).<joint>\n: The connection between two links. It defines how one link moves relative to another (e.g., rotating like an elbow, sliding, or being fixed).\nKey URDF Tags\nThe <robot>\nTag\nEvery URDF file must start and end with the <robot>\ntag. It's the root element, and you give your robot a name here.\n<robot name=\"simple_humanoid\">\n... all your links and joints go here ...\n</robot>\nThe <link>\nTag\nEach link has a name and can contain three important sub-tags:\n<visual>\n: Defines the appearance of the link (shape, size, color, texture). This is what you see in simulation tools like RViz.<collision>\n: Defines the collision geometry of the link. This is what the physics engine uses to calculate collisions with other objects. It's often a simpler shape than the visual one for performance reasons.<inertial>\n: Defines the dynamic properties of the link: its mass, center of mass, and inertia matrix. This is crucial for realistic physics simulation.\n<link name=\"torso\">\n<visual>\n<geometry>\n<box size=\"0.2 0.3 0.5\"/>\n</geometry>\n<material name=\"grey\">\n<color rgba=\"0.5 0.5 0.5 1\"/>\n</material>\n</visual>\n<collision>\n<geometry>\n<box size=\"0.2 0.3 0.5\"/>\n</geometry>\n</collision>\n<inertial>\n<mass value=\"10\"/>\n<inertia ixx=\"0.4\" ixy=\"0.0\" ixz=\"0.0\" iyy=\"0.4\" iyz=\"0.0\" izz=\"0.2\"/>\n</inertial>\n</link>\nThe <joint>\nTag\nEach joint connects two links, called the parent and the child. It also defines the motion allowed between them.\nname\n: The name of the joint (e.g., \"left_shoulder_joint\").type\n: The type of motion. Common types are:revolute\n: Rotates around an axis (like an elbow). Requires<limit>\ntags for upper and lower angles.continuous\n: Rotates around an axis with no angle limits.prismatic\n: Slides along an axis (like a piston).fixed\n: No motion is allowed between the two links.\n<parent link=\"...\" />\n: The name of the parent link.<child link=\"...\" />\n: The name of the child link.<origin xyz=\"...\" rpy=\"...\" />\n: The transform (position and orientation) from the parent link's origin to the joint's origin.<axis xyz=\"...\" />\n: The axis of rotation or translation forrevolute\nandprismatic\njoints.\n<joint name=\"neck_joint\" type=\"revolute\">\n<parent link=\"torso\"/>\n<child link=\"head\"/>\n<origin xyz=\"0 0 0.25\" rpy=\"0 0 0\"/>\n<axis xyz=\"0 0 1\"/>\n<limit lower=\"-0.5\" upper=\"0.5\" effort=\"10\" velocity=\"1.0\"/>\n</joint>\nSimple Humanoid URDF Example\nThis example describes a very simple humanoid robot. It has a torso, a head, and a single arm with a shoulder and elbow joint. This demonstrates the parent-child tree structure. The \"torso\" is the root link of our tree.\n<!-- simple_humanoid.urdf -->\n<robot name=\"simple_humanoid\">\n<!-- A. Materials (for color) -->\n<material name=\"blue\">\n<color rgba=\"0.0 0.0 0.8 1.0\"/>\n</material>\n<material name=\"white\">\n<color rgba=\"1.0 1.0 1.0 1.0\"/>\n</material>\n<!-- B. Base Link (Torso) -->\n<link name=\"torso\">\n<visual>\n<geometry><box size=\"0.2 0.3 0.5\"/></geometry>\n<origin xyz=\"0 0 0\" rpy=\"0 0 0\"/>\n<material name=\"blue\"/>\n</visual>\n<collision><geometry><box size=\"0.2 0.3 0.5\"/></geometry></collision>\n<inertial>\n<mass value=\"10\"/>\n<inertia ixx=\"1.0\" ixy=\"0\" ixz=\"0\" iyy=\"1.0\" iyz=\"0\" izz=\"1.0\"/>\n</inertial>\n</link>\n<!-- C. Head Link -->\n<link name=\"head\">\n<visual>\n<geometry><sphere radius=\"0.1\"/></geometry>\n<origin xyz=\"0 0 0\" rpy=\"0 0 0\"/>\n<material name=\"white\"/>\n</visual>\n<collision><geometry><sphere radius=\"0.1\"/></geometry></collision>\n<inertial>\n<mass value=\"1\"/>\n<inertia ixx=\"0.1\" ixy=\"0\" ixz=\"0\" iyy=\"0.1\" iyz=\"0\" izz=\"0.1\"/>\n</inertial>\n</link>\n<!-- D. Neck Joint (Connects Torso to Head) -->\n<joint name=\"neck_joint\" type=\"revolute\">\n<parent link=\"torso\"/>\n<child link=\"head\"/>\n<origin xyz=\"0 0 0.30\" rpy=\"0 0 0\"/> <!-- Position of head relative to torso -->\n<axis xyz=\"0 0 1\"/>\n<limit lower=\"-0.7\" upper=\"0.7\" effort=\"5\" velocity=\"0.5\"/>\n</joint>\n<!-- E. Right Arm -->\n<link name=\"right_upper_arm\">\n<visual>\n<geometry><cylinder length=\"0.3\" radius=\"0.05\"/></geometry>\n<origin xyz=\"0 0 -0.15\" rpy=\"0 0 0\"/>\n<material name=\"white\"/>\n</visual>\n<collision><geometry><cylinder length=\"0.3\" radius=\"0.05\"/></geometry></collision>\n<inertial>\n<mass value=\"2\"/>\n<inertia ixx=\"0.1\" ixy=\"0\" ixz=\"0\" iyy=\"0.1\" iyz=\"0\" izz=\"0.1\"/>\n</inertial>\n</link>\n<!-- F. Right Shoulder Joint -->\n<joint name=\"right_shoulder_joint\" type=\"revolute\">\n<parent link=\"torso\"/>\n<child link=\"right_upper_arm\"/>\n<origin xyz=\"0 -0.20 0.15\" rpy=\"1.5707 0 0\"/> <!-- Position of arm relative to torso -->\n<axis xyz=\"0 0 1\"/>\n<limit lower=\"-1.57\" upper=\"1.57\" effort=\"10\" velocity=\"1.0\"/>\n</joint>\n</robot>\nHow to Use This URDF\n- Save the code above as\nsimple_humanoid.urdf\n. - You can check its validity using the\ncheck_urdf\ncommand:check_urdf simple_humanoid.urdf\n. - To visualize it, you can use ROS tools like RViz. This typically requires a small launch file to publish the robot's state, which is a topic for a later module.\nThis simple example provides the foundation. A full humanoid model would extend this tree with more links and joints for the legs, a more complex arm, and hands.\n--------------------------------------------------------------------------------"
  },
  {
    "url": "https://Muham251.github.io/ai-hackathon-1/docs/module-1/urdf-essentials#how-to-use-this-urdf",
    "text": "URDF Basics for Humanoid Models\nThe Unified Robot Description Format (URDF) is an XML format used in ROS to describe all the physical elements of a robot model. For a humanoid, the URDF file is its digital blueprint. It defines the robot's body parts, how they are connected, and their physical properties.\nA URDF file describes the robot as a tree of links connected by joints.\n<link>\n: A physical part of the robot (e.g., a forearm, a foot, a torso).<joint>\n: The connection between two links. It defines how one link moves relative to another (e.g., rotating like an elbow, sliding, or being fixed).\nKey URDF Tags\nThe <robot>\nTag\nEvery URDF file must start and end with the <robot>\ntag. It's the root element, and you give your robot a name here.\n<robot name=\"simple_humanoid\">\n... all your links and joints go here ...\n</robot>\nThe <link>\nTag\nEach link has a name and can contain three important sub-tags:\n<visual>\n: Defines the appearance of the link (shape, size, color, texture). This is what you see in simulation tools like RViz.<collision>\n: Defines the collision geometry of the link. This is what the physics engine uses to calculate collisions with other objects. It's often a simpler shape than the visual one for performance reasons.<inertial>\n: Defines the dynamic properties of the link: its mass, center of mass, and inertia matrix. This is crucial for realistic physics simulation.\n<link name=\"torso\">\n<visual>\n<geometry>\n<box size=\"0.2 0.3 0.5\"/>\n</geometry>\n<material name=\"grey\">\n<color rgba=\"0.5 0.5 0.5 1\"/>\n</material>\n</visual>\n<collision>\n<geometry>\n<box size=\"0.2 0.3 0.5\"/>\n</geometry>\n</collision>\n<inertial>\n<mass value=\"10\"/>\n<inertia ixx=\"0.4\" ixy=\"0.0\" ixz=\"0.0\" iyy=\"0.4\" iyz=\"0.0\" izz=\"0.2\"/>\n</inertial>\n</link>\nThe <joint>\nTag\nEach joint connects two links, called the parent and the child. It also defines the motion allowed between them.\nname\n: The name of the joint (e.g., \"left_shoulder_joint\").type\n: The type of motion. Common types are:revolute\n: Rotates around an axis (like an elbow). Requires<limit>\ntags for upper and lower angles.continuous\n: Rotates around an axis with no angle limits.prismatic\n: Slides along an axis (like a piston).fixed\n: No motion is allowed between the two links.\n<parent link=\"...\" />\n: The name of the parent link.<child link=\"...\" />\n: The name of the child link.<origin xyz=\"...\" rpy=\"...\" />\n: The transform (position and orientation) from the parent link's origin to the joint's origin.<axis xyz=\"...\" />\n: The axis of rotation or translation forrevolute\nandprismatic\njoints.\n<joint name=\"neck_joint\" type=\"revolute\">\n<parent link=\"torso\"/>\n<child link=\"head\"/>\n<origin xyz=\"0 0 0.25\" rpy=\"0 0 0\"/>\n<axis xyz=\"0 0 1\"/>\n<limit lower=\"-0.5\" upper=\"0.5\" effort=\"10\" velocity=\"1.0\"/>\n</joint>\nSimple Humanoid URDF Example\nThis example describes a very simple humanoid robot. It has a torso, a head, and a single arm with a shoulder and elbow joint. This demonstrates the parent-child tree structure. The \"torso\" is the root link of our tree.\n<!-- simple_humanoid.urdf -->\n<robot name=\"simple_humanoid\">\n<!-- A. Materials (for color) -->\n<material name=\"blue\">\n<color rgba=\"0.0 0.0 0.8 1.0\"/>\n</material>\n<material name=\"white\">\n<color rgba=\"1.0 1.0 1.0 1.0\"/>\n</material>\n<!-- B. Base Link (Torso) -->\n<link name=\"torso\">\n<visual>\n<geometry><box size=\"0.2 0.3 0.5\"/></geometry>\n<origin xyz=\"0 0 0\" rpy=\"0 0 0\"/>\n<material name=\"blue\"/>\n</visual>\n<collision><geometry><box size=\"0.2 0.3 0.5\"/></geometry></collision>\n<inertial>\n<mass value=\"10\"/>\n<inertia ixx=\"1.0\" ixy=\"0\" ixz=\"0\" iyy=\"1.0\" iyz=\"0\" izz=\"1.0\"/>\n</inertial>\n</link>\n<!-- C. Head Link -->\n<link name=\"head\">\n<visual>\n<geometry><sphere radius=\"0.1\"/></geometry>\n<origin xyz=\"0 0 0\" rpy=\"0 0 0\"/>\n<material name=\"white\"/>\n</visual>\n<collision><geometry><sphere radius=\"0.1\"/></geometry></collision>\n<inertial>\n<mass value=\"1\"/>\n<inertia ixx=\"0.1\" ixy=\"0\" ixz=\"0\" iyy=\"0.1\" iyz=\"0\" izz=\"0.1\"/>\n</inertial>\n</link>\n<!-- D. Neck Joint (Connects Torso to Head) -->\n<joint name=\"neck_joint\" type=\"revolute\">\n<parent link=\"torso\"/>\n<child link=\"head\"/>\n<origin xyz=\"0 0 0.30\" rpy=\"0 0 0\"/> <!-- Position of head relative to torso -->\n<axis xyz=\"0 0 1\"/>\n<limit lower=\"-0.7\" upper=\"0.7\" effort=\"5\" velocity=\"0.5\"/>\n</joint>\n<!-- E. Right Arm -->\n<link name=\"right_upper_arm\">\n<visual>\n<geometry><cylinder length=\"0.3\" radius=\"0.05\"/></geometry>\n<origin xyz=\"0 0 -0.15\" rpy=\"0 0 0\"/>\n<material name=\"white\"/>\n</visual>\n<collision><geometry><cylinder length=\"0.3\" radius=\"0.05\"/></geometry></collision>\n<inertial>\n<mass value=\"2\"/>\n<inertia ixx=\"0.1\" ixy=\"0\" ixz=\"0\" iyy=\"0.1\" iyz=\"0\" izz=\"0.1\"/>\n</inertial>\n</link>\n<!-- F. Right Shoulder Joint -->\n<joint name=\"right_shoulder_joint\" type=\"revolute\">\n<parent link=\"torso\"/>\n<child link=\"right_upper_arm\"/>\n<origin xyz=\"0 -0.20 0.15\" rpy=\"1.5707 0 0\"/> <!-- Position of arm relative to torso -->\n<axis xyz=\"0 0 1\"/>\n<limit lower=\"-1.57\" upper=\"1.57\" effort=\"10\" velocity=\"1.0\"/>\n</joint>\n</robot>\nHow to Use This URDF\n- Save the code above as\nsimple_humanoid.urdf\n. - You can check its validity using the\ncheck_urdf\ncommand:check_urdf simple_humanoid.urdf\n. - To visualize it, you can use ROS tools like RViz. This typically requires a small launch file to publish the robot's state, which is a topic for a later module.\nThis simple example provides the foundation. A full humanoid model would extend this tree with more links and joints for the legs, a more complex arm, and hands.\n--------------------------------------------------------------------------------"
  },
  {
    "url": "https://Muham251.github.io/ai-hackathon-1/docs/module-1/ros-2-basics-for-humanoid-control#target-audience",
    "text": "Module 1: The Robotic Nervous System (ROS 2)\nWelcome to Module 1! This module introduces the Robot Operating System (ROS 2), the fundamental software framework that acts as the nervous system for modern robots, including humanoids. As we journey into controlling complex humanoid robots, understanding ROS 2 is the essential first step.\nTarget Audience\nThis module is designed for students and developers who are new to robotics but have some programming experience, particularly in Python. If you're aiming to understand how to make a humanoid robot walk, talk, or interact with its environment, you're in the right place.\nWhat is ROS 2?\nROS 2 is a flexible framework for writing robot software. It is a set of software libraries and tools that help you build robot applications. Think of it as the middleware that handles all the complex communication between different parts of your robot's software. From low-level motor control to high-level planning and perception, ROS 2 provides the tools to manage it all.\nFor a humanoid robot, this is crucial. A humanoid may have dozens of motors (actuators), a suite of sensors (cameras, IMUs, force sensors), and multiple computers running different software modules. ROS 2 allows these components to communicate with each other in a standardized way.\nLearning Objectives\nBy the end of this module, you will be able to:\n- Explain the core components of the ROS 2 architecture.\n- Create and run ROS 2 nodes, topics, and services.\n- Write a simple Python agent that communicates with a ROS 2 system.\n- Understand and write a basic URDF file for a humanoid robot model.\nModule Chapters\nThis module is divided into the following chapters:\n- ROS 2 Fundamentals (Nodes, Topics, Services): A deep dive into the core communication patterns in ROS 2.\n- URDF Basics for Humanoid Models: An introduction to the Unified Robot Description Format (URDF) for modeling your humanoid.\nLet's get started!\n--------------------------------------------------------------------------------"
  },
  {
    "url": "https://Muham251.github.io/ai-hackathon-1/docs/module-1/ros-2-basics-for-humanoid-control#what-is-ros-2",
    "text": "Module 1: The Robotic Nervous System (ROS 2)\nWelcome to Module 1! This module introduces the Robot Operating System (ROS 2), the fundamental software framework that acts as the nervous system for modern robots, including humanoids. As we journey into controlling complex humanoid robots, understanding ROS 2 is the essential first step.\nTarget Audience\nThis module is designed for students and developers who are new to robotics but have some programming experience, particularly in Python. If you're aiming to understand how to make a humanoid robot walk, talk, or interact with its environment, you're in the right place.\nWhat is ROS 2?\nROS 2 is a flexible framework for writing robot software. It is a set of software libraries and tools that help you build robot applications. Think of it as the middleware that handles all the complex communication between different parts of your robot's software. From low-level motor control to high-level planning and perception, ROS 2 provides the tools to manage it all.\nFor a humanoid robot, this is crucial. A humanoid may have dozens of motors (actuators), a suite of sensors (cameras, IMUs, force sensors), and multiple computers running different software modules. ROS 2 allows these components to communicate with each other in a standardized way.\nLearning Objectives\nBy the end of this module, you will be able to:\n- Explain the core components of the ROS 2 architecture.\n- Create and run ROS 2 nodes, topics, and services.\n- Write a simple Python agent that communicates with a ROS 2 system.\n- Understand and write a basic URDF file for a humanoid robot model.\nModule Chapters\nThis module is divided into the following chapters:\n- ROS 2 Fundamentals (Nodes, Topics, Services): A deep dive into the core communication patterns in ROS 2.\n- URDF Basics for Humanoid Models: An introduction to the Unified Robot Description Format (URDF) for modeling your humanoid.\nLet's get started!\n--------------------------------------------------------------------------------"
  },
  {
    "url": "https://Muham251.github.io/ai-hackathon-1/docs/module-1/ros-2-basics-for-humanoid-control#learning-objectives",
    "text": "Module 1: The Robotic Nervous System (ROS 2)\nWelcome to Module 1! This module introduces the Robot Operating System (ROS 2), the fundamental software framework that acts as the nervous system for modern robots, including humanoids. As we journey into controlling complex humanoid robots, understanding ROS 2 is the essential first step.\nTarget Audience\nThis module is designed for students and developers who are new to robotics but have some programming experience, particularly in Python. If you're aiming to understand how to make a humanoid robot walk, talk, or interact with its environment, you're in the right place.\nWhat is ROS 2?\nROS 2 is a flexible framework for writing robot software. It is a set of software libraries and tools that help you build robot applications. Think of it as the middleware that handles all the complex communication between different parts of your robot's software. From low-level motor control to high-level planning and perception, ROS 2 provides the tools to manage it all.\nFor a humanoid robot, this is crucial. A humanoid may have dozens of motors (actuators), a suite of sensors (cameras, IMUs, force sensors), and multiple computers running different software modules. ROS 2 allows these components to communicate with each other in a standardized way.\nLearning Objectives\nBy the end of this module, you will be able to:\n- Explain the core components of the ROS 2 architecture.\n- Create and run ROS 2 nodes, topics, and services.\n- Write a simple Python agent that communicates with a ROS 2 system.\n- Understand and write a basic URDF file for a humanoid robot model.\nModule Chapters\nThis module is divided into the following chapters:\n- ROS 2 Fundamentals (Nodes, Topics, Services): A deep dive into the core communication patterns in ROS 2.\n- URDF Basics for Humanoid Models: An introduction to the Unified Robot Description Format (URDF) for modeling your humanoid.\nLet's get started!\n--------------------------------------------------------------------------------"
  },
  {
    "url": "https://Muham251.github.io/ai-hackathon-1/docs/module-1/ros-2-basics-for-humanoid-control#module-chapters",
    "text": "Module 1: The Robotic Nervous System (ROS 2)\nWelcome to Module 1! This module introduces the Robot Operating System (ROS 2), the fundamental software framework that acts as the nervous system for modern robots, including humanoids. As we journey into controlling complex humanoid robots, understanding ROS 2 is the essential first step.\nTarget Audience\nThis module is designed for students and developers who are new to robotics but have some programming experience, particularly in Python. If you're aiming to understand how to make a humanoid robot walk, talk, or interact with its environment, you're in the right place.\nWhat is ROS 2?\nROS 2 is a flexible framework for writing robot software. It is a set of software libraries and tools that help you build robot applications. Think of it as the middleware that handles all the complex communication between different parts of your robot's software. From low-level motor control to high-level planning and perception, ROS 2 provides the tools to manage it all.\nFor a humanoid robot, this is crucial. A humanoid may have dozens of motors (actuators), a suite of sensors (cameras, IMUs, force sensors), and multiple computers running different software modules. ROS 2 allows these components to communicate with each other in a standardized way.\nLearning Objectives\nBy the end of this module, you will be able to:\n- Explain the core components of the ROS 2 architecture.\n- Create and run ROS 2 nodes, topics, and services.\n- Write a simple Python agent that communicates with a ROS 2 system.\n- Understand and write a basic URDF file for a humanoid robot model.\nModule Chapters\nThis module is divided into the following chapters:\n- ROS 2 Fundamentals (Nodes, Topics, Services): A deep dive into the core communication patterns in ROS 2.\n- URDF Basics for Humanoid Models: An introduction to the Unified Robot Description Format (URDF) for modeling your humanoid.\nLet's get started!\n--------------------------------------------------------------------------------"
  },
  {
    "url": "https://Muham251.github.io/ai-hackathon-1/docs/module-1/nodes-topics-services-rclpy#1-ros-2-nodes",
    "text": "ROS 2 Fundamentals & Python\nThis chapter covers the three fundamental communication concepts in ROS 2: Nodes, Topics, and Services. We'll also see how to implement them using rclpy\n, the official Python client library for ROS 2.\n1. ROS 2 Nodes\nA Node is the smallest, most fundamental unit of a ROS 2 system. Think of a node as a single, independent process in your robot's software architecture. Each node should have a single, well-defined purpose.\nFor a humanoid robot, you might have nodes for:\n- Reading sensor data from an IMU.\n- Controlling the motors in the right leg.\n- Processing camera images.\n- Planning footsteps for walking.\nNodes are what allow ROS 2 systems to be modular and fault-tolerant. If one node crashes, the rest of the system can continue to run.\nExample: A Simple rclpy\nNode\nHere is the \"hello world\" of rclpy\n: a simple node that initializes itself, prints a message, and then shuts down.\n# 'hello_node.py'\nimport rclpy\nfrom rclpy.node import Node\ndef main(args=None):\n# 1. Initialize the rclpy library\nrclpy.init(args=args)\n# 2. Create a Node\n# The node is named 'hello_ros_node'\nnode = Node('hello_ros_node')\n# 3. From this point, the node is running.\n# You can add your logic here.\nnode.get_logger().info('Hello, ROS 2 World!')\n# 4. The node is destroyed when the 'with' statement exits.\n# This is important for cleanup.\nnode.destroy_node()\n# 5. Shutdown the rclpy library\nrclpy.shutdown()\nif __name__ == '__main__':\nmain()\nTo run this code:\n- Save it as\nhello_node.py\n. - Make sure you have a ROS 2 environment sourced.\n- Run\npython hello_node.py\n.\n2. ROS 2 Topics (Publish/Subscribe)\nTopics are the primary mechanism for one-way, asynchronous communication in ROS 2. Nodes can publish messages to a topic, and other nodes can subscribe to that topic to receive those messages. This is a decoupled communication model; publishers and subscribers don't need to know about each other.\n- Use Case: Continuously streaming data, like sensor readings or motor commands.\nDiagram: Publisher-Subscriber Model\ngraph TD\nA[Publisher Node] -- Publishes message --> B(Topic: /sensor_data);\nB -- Message is delivered --> C[Subscriber Node 1];\nB -- Message is delivered --> D[Subscriber Node 2];\nExample: A Simple Publisher and Subscriber\nFirst, we define a publisher node that sends a simple string message every second.\n# 'publisher_node.py'\nimport rclpy\nfrom rclpy.node import Node\nfrom std_msgs.msg import String\nclass SimplePublisher(Node):\ndef __init__(self):\nsuper().__init__('simple_publisher')\n# Create a publisher on the 'chatter' topic\nself.publisher_ = self.create_publisher(String, 'chatter', 10)\nself.timer = self.create_timer(1.0, self.timer_callback)\nself.i = 0\ndef timer_callback(self):\nmsg = String()\nmsg.data = f'Hello from publisher: {self.i}'\nself.publisher_.publish(msg)\nself.get_logger().info(f'Publishing: \"{msg.data}\"')\nself.i += 1\ndef main(args=None):\nrclpy.init(args=args)\npublisher = SimplePublisher()\nrclpy.spin(publisher) # Keep the node alive\npublisher.destroy_node()\nrclpy.shutdown()\nif __name__ == '__main__':\nmain()\nNext, a subscriber node that listens to the chatter\ntopic.\n# 'subscriber_node.py'\nimport rclpy\nfrom rclpy.node import Node\nfrom std_msgs.msg import String\nclass SimpleSubscriber(Node):\ndef __init__(self):\nsuper().__init__('simple_subscriber')\n# Create a subscription to the 'chatter' topic\nself.subscription = self.create_subscription(\nString,\n'chatter',\nself.listener_callback,\n10)\nself.subscription # prevent unused variable warning\ndef listener_callback(self, msg):\nself.get_logger().info(f'I heard: \"{msg.data}\"')\ndef main(args=None):\nrclpy.init(args=args)\nsubscriber = SimpleSubscriber()\nrclpy.spin(subscriber)\nsubscriber.destroy_node()\nrclpy.shutdown()\nif __name__ == '__main__':\nmain()\nTo run this example:\n- Open two terminals with your ROS 2 environment sourced.\n- In the first terminal, run\npython publisher_node.py\n. - In the second terminal, run\npython subscriber_node.py\n. You will see the messages from the publisher being received.\n3. ROS 2 Services (Request/Response)\nServices are for two-way, synchronous communication. A Service Server node provides a service, and a Service Client node can send a request and wait for a response. This is a tightly coupled, blocking communication model.\n- Use Case: Triggering a specific action that has a clear start and end, like \"open gripper\" or \"calculate inverse kinematics\".\nDiagram: Service Client/Server Model\ngraph TD\nA[Service Client Node] -- Sends Request --> B(Service: /add_two_ints);\nB -- Delivers Request --> C[Service Server Node];\nC -- Processes and sends Response --> B;\nB -- Delivers Response --> A;\nExample: A Simple Service Server and Client\nFirst, the server node that offers a service to add two integers. ROS 2 has a standard service definition for this, example_interfaces/srv/AddTwoInts\n.\n# 'service_server_node.py'\nimport rclpy\nfrom rclpy.node import Node\nfrom example_interfaces.srv import AddTwoInts\nclass SimpleServiceServer(Node):\ndef __init__(self):\nsuper().__init__('simple_service_server')\n# Create a service named 'add_two_ints'\nself.srv = self.create_service(AddTwoInts, 'add_two_ints', self.add_two_ints_callback)\ndef add_two_ints_callback(self, request, response):\nresponse.sum = request.a + request.b\nself.get_logger().info(f'Incoming request: a={request.a}, b={request.b}. Returning sum={response.sum}')\nreturn response\ndef main(args=None):\nrclpy.init(args=args)\nserver = SimpleServiceServer()\nrclpy.spin(server)\nserver.destroy_node()\nrclpy.shutdown()\nif __name__ == '__main__':\nmain()\nNext, the client node that calls the service.\n# 'service_client_node.py'\nimport rclpy\nfrom rclpy.node import Node\nfrom example_interfaces.srv import AddTwoInts\nimport sys\nclass SimpleServiceClient(Node):\ndef __init__(self):\nsuper().__init__('simple_service_client')\n# Create a client for the 'add_two_ints' service\nself.client = self.create_client(AddTwoInts, 'add_two_ints')\nwhile not self.client.wait_for_service(timeout_sec=1.0):\nself.get_logger().info('Service not available, waiting again...')\nself.req = AddTwoInts.Request()\ndef send_request(self, a, b):\nself.req.a = a\nself.req.b = b\nself.future = self.client.call_async(self.req)\nrclpy.spin_until_future_complete(self, self.future)\nreturn self.future.result()\ndef main(args=None):\nrclpy.init(args=args)\nif len(sys.argv) != 3:\nprint(\"Usage: python service_client_node.py <int1> <int2>\")\nreturn\nclient = SimpleServiceClient()\nresponse = client.send_request(int(sys.argv[1]), int(sys.argv[2]))\nif response:\nclient.get_logger().info(\nf'Result of add_two_ints: for {sys.argv[1]} + {sys.argv[2]} = {response.sum}')\nelse:\nclient.get_logger().error('Service call failed')\nclient.destroy_node()\nrclpy.shutdown()\nif __name__ == '__main__':\nmain()\nTo run this example:\n- Open two terminals with your ROS 2 environment sourced.\n- In the first, run\npython service_server_node.py\n. - In the second, run\npython service_client_node.py 5 10\n. The client will send the request and print the response.\n--------------------------------------------------------------------------------"
  },
  {
    "url": "https://Muham251.github.io/ai-hackathon-1/docs/module-1/nodes-topics-services-rclpy#example-a-simple-rclpy-node",
    "text": "ROS 2 Fundamentals & Python\nThis chapter covers the three fundamental communication concepts in ROS 2: Nodes, Topics, and Services. We'll also see how to implement them using rclpy\n, the official Python client library for ROS 2.\n1. ROS 2 Nodes\nA Node is the smallest, most fundamental unit of a ROS 2 system. Think of a node as a single, independent process in your robot's software architecture. Each node should have a single, well-defined purpose.\nFor a humanoid robot, you might have nodes for:\n- Reading sensor data from an IMU.\n- Controlling the motors in the right leg.\n- Processing camera images.\n- Planning footsteps for walking.\nNodes are what allow ROS 2 systems to be modular and fault-tolerant. If one node crashes, the rest of the system can continue to run.\nExample: A Simple rclpy\nNode\nHere is the \"hello world\" of rclpy\n: a simple node that initializes itself, prints a message, and then shuts down.\n# 'hello_node.py'\nimport rclpy\nfrom rclpy.node import Node\ndef main(args=None):\n# 1. Initialize the rclpy library\nrclpy.init(args=args)\n# 2. Create a Node\n# The node is named 'hello_ros_node'\nnode = Node('hello_ros_node')\n# 3. From this point, the node is running.\n# You can add your logic here.\nnode.get_logger().info('Hello, ROS 2 World!')\n# 4. The node is destroyed when the 'with' statement exits.\n# This is important for cleanup.\nnode.destroy_node()\n# 5. Shutdown the rclpy library\nrclpy.shutdown()\nif __name__ == '__main__':\nmain()\nTo run this code:\n- Save it as\nhello_node.py\n. - Make sure you have a ROS 2 environment sourced.\n- Run\npython hello_node.py\n.\n2. ROS 2 Topics (Publish/Subscribe)\nTopics are the primary mechanism for one-way, asynchronous communication in ROS 2. Nodes can publish messages to a topic, and other nodes can subscribe to that topic to receive those messages. This is a decoupled communication model; publishers and subscribers don't need to know about each other.\n- Use Case: Continuously streaming data, like sensor readings or motor commands.\nDiagram: Publisher-Subscriber Model\ngraph TD\nA[Publisher Node] -- Publishes message --> B(Topic: /sensor_data);\nB -- Message is delivered --> C[Subscriber Node 1];\nB -- Message is delivered --> D[Subscriber Node 2];\nExample: A Simple Publisher and Subscriber\nFirst, we define a publisher node that sends a simple string message every second.\n# 'publisher_node.py'\nimport rclpy\nfrom rclpy.node import Node\nfrom std_msgs.msg import String\nclass SimplePublisher(Node):\ndef __init__(self):\nsuper().__init__('simple_publisher')\n# Create a publisher on the 'chatter' topic\nself.publisher_ = self.create_publisher(String, 'chatter', 10)\nself.timer = self.create_timer(1.0, self.timer_callback)\nself.i = 0\ndef timer_callback(self):\nmsg = String()\nmsg.data = f'Hello from publisher: {self.i}'\nself.publisher_.publish(msg)\nself.get_logger().info(f'Publishing: \"{msg.data}\"')\nself.i += 1\ndef main(args=None):\nrclpy.init(args=args)\npublisher = SimplePublisher()\nrclpy.spin(publisher) # Keep the node alive\npublisher.destroy_node()\nrclpy.shutdown()\nif __name__ == '__main__':\nmain()\nNext, a subscriber node that listens to the chatter\ntopic.\n# 'subscriber_node.py'\nimport rclpy\nfrom rclpy.node import Node\nfrom std_msgs.msg import String\nclass SimpleSubscriber(Node):\ndef __init__(self):\nsuper().__init__('simple_subscriber')\n# Create a subscription to the 'chatter' topic\nself.subscription = self.create_subscription(\nString,\n'chatter',\nself.listener_callback,\n10)\nself.subscription # prevent unused variable warning\ndef listener_callback(self, msg):\nself.get_logger().info(f'I heard: \"{msg.data}\"')\ndef main(args=None):\nrclpy.init(args=args)\nsubscriber = SimpleSubscriber()\nrclpy.spin(subscriber)\nsubscriber.destroy_node()\nrclpy.shutdown()\nif __name__ == '__main__':\nmain()\nTo run this example:\n- Open two terminals with your ROS 2 environment sourced.\n- In the first terminal, run\npython publisher_node.py\n. - In the second terminal, run\npython subscriber_node.py\n. You will see the messages from the publisher being received.\n3. ROS 2 Services (Request/Response)\nServices are for two-way, synchronous communication. A Service Server node provides a service, and a Service Client node can send a request and wait for a response. This is a tightly coupled, blocking communication model.\n- Use Case: Triggering a specific action that has a clear start and end, like \"open gripper\" or \"calculate inverse kinematics\".\nDiagram: Service Client/Server Model\ngraph TD\nA[Service Client Node] -- Sends Request --> B(Service: /add_two_ints);\nB -- Delivers Request --> C[Service Server Node];\nC -- Processes and sends Response --> B;\nB -- Delivers Response --> A;\nExample: A Simple Service Server and Client\nFirst, the server node that offers a service to add two integers. ROS 2 has a standard service definition for this, example_interfaces/srv/AddTwoInts\n.\n# 'service_server_node.py'\nimport rclpy\nfrom rclpy.node import Node\nfrom example_interfaces.srv import AddTwoInts\nclass SimpleServiceServer(Node):\ndef __init__(self):\nsuper().__init__('simple_service_server')\n# Create a service named 'add_two_ints'\nself.srv = self.create_service(AddTwoInts, 'add_two_ints', self.add_two_ints_callback)\ndef add_two_ints_callback(self, request, response):\nresponse.sum = request.a + request.b\nself.get_logger().info(f'Incoming request: a={request.a}, b={request.b}. Returning sum={response.sum}')\nreturn response\ndef main(args=None):\nrclpy.init(args=args)\nserver = SimpleServiceServer()\nrclpy.spin(server)\nserver.destroy_node()\nrclpy.shutdown()\nif __name__ == '__main__':\nmain()\nNext, the client node that calls the service.\n# 'service_client_node.py'\nimport rclpy\nfrom rclpy.node import Node\nfrom example_interfaces.srv import AddTwoInts\nimport sys\nclass SimpleServiceClient(Node):\ndef __init__(self):\nsuper().__init__('simple_service_client')\n# Create a client for the 'add_two_ints' service\nself.client = self.create_client(AddTwoInts, 'add_two_ints')\nwhile not self.client.wait_for_service(timeout_sec=1.0):\nself.get_logger().info('Service not available, waiting again...')\nself.req = AddTwoInts.Request()\ndef send_request(self, a, b):\nself.req.a = a\nself.req.b = b\nself.future = self.client.call_async(self.req)\nrclpy.spin_until_future_complete(self, self.future)\nreturn self.future.result()\ndef main(args=None):\nrclpy.init(args=args)\nif len(sys.argv) != 3:\nprint(\"Usage: python service_client_node.py <int1> <int2>\")\nreturn\nclient = SimpleServiceClient()\nresponse = client.send_request(int(sys.argv[1]), int(sys.argv[2]))\nif response:\nclient.get_logger().info(\nf'Result of add_two_ints: for {sys.argv[1]} + {sys.argv[2]} = {response.sum}')\nelse:\nclient.get_logger().error('Service call failed')\nclient.destroy_node()\nrclpy.shutdown()\nif __name__ == '__main__':\nmain()\nTo run this example:\n- Open two terminals with your ROS 2 environment sourced.\n- In the first, run\npython service_server_node.py\n. - In the second, run\npython service_client_node.py 5 10\n. The client will send the request and print the response.\n--------------------------------------------------------------------------------"
  },
  {
    "url": "https://Muham251.github.io/ai-hackathon-1/docs/module-1/nodes-topics-services-rclpy#2-ros-2-topics-publishsubscribe",
    "text": "ROS 2 Fundamentals & Python\nThis chapter covers the three fundamental communication concepts in ROS 2: Nodes, Topics, and Services. We'll also see how to implement them using rclpy\n, the official Python client library for ROS 2.\n1. ROS 2 Nodes\nA Node is the smallest, most fundamental unit of a ROS 2 system. Think of a node as a single, independent process in your robot's software architecture. Each node should have a single, well-defined purpose.\nFor a humanoid robot, you might have nodes for:\n- Reading sensor data from an IMU.\n- Controlling the motors in the right leg.\n- Processing camera images.\n- Planning footsteps for walking.\nNodes are what allow ROS 2 systems to be modular and fault-tolerant. If one node crashes, the rest of the system can continue to run.\nExample: A Simple rclpy\nNode\nHere is the \"hello world\" of rclpy\n: a simple node that initializes itself, prints a message, and then shuts down.\n# 'hello_node.py'\nimport rclpy\nfrom rclpy.node import Node\ndef main(args=None):\n# 1. Initialize the rclpy library\nrclpy.init(args=args)\n# 2. Create a Node\n# The node is named 'hello_ros_node'\nnode = Node('hello_ros_node')\n# 3. From this point, the node is running.\n# You can add your logic here.\nnode.get_logger().info('Hello, ROS 2 World!')\n# 4. The node is destroyed when the 'with' statement exits.\n# This is important for cleanup.\nnode.destroy_node()\n# 5. Shutdown the rclpy library\nrclpy.shutdown()\nif __name__ == '__main__':\nmain()\nTo run this code:\n- Save it as\nhello_node.py\n. - Make sure you have a ROS 2 environment sourced.\n- Run\npython hello_node.py\n.\n2. ROS 2 Topics (Publish/Subscribe)\nTopics are the primary mechanism for one-way, asynchronous communication in ROS 2. Nodes can publish messages to a topic, and other nodes can subscribe to that topic to receive those messages. This is a decoupled communication model; publishers and subscribers don't need to know about each other.\n- Use Case: Continuously streaming data, like sensor readings or motor commands.\nDiagram: Publisher-Subscriber Model\ngraph TD\nA[Publisher Node] -- Publishes message --> B(Topic: /sensor_data);\nB -- Message is delivered --> C[Subscriber Node 1];\nB -- Message is delivered --> D[Subscriber Node 2];\nExample: A Simple Publisher and Subscriber\nFirst, we define a publisher node that sends a simple string message every second.\n# 'publisher_node.py'\nimport rclpy\nfrom rclpy.node import Node\nfrom std_msgs.msg import String\nclass SimplePublisher(Node):\ndef __init__(self):\nsuper().__init__('simple_publisher')\n# Create a publisher on the 'chatter' topic\nself.publisher_ = self.create_publisher(String, 'chatter', 10)\nself.timer = self.create_timer(1.0, self.timer_callback)\nself.i = 0\ndef timer_callback(self):\nmsg = String()\nmsg.data = f'Hello from publisher: {self.i}'\nself.publisher_.publish(msg)\nself.get_logger().info(f'Publishing: \"{msg.data}\"')\nself.i += 1\ndef main(args=None):\nrclpy.init(args=args)\npublisher = SimplePublisher()\nrclpy.spin(publisher) # Keep the node alive\npublisher.destroy_node()\nrclpy.shutdown()\nif __name__ == '__main__':\nmain()\nNext, a subscriber node that listens to the chatter\ntopic.\n# 'subscriber_node.py'\nimport rclpy\nfrom rclpy.node import Node\nfrom std_msgs.msg import String\nclass SimpleSubscriber(Node):\ndef __init__(self):\nsuper().__init__('simple_subscriber')\n# Create a subscription to the 'chatter' topic\nself.subscription = self.create_subscription(\nString,\n'chatter',\nself.listener_callback,\n10)\nself.subscription # prevent unused variable warning\ndef listener_callback(self, msg):\nself.get_logger().info(f'I heard: \"{msg.data}\"')\ndef main(args=None):\nrclpy.init(args=args)\nsubscriber = SimpleSubscriber()\nrclpy.spin(subscriber)\nsubscriber.destroy_node()\nrclpy.shutdown()\nif __name__ == '__main__':\nmain()\nTo run this example:\n- Open two terminals with your ROS 2 environment sourced.\n- In the first terminal, run\npython publisher_node.py\n. - In the second terminal, run\npython subscriber_node.py\n. You will see the messages from the publisher being received.\n3. ROS 2 Services (Request/Response)\nServices are for two-way, synchronous communication. A Service Server node provides a service, and a Service Client node can send a request and wait for a response. This is a tightly coupled, blocking communication model.\n- Use Case: Triggering a specific action that has a clear start and end, like \"open gripper\" or \"calculate inverse kinematics\".\nDiagram: Service Client/Server Model\ngraph TD\nA[Service Client Node] -- Sends Request --> B(Service: /add_two_ints);\nB -- Delivers Request --> C[Service Server Node];\nC -- Processes and sends Response --> B;\nB -- Delivers Response --> A;\nExample: A Simple Service Server and Client\nFirst, the server node that offers a service to add two integers. ROS 2 has a standard service definition for this, example_interfaces/srv/AddTwoInts\n.\n# 'service_server_node.py'\nimport rclpy\nfrom rclpy.node import Node\nfrom example_interfaces.srv import AddTwoInts\nclass SimpleServiceServer(Node):\ndef __init__(self):\nsuper().__init__('simple_service_server')\n# Create a service named 'add_two_ints'\nself.srv = self.create_service(AddTwoInts, 'add_two_ints', self.add_two_ints_callback)\ndef add_two_ints_callback(self, request, response):\nresponse.sum = request.a + request.b\nself.get_logger().info(f'Incoming request: a={request.a}, b={request.b}. Returning sum={response.sum}')\nreturn response\ndef main(args=None):\nrclpy.init(args=args)\nserver = SimpleServiceServer()\nrclpy.spin(server)\nserver.destroy_node()\nrclpy.shutdown()\nif __name__ == '__main__':\nmain()\nNext, the client node that calls the service.\n# 'service_client_node.py'\nimport rclpy\nfrom rclpy.node import Node\nfrom example_interfaces.srv import AddTwoInts\nimport sys\nclass SimpleServiceClient(Node):\ndef __init__(self):\nsuper().__init__('simple_service_client')\n# Create a client for the 'add_two_ints' service\nself.client = self.create_client(AddTwoInts, 'add_two_ints')\nwhile not self.client.wait_for_service(timeout_sec=1.0):\nself.get_logger().info('Service not available, waiting again...')\nself.req = AddTwoInts.Request()\ndef send_request(self, a, b):\nself.req.a = a\nself.req.b = b\nself.future = self.client.call_async(self.req)\nrclpy.spin_until_future_complete(self, self.future)\nreturn self.future.result()\ndef main(args=None):\nrclpy.init(args=args)\nif len(sys.argv) != 3:\nprint(\"Usage: python service_client_node.py <int1> <int2>\")\nreturn\nclient = SimpleServiceClient()\nresponse = client.send_request(int(sys.argv[1]), int(sys.argv[2]))\nif response:\nclient.get_logger().info(\nf'Result of add_two_ints: for {sys.argv[1]} + {sys.argv[2]} = {response.sum}')\nelse:\nclient.get_logger().error('Service call failed')\nclient.destroy_node()\nrclpy.shutdown()\nif __name__ == '__main__':\nmain()\nTo run this example:\n- Open two terminals with your ROS 2 environment sourced.\n- In the first, run\npython service_server_node.py\n. - In the second, run\npython service_client_node.py 5 10\n. The client will send the request and print the response.\n--------------------------------------------------------------------------------"
  },
  {
    "url": "https://Muham251.github.io/ai-hackathon-1/docs/module-1/nodes-topics-services-rclpy#diagram-publisher-subscriber-model",
    "text": "ROS 2 Fundamentals & Python\nThis chapter covers the three fundamental communication concepts in ROS 2: Nodes, Topics, and Services. We'll also see how to implement them using rclpy\n, the official Python client library for ROS 2.\n1. ROS 2 Nodes\nA Node is the smallest, most fundamental unit of a ROS 2 system. Think of a node as a single, independent process in your robot's software architecture. Each node should have a single, well-defined purpose.\nFor a humanoid robot, you might have nodes for:\n- Reading sensor data from an IMU.\n- Controlling the motors in the right leg.\n- Processing camera images.\n- Planning footsteps for walking.\nNodes are what allow ROS 2 systems to be modular and fault-tolerant. If one node crashes, the rest of the system can continue to run.\nExample: A Simple rclpy\nNode\nHere is the \"hello world\" of rclpy\n: a simple node that initializes itself, prints a message, and then shuts down.\n# 'hello_node.py'\nimport rclpy\nfrom rclpy.node import Node\ndef main(args=None):\n# 1. Initialize the rclpy library\nrclpy.init(args=args)\n# 2. Create a Node\n# The node is named 'hello_ros_node'\nnode = Node('hello_ros_node')\n# 3. From this point, the node is running.\n# You can add your logic here.\nnode.get_logger().info('Hello, ROS 2 World!')\n# 4. The node is destroyed when the 'with' statement exits.\n# This is important for cleanup.\nnode.destroy_node()\n# 5. Shutdown the rclpy library\nrclpy.shutdown()\nif __name__ == '__main__':\nmain()\nTo run this code:\n- Save it as\nhello_node.py\n. - Make sure you have a ROS 2 environment sourced.\n- Run\npython hello_node.py\n.\n2. ROS 2 Topics (Publish/Subscribe)\nTopics are the primary mechanism for one-way, asynchronous communication in ROS 2. Nodes can publish messages to a topic, and other nodes can subscribe to that topic to receive those messages. This is a decoupled communication model; publishers and subscribers don't need to know about each other.\n- Use Case: Continuously streaming data, like sensor readings or motor commands.\nDiagram: Publisher-Subscriber Model\ngraph TD\nA[Publisher Node] -- Publishes message --> B(Topic: /sensor_data);\nB -- Message is delivered --> C[Subscriber Node 1];\nB -- Message is delivered --> D[Subscriber Node 2];\nExample: A Simple Publisher and Subscriber\nFirst, we define a publisher node that sends a simple string message every second.\n# 'publisher_node.py'\nimport rclpy\nfrom rclpy.node import Node\nfrom std_msgs.msg import String\nclass SimplePublisher(Node):\ndef __init__(self):\nsuper().__init__('simple_publisher')\n# Create a publisher on the 'chatter' topic\nself.publisher_ = self.create_publisher(String, 'chatter', 10)\nself.timer = self.create_timer(1.0, self.timer_callback)\nself.i = 0\ndef timer_callback(self):\nmsg = String()\nmsg.data = f'Hello from publisher: {self.i}'\nself.publisher_.publish(msg)\nself.get_logger().info(f'Publishing: \"{msg.data}\"')\nself.i += 1\ndef main(args=None):\nrclpy.init(args=args)\npublisher = SimplePublisher()\nrclpy.spin(publisher) # Keep the node alive\npublisher.destroy_node()\nrclpy.shutdown()\nif __name__ == '__main__':\nmain()\nNext, a subscriber node that listens to the chatter\ntopic.\n# 'subscriber_node.py'\nimport rclpy\nfrom rclpy.node import Node\nfrom std_msgs.msg import String\nclass SimpleSubscriber(Node):\ndef __init__(self):\nsuper().__init__('simple_subscriber')\n# Create a subscription to the 'chatter' topic\nself.subscription = self.create_subscription(\nString,\n'chatter',\nself.listener_callback,\n10)\nself.subscription # prevent unused variable warning\ndef listener_callback(self, msg):\nself.get_logger().info(f'I heard: \"{msg.data}\"')\ndef main(args=None):\nrclpy.init(args=args)\nsubscriber = SimpleSubscriber()\nrclpy.spin(subscriber)\nsubscriber.destroy_node()\nrclpy.shutdown()\nif __name__ == '__main__':\nmain()\nTo run this example:\n- Open two terminals with your ROS 2 environment sourced.\n- In the first terminal, run\npython publisher_node.py\n. - In the second terminal, run\npython subscriber_node.py\n. You will see the messages from the publisher being received.\n3. ROS 2 Services (Request/Response)\nServices are for two-way, synchronous communication. A Service Server node provides a service, and a Service Client node can send a request and wait for a response. This is a tightly coupled, blocking communication model.\n- Use Case: Triggering a specific action that has a clear start and end, like \"open gripper\" or \"calculate inverse kinematics\".\nDiagram: Service Client/Server Model\ngraph TD\nA[Service Client Node] -- Sends Request --> B(Service: /add_two_ints);\nB -- Delivers Request --> C[Service Server Node];\nC -- Processes and sends Response --> B;\nB -- Delivers Response --> A;\nExample: A Simple Service Server and Client\nFirst, the server node that offers a service to add two integers. ROS 2 has a standard service definition for this, example_interfaces/srv/AddTwoInts\n.\n# 'service_server_node.py'\nimport rclpy\nfrom rclpy.node import Node\nfrom example_interfaces.srv import AddTwoInts\nclass SimpleServiceServer(Node):\ndef __init__(self):\nsuper().__init__('simple_service_server')\n# Create a service named 'add_two_ints'\nself.srv = self.create_service(AddTwoInts, 'add_two_ints', self.add_two_ints_callback)\ndef add_two_ints_callback(self, request, response):\nresponse.sum = request.a + request.b\nself.get_logger().info(f'Incoming request: a={request.a}, b={request.b}. Returning sum={response.sum}')\nreturn response\ndef main(args=None):\nrclpy.init(args=args)\nserver = SimpleServiceServer()\nrclpy.spin(server)\nserver.destroy_node()\nrclpy.shutdown()\nif __name__ == '__main__':\nmain()\nNext, the client node that calls the service.\n# 'service_client_node.py'\nimport rclpy\nfrom rclpy.node import Node\nfrom example_interfaces.srv import AddTwoInts\nimport sys\nclass SimpleServiceClient(Node):\ndef __init__(self):\nsuper().__init__('simple_service_client')\n# Create a client for the 'add_two_ints' service\nself.client = self.create_client(AddTwoInts, 'add_two_ints')\nwhile not self.client.wait_for_service(timeout_sec=1.0):\nself.get_logger().info('Service not available, waiting again...')\nself.req = AddTwoInts.Request()\ndef send_request(self, a, b):\nself.req.a = a\nself.req.b = b\nself.future = self.client.call_async(self.req)\nrclpy.spin_until_future_complete(self, self.future)\nreturn self.future.result()\ndef main(args=None):\nrclpy.init(args=args)\nif len(sys.argv) != 3:\nprint(\"Usage: python service_client_node.py <int1> <int2>\")\nreturn\nclient = SimpleServiceClient()\nresponse = client.send_request(int(sys.argv[1]), int(sys.argv[2]))\nif response:\nclient.get_logger().info(\nf'Result of add_two_ints: for {sys.argv[1]} + {sys.argv[2]} = {response.sum}')\nelse:\nclient.get_logger().error('Service call failed')\nclient.destroy_node()\nrclpy.shutdown()\nif __name__ == '__main__':\nmain()\nTo run this example:\n- Open two terminals with your ROS 2 environment sourced.\n- In the first, run\npython service_server_node.py\n. - In the second, run\npython service_client_node.py 5 10\n. The client will send the request and print the response.\n--------------------------------------------------------------------------------"
  },
  {
    "url": "https://Muham251.github.io/ai-hackathon-1/docs/module-1/nodes-topics-services-rclpy#example-a-simple-publisher-and-subscriber",
    "text": "ROS 2 Fundamentals & Python\nThis chapter covers the three fundamental communication concepts in ROS 2: Nodes, Topics, and Services. We'll also see how to implement them using rclpy\n, the official Python client library for ROS 2.\n1. ROS 2 Nodes\nA Node is the smallest, most fundamental unit of a ROS 2 system. Think of a node as a single, independent process in your robot's software architecture. Each node should have a single, well-defined purpose.\nFor a humanoid robot, you might have nodes for:\n- Reading sensor data from an IMU.\n- Controlling the motors in the right leg.\n- Processing camera images.\n- Planning footsteps for walking.\nNodes are what allow ROS 2 systems to be modular and fault-tolerant. If one node crashes, the rest of the system can continue to run.\nExample: A Simple rclpy\nNode\nHere is the \"hello world\" of rclpy\n: a simple node that initializes itself, prints a message, and then shuts down.\n# 'hello_node.py'\nimport rclpy\nfrom rclpy.node import Node\ndef main(args=None):\n# 1. Initialize the rclpy library\nrclpy.init(args=args)\n# 2. Create a Node\n# The node is named 'hello_ros_node'\nnode = Node('hello_ros_node')\n# 3. From this point, the node is running.\n# You can add your logic here.\nnode.get_logger().info('Hello, ROS 2 World!')\n# 4. The node is destroyed when the 'with' statement exits.\n# This is important for cleanup.\nnode.destroy_node()\n# 5. Shutdown the rclpy library\nrclpy.shutdown()\nif __name__ == '__main__':\nmain()\nTo run this code:\n- Save it as\nhello_node.py\n. - Make sure you have a ROS 2 environment sourced.\n- Run\npython hello_node.py\n.\n2. ROS 2 Topics (Publish/Subscribe)\nTopics are the primary mechanism for one-way, asynchronous communication in ROS 2. Nodes can publish messages to a topic, and other nodes can subscribe to that topic to receive those messages. This is a decoupled communication model; publishers and subscribers don't need to know about each other.\n- Use Case: Continuously streaming data, like sensor readings or motor commands.\nDiagram: Publisher-Subscriber Model\ngraph TD\nA[Publisher Node] -- Publishes message --> B(Topic: /sensor_data);\nB -- Message is delivered --> C[Subscriber Node 1];\nB -- Message is delivered --> D[Subscriber Node 2];\nExample: A Simple Publisher and Subscriber\nFirst, we define a publisher node that sends a simple string message every second.\n# 'publisher_node.py'\nimport rclpy\nfrom rclpy.node import Node\nfrom std_msgs.msg import String\nclass SimplePublisher(Node):\ndef __init__(self):\nsuper().__init__('simple_publisher')\n# Create a publisher on the 'chatter' topic\nself.publisher_ = self.create_publisher(String, 'chatter', 10)\nself.timer = self.create_timer(1.0, self.timer_callback)\nself.i = 0\ndef timer_callback(self):\nmsg = String()\nmsg.data = f'Hello from publisher: {self.i}'\nself.publisher_.publish(msg)\nself.get_logger().info(f'Publishing: \"{msg.data}\"')\nself.i += 1\ndef main(args=None):\nrclpy.init(args=args)\npublisher = SimplePublisher()\nrclpy.spin(publisher) # Keep the node alive\npublisher.destroy_node()\nrclpy.shutdown()\nif __name__ == '__main__':\nmain()\nNext, a subscriber node that listens to the chatter\ntopic.\n# 'subscriber_node.py'\nimport rclpy\nfrom rclpy.node import Node\nfrom std_msgs.msg import String\nclass SimpleSubscriber(Node):\ndef __init__(self):\nsuper().__init__('simple_subscriber')\n# Create a subscription to the 'chatter' topic\nself.subscription = self.create_subscription(\nString,\n'chatter',\nself.listener_callback,\n10)\nself.subscription # prevent unused variable warning\ndef listener_callback(self, msg):\nself.get_logger().info(f'I heard: \"{msg.data}\"')\ndef main(args=None):\nrclpy.init(args=args)\nsubscriber = SimpleSubscriber()\nrclpy.spin(subscriber)\nsubscriber.destroy_node()\nrclpy.shutdown()\nif __name__ == '__main__':\nmain()\nTo run this example:\n- Open two terminals with your ROS 2 environment sourced.\n- In the first terminal, run\npython publisher_node.py\n. - In the second terminal, run\npython subscriber_node.py\n. You will see the messages from the publisher being received.\n3. ROS 2 Services (Request/Response)\nServices are for two-way, synchronous communication. A Service Server node provides a service, and a Service Client node can send a request and wait for a response. This is a tightly coupled, blocking communication model.\n- Use Case: Triggering a specific action that has a clear start and end, like \"open gripper\" or \"calculate inverse kinematics\".\nDiagram: Service Client/Server Model\ngraph TD\nA[Service Client Node] -- Sends Request --> B(Service: /add_two_ints);\nB -- Delivers Request --> C[Service Server Node];\nC -- Processes and sends Response --> B;\nB -- Delivers Response --> A;\nExample: A Simple Service Server and Client\nFirst, the server node that offers a service to add two integers. ROS 2 has a standard service definition for this, example_interfaces/srv/AddTwoInts\n.\n# 'service_server_node.py'\nimport rclpy\nfrom rclpy.node import Node\nfrom example_interfaces.srv import AddTwoInts\nclass SimpleServiceServer(Node):\ndef __init__(self):\nsuper().__init__('simple_service_server')\n# Create a service named 'add_two_ints'\nself.srv = self.create_service(AddTwoInts, 'add_two_ints', self.add_two_ints_callback)\ndef add_two_ints_callback(self, request, response):\nresponse.sum = request.a + request.b\nself.get_logger().info(f'Incoming request: a={request.a}, b={request.b}. Returning sum={response.sum}')\nreturn response\ndef main(args=None):\nrclpy.init(args=args)\nserver = SimpleServiceServer()\nrclpy.spin(server)\nserver.destroy_node()\nrclpy.shutdown()\nif __name__ == '__main__':\nmain()\nNext, the client node that calls the service.\n# 'service_client_node.py'\nimport rclpy\nfrom rclpy.node import Node\nfrom example_interfaces.srv import AddTwoInts\nimport sys\nclass SimpleServiceClient(Node):\ndef __init__(self):\nsuper().__init__('simple_service_client')\n# Create a client for the 'add_two_ints' service\nself.client = self.create_client(AddTwoInts, 'add_two_ints')\nwhile not self.client.wait_for_service(timeout_sec=1.0):\nself.get_logger().info('Service not available, waiting again...')\nself.req = AddTwoInts.Request()\ndef send_request(self, a, b):\nself.req.a = a\nself.req.b = b\nself.future = self.client.call_async(self.req)\nrclpy.spin_until_future_complete(self, self.future)\nreturn self.future.result()\ndef main(args=None):\nrclpy.init(args=args)\nif len(sys.argv) != 3:\nprint(\"Usage: python service_client_node.py <int1> <int2>\")\nreturn\nclient = SimpleServiceClient()\nresponse = client.send_request(int(sys.argv[1]), int(sys.argv[2]))\nif response:\nclient.get_logger().info(\nf'Result of add_two_ints: for {sys.argv[1]} + {sys.argv[2]} = {response.sum}')\nelse:\nclient.get_logger().error('Service call failed')\nclient.destroy_node()\nrclpy.shutdown()\nif __name__ == '__main__':\nmain()\nTo run this example:\n- Open two terminals with your ROS 2 environment sourced.\n- In the first, run\npython service_server_node.py\n. - In the second, run\npython service_client_node.py 5 10\n. The client will send the request and print the response.\n--------------------------------------------------------------------------------"
  },
  {
    "url": "https://Muham251.github.io/ai-hackathon-1/docs/module-1/nodes-topics-services-rclpy#3-ros-2-services-requestresponse",
    "text": "ROS 2 Fundamentals & Python\nThis chapter covers the three fundamental communication concepts in ROS 2: Nodes, Topics, and Services. We'll also see how to implement them using rclpy\n, the official Python client library for ROS 2.\n1. ROS 2 Nodes\nA Node is the smallest, most fundamental unit of a ROS 2 system. Think of a node as a single, independent process in your robot's software architecture. Each node should have a single, well-defined purpose.\nFor a humanoid robot, you might have nodes for:\n- Reading sensor data from an IMU.\n- Controlling the motors in the right leg.\n- Processing camera images.\n- Planning footsteps for walking.\nNodes are what allow ROS 2 systems to be modular and fault-tolerant. If one node crashes, the rest of the system can continue to run.\nExample: A Simple rclpy\nNode\nHere is the \"hello world\" of rclpy\n: a simple node that initializes itself, prints a message, and then shuts down.\n# 'hello_node.py'\nimport rclpy\nfrom rclpy.node import Node\ndef main(args=None):\n# 1. Initialize the rclpy library\nrclpy.init(args=args)\n# 2. Create a Node\n# The node is named 'hello_ros_node'\nnode = Node('hello_ros_node')\n# 3. From this point, the node is running.\n# You can add your logic here.\nnode.get_logger().info('Hello, ROS 2 World!')\n# 4. The node is destroyed when the 'with' statement exits.\n# This is important for cleanup.\nnode.destroy_node()\n# 5. Shutdown the rclpy library\nrclpy.shutdown()\nif __name__ == '__main__':\nmain()\nTo run this code:\n- Save it as\nhello_node.py\n. - Make sure you have a ROS 2 environment sourced.\n- Run\npython hello_node.py\n.\n2. ROS 2 Topics (Publish/Subscribe)\nTopics are the primary mechanism for one-way, asynchronous communication in ROS 2. Nodes can publish messages to a topic, and other nodes can subscribe to that topic to receive those messages. This is a decoupled communication model; publishers and subscribers don't need to know about each other.\n- Use Case: Continuously streaming data, like sensor readings or motor commands.\nDiagram: Publisher-Subscriber Model\ngraph TD\nA[Publisher Node] -- Publishes message --> B(Topic: /sensor_data);\nB -- Message is delivered --> C[Subscriber Node 1];\nB -- Message is delivered --> D[Subscriber Node 2];\nExample: A Simple Publisher and Subscriber\nFirst, we define a publisher node that sends a simple string message every second.\n# 'publisher_node.py'\nimport rclpy\nfrom rclpy.node import Node\nfrom std_msgs.msg import String\nclass SimplePublisher(Node):\ndef __init__(self):\nsuper().__init__('simple_publisher')\n# Create a publisher on the 'chatter' topic\nself.publisher_ = self.create_publisher(String, 'chatter', 10)\nself.timer = self.create_timer(1.0, self.timer_callback)\nself.i = 0\ndef timer_callback(self):\nmsg = String()\nmsg.data = f'Hello from publisher: {self.i}'\nself.publisher_.publish(msg)\nself.get_logger().info(f'Publishing: \"{msg.data}\"')\nself.i += 1\ndef main(args=None):\nrclpy.init(args=args)\npublisher = SimplePublisher()\nrclpy.spin(publisher) # Keep the node alive\npublisher.destroy_node()\nrclpy.shutdown()\nif __name__ == '__main__':\nmain()\nNext, a subscriber node that listens to the chatter\ntopic.\n# 'subscriber_node.py'\nimport rclpy\nfrom rclpy.node import Node\nfrom std_msgs.msg import String\nclass SimpleSubscriber(Node):\ndef __init__(self):\nsuper().__init__('simple_subscriber')\n# Create a subscription to the 'chatter' topic\nself.subscription = self.create_subscription(\nString,\n'chatter',\nself.listener_callback,\n10)\nself.subscription # prevent unused variable warning\ndef listener_callback(self, msg):\nself.get_logger().info(f'I heard: \"{msg.data}\"')\ndef main(args=None):\nrclpy.init(args=args)\nsubscriber = SimpleSubscriber()\nrclpy.spin(subscriber)\nsubscriber.destroy_node()\nrclpy.shutdown()\nif __name__ == '__main__':\nmain()\nTo run this example:\n- Open two terminals with your ROS 2 environment sourced.\n- In the first terminal, run\npython publisher_node.py\n. - In the second terminal, run\npython subscriber_node.py\n. You will see the messages from the publisher being received.\n3. ROS 2 Services (Request/Response)\nServices are for two-way, synchronous communication. A Service Server node provides a service, and a Service Client node can send a request and wait for a response. This is a tightly coupled, blocking communication model.\n- Use Case: Triggering a specific action that has a clear start and end, like \"open gripper\" or \"calculate inverse kinematics\".\nDiagram: Service Client/Server Model\ngraph TD\nA[Service Client Node] -- Sends Request --> B(Service: /add_two_ints);\nB -- Delivers Request --> C[Service Server Node];\nC -- Processes and sends Response --> B;\nB -- Delivers Response --> A;\nExample: A Simple Service Server and Client\nFirst, the server node that offers a service to add two integers. ROS 2 has a standard service definition for this, example_interfaces/srv/AddTwoInts\n.\n# 'service_server_node.py'\nimport rclpy\nfrom rclpy.node import Node\nfrom example_interfaces.srv import AddTwoInts\nclass SimpleServiceServer(Node):\ndef __init__(self):\nsuper().__init__('simple_service_server')\n# Create a service named 'add_two_ints'\nself.srv = self.create_service(AddTwoInts, 'add_two_ints', self.add_two_ints_callback)\ndef add_two_ints_callback(self, request, response):\nresponse.sum = request.a + request.b\nself.get_logger().info(f'Incoming request: a={request.a}, b={request.b}. Returning sum={response.sum}')\nreturn response\ndef main(args=None):\nrclpy.init(args=args)\nserver = SimpleServiceServer()\nrclpy.spin(server)\nserver.destroy_node()\nrclpy.shutdown()\nif __name__ == '__main__':\nmain()\nNext, the client node that calls the service.\n# 'service_client_node.py'\nimport rclpy\nfrom rclpy.node import Node\nfrom example_interfaces.srv import AddTwoInts\nimport sys\nclass SimpleServiceClient(Node):\ndef __init__(self):\nsuper().__init__('simple_service_client')\n# Create a client for the 'add_two_ints' service\nself.client = self.create_client(AddTwoInts, 'add_two_ints')\nwhile not self.client.wait_for_service(timeout_sec=1.0):\nself.get_logger().info('Service not available, waiting again...')\nself.req = AddTwoInts.Request()\ndef send_request(self, a, b):\nself.req.a = a\nself.req.b = b\nself.future = self.client.call_async(self.req)\nrclpy.spin_until_future_complete(self, self.future)\nreturn self.future.result()\ndef main(args=None):\nrclpy.init(args=args)\nif len(sys.argv) != 3:\nprint(\"Usage: python service_client_node.py <int1> <int2>\")\nreturn\nclient = SimpleServiceClient()\nresponse = client.send_request(int(sys.argv[1]), int(sys.argv[2]))\nif response:\nclient.get_logger().info(\nf'Result of add_two_ints: for {sys.argv[1]} + {sys.argv[2]} = {response.sum}')\nelse:\nclient.get_logger().error('Service call failed')\nclient.destroy_node()\nrclpy.shutdown()\nif __name__ == '__main__':\nmain()\nTo run this example:\n- Open two terminals with your ROS 2 environment sourced.\n- In the first, run\npython service_server_node.py\n. - In the second, run\npython service_client_node.py 5 10\n. The client will send the request and print the response.\n--------------------------------------------------------------------------------"
  },
  {
    "url": "https://Muham251.github.io/ai-hackathon-1/docs/module-1/nodes-topics-services-rclpy#diagram-service-clientserver-model",
    "text": "ROS 2 Fundamentals & Python\nThis chapter covers the three fundamental communication concepts in ROS 2: Nodes, Topics, and Services. We'll also see how to implement them using rclpy\n, the official Python client library for ROS 2.\n1. ROS 2 Nodes\nA Node is the smallest, most fundamental unit of a ROS 2 system. Think of a node as a single, independent process in your robot's software architecture. Each node should have a single, well-defined purpose.\nFor a humanoid robot, you might have nodes for:\n- Reading sensor data from an IMU.\n- Controlling the motors in the right leg.\n- Processing camera images.\n- Planning footsteps for walking.\nNodes are what allow ROS 2 systems to be modular and fault-tolerant. If one node crashes, the rest of the system can continue to run.\nExample: A Simple rclpy\nNode\nHere is the \"hello world\" of rclpy\n: a simple node that initializes itself, prints a message, and then shuts down.\n# 'hello_node.py'\nimport rclpy\nfrom rclpy.node import Node\ndef main(args=None):\n# 1. Initialize the rclpy library\nrclpy.init(args=args)\n# 2. Create a Node\n# The node is named 'hello_ros_node'\nnode = Node('hello_ros_node')\n# 3. From this point, the node is running.\n# You can add your logic here.\nnode.get_logger().info('Hello, ROS 2 World!')\n# 4. The node is destroyed when the 'with' statement exits.\n# This is important for cleanup.\nnode.destroy_node()\n# 5. Shutdown the rclpy library\nrclpy.shutdown()\nif __name__ == '__main__':\nmain()\nTo run this code:\n- Save it as\nhello_node.py\n. - Make sure you have a ROS 2 environment sourced.\n- Run\npython hello_node.py\n.\n2. ROS 2 Topics (Publish/Subscribe)\nTopics are the primary mechanism for one-way, asynchronous communication in ROS 2. Nodes can publish messages to a topic, and other nodes can subscribe to that topic to receive those messages. This is a decoupled communication model; publishers and subscribers don't need to know about each other.\n- Use Case: Continuously streaming data, like sensor readings or motor commands.\nDiagram: Publisher-Subscriber Model\ngraph TD\nA[Publisher Node] -- Publishes message --> B(Topic: /sensor_data);\nB -- Message is delivered --> C[Subscriber Node 1];\nB -- Message is delivered --> D[Subscriber Node 2];\nExample: A Simple Publisher and Subscriber\nFirst, we define a publisher node that sends a simple string message every second.\n# 'publisher_node.py'\nimport rclpy\nfrom rclpy.node import Node\nfrom std_msgs.msg import String\nclass SimplePublisher(Node):\ndef __init__(self):\nsuper().__init__('simple_publisher')\n# Create a publisher on the 'chatter' topic\nself.publisher_ = self.create_publisher(String, 'chatter', 10)\nself.timer = self.create_timer(1.0, self.timer_callback)\nself.i = 0\ndef timer_callback(self):\nmsg = String()\nmsg.data = f'Hello from publisher: {self.i}'\nself.publisher_.publish(msg)\nself.get_logger().info(f'Publishing: \"{msg.data}\"')\nself.i += 1\ndef main(args=None):\nrclpy.init(args=args)\npublisher = SimplePublisher()\nrclpy.spin(publisher) # Keep the node alive\npublisher.destroy_node()\nrclpy.shutdown()\nif __name__ == '__main__':\nmain()\nNext, a subscriber node that listens to the chatter\ntopic.\n# 'subscriber_node.py'\nimport rclpy\nfrom rclpy.node import Node\nfrom std_msgs.msg import String\nclass SimpleSubscriber(Node):\ndef __init__(self):\nsuper().__init__('simple_subscriber')\n# Create a subscription to the 'chatter' topic\nself.subscription = self.create_subscription(\nString,\n'chatter',\nself.listener_callback,\n10)\nself.subscription # prevent unused variable warning\ndef listener_callback(self, msg):\nself.get_logger().info(f'I heard: \"{msg.data}\"')\ndef main(args=None):\nrclpy.init(args=args)\nsubscriber = SimpleSubscriber()\nrclpy.spin(subscriber)\nsubscriber.destroy_node()\nrclpy.shutdown()\nif __name__ == '__main__':\nmain()\nTo run this example:\n- Open two terminals with your ROS 2 environment sourced.\n- In the first terminal, run\npython publisher_node.py\n. - In the second terminal, run\npython subscriber_node.py\n. You will see the messages from the publisher being received.\n3. ROS 2 Services (Request/Response)\nServices are for two-way, synchronous communication. A Service Server node provides a service, and a Service Client node can send a request and wait for a response. This is a tightly coupled, blocking communication model.\n- Use Case: Triggering a specific action that has a clear start and end, like \"open gripper\" or \"calculate inverse kinematics\".\nDiagram: Service Client/Server Model\ngraph TD\nA[Service Client Node] -- Sends Request --> B(Service: /add_two_ints);\nB -- Delivers Request --> C[Service Server Node];\nC -- Processes and sends Response --> B;\nB -- Delivers Response --> A;\nExample: A Simple Service Server and Client\nFirst, the server node that offers a service to add two integers. ROS 2 has a standard service definition for this, example_interfaces/srv/AddTwoInts\n.\n# 'service_server_node.py'\nimport rclpy\nfrom rclpy.node import Node\nfrom example_interfaces.srv import AddTwoInts\nclass SimpleServiceServer(Node):\ndef __init__(self):\nsuper().__init__('simple_service_server')\n# Create a service named 'add_two_ints'\nself.srv = self.create_service(AddTwoInts, 'add_two_ints', self.add_two_ints_callback)\ndef add_two_ints_callback(self, request, response):\nresponse.sum = request.a + request.b\nself.get_logger().info(f'Incoming request: a={request.a}, b={request.b}. Returning sum={response.sum}')\nreturn response\ndef main(args=None):\nrclpy.init(args=args)\nserver = SimpleServiceServer()\nrclpy.spin(server)\nserver.destroy_node()\nrclpy.shutdown()\nif __name__ == '__main__':\nmain()\nNext, the client node that calls the service.\n# 'service_client_node.py'\nimport rclpy\nfrom rclpy.node import Node\nfrom example_interfaces.srv import AddTwoInts\nimport sys\nclass SimpleServiceClient(Node):\ndef __init__(self):\nsuper().__init__('simple_service_client')\n# Create a client for the 'add_two_ints' service\nself.client = self.create_client(AddTwoInts, 'add_two_ints')\nwhile not self.client.wait_for_service(timeout_sec=1.0):\nself.get_logger().info('Service not available, waiting again...')\nself.req = AddTwoInts.Request()\ndef send_request(self, a, b):\nself.req.a = a\nself.req.b = b\nself.future = self.client.call_async(self.req)\nrclpy.spin_until_future_complete(self, self.future)\nreturn self.future.result()\ndef main(args=None):\nrclpy.init(args=args)\nif len(sys.argv) != 3:\nprint(\"Usage: python service_client_node.py <int1> <int2>\")\nreturn\nclient = SimpleServiceClient()\nresponse = client.send_request(int(sys.argv[1]), int(sys.argv[2]))\nif response:\nclient.get_logger().info(\nf'Result of add_two_ints: for {sys.argv[1]} + {sys.argv[2]} = {response.sum}')\nelse:\nclient.get_logger().error('Service call failed')\nclient.destroy_node()\nrclpy.shutdown()\nif __name__ == '__main__':\nmain()\nTo run this example:\n- Open two terminals with your ROS 2 environment sourced.\n- In the first, run\npython service_server_node.py\n. - In the second, run\npython service_client_node.py 5 10\n. The client will send the request and print the response.\n--------------------------------------------------------------------------------"
  },
  {
    "url": "https://Muham251.github.io/ai-hackathon-1/docs/module-1/nodes-topics-services-rclpy#example-a-simple-service-server-and-client",
    "text": "ROS 2 Fundamentals & Python\nThis chapter covers the three fundamental communication concepts in ROS 2: Nodes, Topics, and Services. We'll also see how to implement them using rclpy\n, the official Python client library for ROS 2.\n1. ROS 2 Nodes\nA Node is the smallest, most fundamental unit of a ROS 2 system. Think of a node as a single, independent process in your robot's software architecture. Each node should have a single, well-defined purpose.\nFor a humanoid robot, you might have nodes for:\n- Reading sensor data from an IMU.\n- Controlling the motors in the right leg.\n- Processing camera images.\n- Planning footsteps for walking.\nNodes are what allow ROS 2 systems to be modular and fault-tolerant. If one node crashes, the rest of the system can continue to run.\nExample: A Simple rclpy\nNode\nHere is the \"hello world\" of rclpy\n: a simple node that initializes itself, prints a message, and then shuts down.\n# 'hello_node.py'\nimport rclpy\nfrom rclpy.node import Node\ndef main(args=None):\n# 1. Initialize the rclpy library\nrclpy.init(args=args)\n# 2. Create a Node\n# The node is named 'hello_ros_node'\nnode = Node('hello_ros_node')\n# 3. From this point, the node is running.\n# You can add your logic here.\nnode.get_logger().info('Hello, ROS 2 World!')\n# 4. The node is destroyed when the 'with' statement exits.\n# This is important for cleanup.\nnode.destroy_node()\n# 5. Shutdown the rclpy library\nrclpy.shutdown()\nif __name__ == '__main__':\nmain()\nTo run this code:\n- Save it as\nhello_node.py\n. - Make sure you have a ROS 2 environment sourced.\n- Run\npython hello_node.py\n.\n2. ROS 2 Topics (Publish/Subscribe)\nTopics are the primary mechanism for one-way, asynchronous communication in ROS 2. Nodes can publish messages to a topic, and other nodes can subscribe to that topic to receive those messages. This is a decoupled communication model; publishers and subscribers don't need to know about each other.\n- Use Case: Continuously streaming data, like sensor readings or motor commands.\nDiagram: Publisher-Subscriber Model\ngraph TD\nA[Publisher Node] -- Publishes message --> B(Topic: /sensor_data);\nB -- Message is delivered --> C[Subscriber Node 1];\nB -- Message is delivered --> D[Subscriber Node 2];\nExample: A Simple Publisher and Subscriber\nFirst, we define a publisher node that sends a simple string message every second.\n# 'publisher_node.py'\nimport rclpy\nfrom rclpy.node import Node\nfrom std_msgs.msg import String\nclass SimplePublisher(Node):\ndef __init__(self):\nsuper().__init__('simple_publisher')\n# Create a publisher on the 'chatter' topic\nself.publisher_ = self.create_publisher(String, 'chatter', 10)\nself.timer = self.create_timer(1.0, self.timer_callback)\nself.i = 0\ndef timer_callback(self):\nmsg = String()\nmsg.data = f'Hello from publisher: {self.i}'\nself.publisher_.publish(msg)\nself.get_logger().info(f'Publishing: \"{msg.data}\"')\nself.i += 1\ndef main(args=None):\nrclpy.init(args=args)\npublisher = SimplePublisher()\nrclpy.spin(publisher) # Keep the node alive\npublisher.destroy_node()\nrclpy.shutdown()\nif __name__ == '__main__':\nmain()\nNext, a subscriber node that listens to the chatter\ntopic.\n# 'subscriber_node.py'\nimport rclpy\nfrom rclpy.node import Node\nfrom std_msgs.msg import String\nclass SimpleSubscriber(Node):\ndef __init__(self):\nsuper().__init__('simple_subscriber')\n# Create a subscription to the 'chatter' topic\nself.subscription = self.create_subscription(\nString,\n'chatter',\nself.listener_callback,\n10)\nself.subscription # prevent unused variable warning\ndef listener_callback(self, msg):\nself.get_logger().info(f'I heard: \"{msg.data}\"')\ndef main(args=None):\nrclpy.init(args=args)\nsubscriber = SimpleSubscriber()\nrclpy.spin(subscriber)\nsubscriber.destroy_node()\nrclpy.shutdown()\nif __name__ == '__main__':\nmain()\nTo run this example:\n- Open two terminals with your ROS 2 environment sourced.\n- In the first terminal, run\npython publisher_node.py\n. - In the second terminal, run\npython subscriber_node.py\n. You will see the messages from the publisher being received.\n3. ROS 2 Services (Request/Response)\nServices are for two-way, synchronous communication. A Service Server node provides a service, and a Service Client node can send a request and wait for a response. This is a tightly coupled, blocking communication model.\n- Use Case: Triggering a specific action that has a clear start and end, like \"open gripper\" or \"calculate inverse kinematics\".\nDiagram: Service Client/Server Model\ngraph TD\nA[Service Client Node] -- Sends Request --> B(Service: /add_two_ints);\nB -- Delivers Request --> C[Service Server Node];\nC -- Processes and sends Response --> B;\nB -- Delivers Response --> A;\nExample: A Simple Service Server and Client\nFirst, the server node that offers a service to add two integers. ROS 2 has a standard service definition for this, example_interfaces/srv/AddTwoInts\n.\n# 'service_server_node.py'\nimport rclpy\nfrom rclpy.node import Node\nfrom example_interfaces.srv import AddTwoInts\nclass SimpleServiceServer(Node):\ndef __init__(self):\nsuper().__init__('simple_service_server')\n# Create a service named 'add_two_ints'\nself.srv = self.create_service(AddTwoInts, 'add_two_ints', self.add_two_ints_callback)\ndef add_two_ints_callback(self, request, response):\nresponse.sum = request.a + request.b\nself.get_logger().info(f'Incoming request: a={request.a}, b={request.b}. Returning sum={response.sum}')\nreturn response\ndef main(args=None):\nrclpy.init(args=args)\nserver = SimpleServiceServer()\nrclpy.spin(server)\nserver.destroy_node()\nrclpy.shutdown()\nif __name__ == '__main__':\nmain()\nNext, the client node that calls the service.\n# 'service_client_node.py'\nimport rclpy\nfrom rclpy.node import Node\nfrom example_interfaces.srv import AddTwoInts\nimport sys\nclass SimpleServiceClient(Node):\ndef __init__(self):\nsuper().__init__('simple_service_client')\n# Create a client for the 'add_two_ints' service\nself.client = self.create_client(AddTwoInts, 'add_two_ints')\nwhile not self.client.wait_for_service(timeout_sec=1.0):\nself.get_logger().info('Service not available, waiting again...')\nself.req = AddTwoInts.Request()\ndef send_request(self, a, b):\nself.req.a = a\nself.req.b = b\nself.future = self.client.call_async(self.req)\nrclpy.spin_until_future_complete(self, self.future)\nreturn self.future.result()\ndef main(args=None):\nrclpy.init(args=args)\nif len(sys.argv) != 3:\nprint(\"Usage: python service_client_node.py <int1> <int2>\")\nreturn\nclient = SimpleServiceClient()\nresponse = client.send_request(int(sys.argv[1]), int(sys.argv[2]))\nif response:\nclient.get_logger().info(\nf'Result of add_two_ints: for {sys.argv[1]} + {sys.argv[2]} = {response.sum}')\nelse:\nclient.get_logger().error('Service call failed')\nclient.destroy_node()\nrclpy.shutdown()\nif __name__ == '__main__':\nmain()\nTo run this example:\n- Open two terminals with your ROS 2 environment sourced.\n- In the first, run\npython service_server_node.py\n. - In the second, run\npython service_client_node.py 5 10\n. The client will send the request and print the response.\n--------------------------------------------------------------------------------"
  },
  {
    "url": "https://Muham251.github.io/ai-hackathon-1/docs/module-1/#user-scenarios--testing-mandatory",
    "text": "Feature Specification: Module 1 \u2014 The Robotic Nervous System (ROS 2)\nFeature Branch: module-1\nCreated: December 7, 2025\nStatus: Draft\nInput: User description: \"Module: 1 \u2014 The Robotic Nervous System (ROS 2)\"\nUser Scenarios & Testing (mandatory)\nUser Story 1 - ROS 2 Fundamentals (Priority: P1)\nAs a beginner robotics student, I want to understand ROS 2 nodes and topics so I can run basic robot programs.\nWhy this priority: This is the foundational knowledge required to work with any ROS 2 system, and without understanding these core concepts, the learner cannot progress to more advanced topics.\nIndependent Test: Can be fully tested by successfully running a basic publisher/subscriber example and explaining the communication patterns.\nAcceptance Scenarios:\n- Given a basic understanding of programming concepts, When the user reads the ROS 2 fundamentals chapter, Then they can explain the roles of nodes and topics\n- Given a working ROS 2 environment, When the user runs the provided publisher/subscriber example, Then they can observe the message passing between nodes\nUser Story 2 - Python Integration with rclpy (Priority: P2)\nAs an AI developer, I want to use rclpy to connect Python agents to robot controllers.\nWhy this priority: Python is the primary language for AI development, so connecting AI agents to ROS 2 systems is critical for the target audience.\nIndependent Test: Can be fully tested by successfully running a Python script that communicates with ROS 2 and demonstrates control concepts.\nAcceptance Scenarios:\n- Given a ROS 2 environment with rclpy installed, When the user runs the Python examples, Then they can see the interaction between Python code and robot controllers\nUser Story 3 - URDF for Humanoid Robots (Priority: P3)\nAs a humanoid robotics learner, I want to understand URDF so I can model a robot body.\nWhy this priority: Understanding robot modeling is essential for working with humanoid robots, which is the main focus of this curriculum.\nIndependent Test: Can be fully tested by successfully loading and modifying a basic humanoid URDF model.\nAcceptance Scenarios:\n- Given a basic URDF file, When the user modifies it following the tutorial, Then they can create a valid robot body model\nEdge Cases\n- ROS 2 distribution mismatch\n- Missing dependencies during setup\n- URDF syntax errors causing launch failures\n- Python environment misconfiguration\nRequirements (mandatory)\nFunctional Requirements\n- FR-001: The module MUST explain ROS 2 nodes, topics, and services accurately.\n- FR-002: The module MUST include executable rclpy examples.\n- FR-003: The module MUST include a valid humanoid URDF example.\n- FR-004: The module MUST avoid ROS 1 concepts.\nKey Entities\n- ROS 2 Nodes\n- Topics and Services\n- rclpy Python scripts\n- URDF files (links, joints, hierarchy)\n- Code blocks and CLI commands\n- Diagrams (conceptual or ASCII)\nSuccess Criteria (mandatory)\nMeasurable Outcomes\n- SC-001: Users can successfully run a ROS 2 publisher/subscriber example.\n- SC-002: Users can modify and execute a Python rclpy node.\n- SC-003: Users can explain the role of nodes and topics.\n- SC-004: Users can understand and edit a basic humanoid URDF model.\nOut of Scope\n- ROS 1\n- Simulation tools (Gazebo / Isaac)\n- Hardware deployment\nContent Structure\n- Chapter 1: ROS 2 Fundamentals for Humanoid Robots\n- Chapter 2: Nodes, Topics, Services & rclpy Integration\n- Chapter 3: URDF Basics for Humanoids\n--------------------------------------------------------------------------------"
  },
  {
    "url": "https://Muham251.github.io/ai-hackathon-1/docs/module-1/#user-story-1---ros-2-fundamentals-priority-p1",
    "text": "Feature Specification: Module 1 \u2014 The Robotic Nervous System (ROS 2)\nFeature Branch: module-1\nCreated: December 7, 2025\nStatus: Draft\nInput: User description: \"Module: 1 \u2014 The Robotic Nervous System (ROS 2)\"\nUser Scenarios & Testing (mandatory)\nUser Story 1 - ROS 2 Fundamentals (Priority: P1)\nAs a beginner robotics student, I want to understand ROS 2 nodes and topics so I can run basic robot programs.\nWhy this priority: This is the foundational knowledge required to work with any ROS 2 system, and without understanding these core concepts, the learner cannot progress to more advanced topics.\nIndependent Test: Can be fully tested by successfully running a basic publisher/subscriber example and explaining the communication patterns.\nAcceptance Scenarios:\n- Given a basic understanding of programming concepts, When the user reads the ROS 2 fundamentals chapter, Then they can explain the roles of nodes and topics\n- Given a working ROS 2 environment, When the user runs the provided publisher/subscriber example, Then they can observe the message passing between nodes\nUser Story 2 - Python Integration with rclpy (Priority: P2)\nAs an AI developer, I want to use rclpy to connect Python agents to robot controllers.\nWhy this priority: Python is the primary language for AI development, so connecting AI agents to ROS 2 systems is critical for the target audience.\nIndependent Test: Can be fully tested by successfully running a Python script that communicates with ROS 2 and demonstrates control concepts.\nAcceptance Scenarios:\n- Given a ROS 2 environment with rclpy installed, When the user runs the Python examples, Then they can see the interaction between Python code and robot controllers\nUser Story 3 - URDF for Humanoid Robots (Priority: P3)\nAs a humanoid robotics learner, I want to understand URDF so I can model a robot body.\nWhy this priority: Understanding robot modeling is essential for working with humanoid robots, which is the main focus of this curriculum.\nIndependent Test: Can be fully tested by successfully loading and modifying a basic humanoid URDF model.\nAcceptance Scenarios:\n- Given a basic URDF file, When the user modifies it following the tutorial, Then they can create a valid robot body model\nEdge Cases\n- ROS 2 distribution mismatch\n- Missing dependencies during setup\n- URDF syntax errors causing launch failures\n- Python environment misconfiguration\nRequirements (mandatory)\nFunctional Requirements\n- FR-001: The module MUST explain ROS 2 nodes, topics, and services accurately.\n- FR-002: The module MUST include executable rclpy examples.\n- FR-003: The module MUST include a valid humanoid URDF example.\n- FR-004: The module MUST avoid ROS 1 concepts.\nKey Entities\n- ROS 2 Nodes\n- Topics and Services\n- rclpy Python scripts\n- URDF files (links, joints, hierarchy)\n- Code blocks and CLI commands\n- Diagrams (conceptual or ASCII)\nSuccess Criteria (mandatory)\nMeasurable Outcomes\n- SC-001: Users can successfully run a ROS 2 publisher/subscriber example.\n- SC-002: Users can modify and execute a Python rclpy node.\n- SC-003: Users can explain the role of nodes and topics.\n- SC-004: Users can understand and edit a basic humanoid URDF model.\nOut of Scope\n- ROS 1\n- Simulation tools (Gazebo / Isaac)\n- Hardware deployment\nContent Structure\n- Chapter 1: ROS 2 Fundamentals for Humanoid Robots\n- Chapter 2: Nodes, Topics, Services & rclpy Integration\n- Chapter 3: URDF Basics for Humanoids\n--------------------------------------------------------------------------------"
  },
  {
    "url": "https://Muham251.github.io/ai-hackathon-1/docs/module-1/#user-story-2---python-integration-with-rclpy-priority-p2",
    "text": "Feature Specification: Module 1 \u2014 The Robotic Nervous System (ROS 2)\nFeature Branch: module-1\nCreated: December 7, 2025\nStatus: Draft\nInput: User description: \"Module: 1 \u2014 The Robotic Nervous System (ROS 2)\"\nUser Scenarios & Testing (mandatory)\nUser Story 1 - ROS 2 Fundamentals (Priority: P1)\nAs a beginner robotics student, I want to understand ROS 2 nodes and topics so I can run basic robot programs.\nWhy this priority: This is the foundational knowledge required to work with any ROS 2 system, and without understanding these core concepts, the learner cannot progress to more advanced topics.\nIndependent Test: Can be fully tested by successfully running a basic publisher/subscriber example and explaining the communication patterns.\nAcceptance Scenarios:\n- Given a basic understanding of programming concepts, When the user reads the ROS 2 fundamentals chapter, Then they can explain the roles of nodes and topics\n- Given a working ROS 2 environment, When the user runs the provided publisher/subscriber example, Then they can observe the message passing between nodes\nUser Story 2 - Python Integration with rclpy (Priority: P2)\nAs an AI developer, I want to use rclpy to connect Python agents to robot controllers.\nWhy this priority: Python is the primary language for AI development, so connecting AI agents to ROS 2 systems is critical for the target audience.\nIndependent Test: Can be fully tested by successfully running a Python script that communicates with ROS 2 and demonstrates control concepts.\nAcceptance Scenarios:\n- Given a ROS 2 environment with rclpy installed, When the user runs the Python examples, Then they can see the interaction between Python code and robot controllers\nUser Story 3 - URDF for Humanoid Robots (Priority: P3)\nAs a humanoid robotics learner, I want to understand URDF so I can model a robot body.\nWhy this priority: Understanding robot modeling is essential for working with humanoid robots, which is the main focus of this curriculum.\nIndependent Test: Can be fully tested by successfully loading and modifying a basic humanoid URDF model.\nAcceptance Scenarios:\n- Given a basic URDF file, When the user modifies it following the tutorial, Then they can create a valid robot body model\nEdge Cases\n- ROS 2 distribution mismatch\n- Missing dependencies during setup\n- URDF syntax errors causing launch failures\n- Python environment misconfiguration\nRequirements (mandatory)\nFunctional Requirements\n- FR-001: The module MUST explain ROS 2 nodes, topics, and services accurately.\n- FR-002: The module MUST include executable rclpy examples.\n- FR-003: The module MUST include a valid humanoid URDF example.\n- FR-004: The module MUST avoid ROS 1 concepts.\nKey Entities\n- ROS 2 Nodes\n- Topics and Services\n- rclpy Python scripts\n- URDF files (links, joints, hierarchy)\n- Code blocks and CLI commands\n- Diagrams (conceptual or ASCII)\nSuccess Criteria (mandatory)\nMeasurable Outcomes\n- SC-001: Users can successfully run a ROS 2 publisher/subscriber example.\n- SC-002: Users can modify and execute a Python rclpy node.\n- SC-003: Users can explain the role of nodes and topics.\n- SC-004: Users can understand and edit a basic humanoid URDF model.\nOut of Scope\n- ROS 1\n- Simulation tools (Gazebo / Isaac)\n- Hardware deployment\nContent Structure\n- Chapter 1: ROS 2 Fundamentals for Humanoid Robots\n- Chapter 2: Nodes, Topics, Services & rclpy Integration\n- Chapter 3: URDF Basics for Humanoids\n--------------------------------------------------------------------------------"
  },
  {
    "url": "https://Muham251.github.io/ai-hackathon-1/docs/module-1/#user-story-3---urdf-for-humanoid-robots-priority-p3",
    "text": "Feature Specification: Module 1 \u2014 The Robotic Nervous System (ROS 2)\nFeature Branch: module-1\nCreated: December 7, 2025\nStatus: Draft\nInput: User description: \"Module: 1 \u2014 The Robotic Nervous System (ROS 2)\"\nUser Scenarios & Testing (mandatory)\nUser Story 1 - ROS 2 Fundamentals (Priority: P1)\nAs a beginner robotics student, I want to understand ROS 2 nodes and topics so I can run basic robot programs.\nWhy this priority: This is the foundational knowledge required to work with any ROS 2 system, and without understanding these core concepts, the learner cannot progress to more advanced topics.\nIndependent Test: Can be fully tested by successfully running a basic publisher/subscriber example and explaining the communication patterns.\nAcceptance Scenarios:\n- Given a basic understanding of programming concepts, When the user reads the ROS 2 fundamentals chapter, Then they can explain the roles of nodes and topics\n- Given a working ROS 2 environment, When the user runs the provided publisher/subscriber example, Then they can observe the message passing between nodes\nUser Story 2 - Python Integration with rclpy (Priority: P2)\nAs an AI developer, I want to use rclpy to connect Python agents to robot controllers.\nWhy this priority: Python is the primary language for AI development, so connecting AI agents to ROS 2 systems is critical for the target audience.\nIndependent Test: Can be fully tested by successfully running a Python script that communicates with ROS 2 and demonstrates control concepts.\nAcceptance Scenarios:\n- Given a ROS 2 environment with rclpy installed, When the user runs the Python examples, Then they can see the interaction between Python code and robot controllers\nUser Story 3 - URDF for Humanoid Robots (Priority: P3)\nAs a humanoid robotics learner, I want to understand URDF so I can model a robot body.\nWhy this priority: Understanding robot modeling is essential for working with humanoid robots, which is the main focus of this curriculum.\nIndependent Test: Can be fully tested by successfully loading and modifying a basic humanoid URDF model.\nAcceptance Scenarios:\n- Given a basic URDF file, When the user modifies it following the tutorial, Then they can create a valid robot body model\nEdge Cases\n- ROS 2 distribution mismatch\n- Missing dependencies during setup\n- URDF syntax errors causing launch failures\n- Python environment misconfiguration\nRequirements (mandatory)\nFunctional Requirements\n- FR-001: The module MUST explain ROS 2 nodes, topics, and services accurately.\n- FR-002: The module MUST include executable rclpy examples.\n- FR-003: The module MUST include a valid humanoid URDF example.\n- FR-004: The module MUST avoid ROS 1 concepts.\nKey Entities\n- ROS 2 Nodes\n- Topics and Services\n- rclpy Python scripts\n- URDF files (links, joints, hierarchy)\n- Code blocks and CLI commands\n- Diagrams (conceptual or ASCII)\nSuccess Criteria (mandatory)\nMeasurable Outcomes\n- SC-001: Users can successfully run a ROS 2 publisher/subscriber example.\n- SC-002: Users can modify and execute a Python rclpy node.\n- SC-003: Users can explain the role of nodes and topics.\n- SC-004: Users can understand and edit a basic humanoid URDF model.\nOut of Scope\n- ROS 1\n- Simulation tools (Gazebo / Isaac)\n- Hardware deployment\nContent Structure\n- Chapter 1: ROS 2 Fundamentals for Humanoid Robots\n- Chapter 2: Nodes, Topics, Services & rclpy Integration\n- Chapter 3: URDF Basics for Humanoids\n--------------------------------------------------------------------------------"
  },
  {
    "url": "https://Muham251.github.io/ai-hackathon-1/docs/module-1/#edge-cases",
    "text": "Feature Specification: Module 1 \u2014 The Robotic Nervous System (ROS 2)\nFeature Branch: module-1\nCreated: December 7, 2025\nStatus: Draft\nInput: User description: \"Module: 1 \u2014 The Robotic Nervous System (ROS 2)\"\nUser Scenarios & Testing (mandatory)\nUser Story 1 - ROS 2 Fundamentals (Priority: P1)\nAs a beginner robotics student, I want to understand ROS 2 nodes and topics so I can run basic robot programs.\nWhy this priority: This is the foundational knowledge required to work with any ROS 2 system, and without understanding these core concepts, the learner cannot progress to more advanced topics.\nIndependent Test: Can be fully tested by successfully running a basic publisher/subscriber example and explaining the communication patterns.\nAcceptance Scenarios:\n- Given a basic understanding of programming concepts, When the user reads the ROS 2 fundamentals chapter, Then they can explain the roles of nodes and topics\n- Given a working ROS 2 environment, When the user runs the provided publisher/subscriber example, Then they can observe the message passing between nodes\nUser Story 2 - Python Integration with rclpy (Priority: P2)\nAs an AI developer, I want to use rclpy to connect Python agents to robot controllers.\nWhy this priority: Python is the primary language for AI development, so connecting AI agents to ROS 2 systems is critical for the target audience.\nIndependent Test: Can be fully tested by successfully running a Python script that communicates with ROS 2 and demonstrates control concepts.\nAcceptance Scenarios:\n- Given a ROS 2 environment with rclpy installed, When the user runs the Python examples, Then they can see the interaction between Python code and robot controllers\nUser Story 3 - URDF for Humanoid Robots (Priority: P3)\nAs a humanoid robotics learner, I want to understand URDF so I can model a robot body.\nWhy this priority: Understanding robot modeling is essential for working with humanoid robots, which is the main focus of this curriculum.\nIndependent Test: Can be fully tested by successfully loading and modifying a basic humanoid URDF model.\nAcceptance Scenarios:\n- Given a basic URDF file, When the user modifies it following the tutorial, Then they can create a valid robot body model\nEdge Cases\n- ROS 2 distribution mismatch\n- Missing dependencies during setup\n- URDF syntax errors causing launch failures\n- Python environment misconfiguration\nRequirements (mandatory)\nFunctional Requirements\n- FR-001: The module MUST explain ROS 2 nodes, topics, and services accurately.\n- FR-002: The module MUST include executable rclpy examples.\n- FR-003: The module MUST include a valid humanoid URDF example.\n- FR-004: The module MUST avoid ROS 1 concepts.\nKey Entities\n- ROS 2 Nodes\n- Topics and Services\n- rclpy Python scripts\n- URDF files (links, joints, hierarchy)\n- Code blocks and CLI commands\n- Diagrams (conceptual or ASCII)\nSuccess Criteria (mandatory)\nMeasurable Outcomes\n- SC-001: Users can successfully run a ROS 2 publisher/subscriber example.\n- SC-002: Users can modify and execute a Python rclpy node.\n- SC-003: Users can explain the role of nodes and topics.\n- SC-004: Users can understand and edit a basic humanoid URDF model.\nOut of Scope\n- ROS 1\n- Simulation tools (Gazebo / Isaac)\n- Hardware deployment\nContent Structure\n- Chapter 1: ROS 2 Fundamentals for Humanoid Robots\n- Chapter 2: Nodes, Topics, Services & rclpy Integration\n- Chapter 3: URDF Basics for Humanoids\n--------------------------------------------------------------------------------"
  },
  {
    "url": "https://Muham251.github.io/ai-hackathon-1/docs/module-1/#requirements-mandatory",
    "text": "Feature Specification: Module 1 \u2014 The Robotic Nervous System (ROS 2)\nFeature Branch: module-1\nCreated: December 7, 2025\nStatus: Draft\nInput: User description: \"Module: 1 \u2014 The Robotic Nervous System (ROS 2)\"\nUser Scenarios & Testing (mandatory)\nUser Story 1 - ROS 2 Fundamentals (Priority: P1)\nAs a beginner robotics student, I want to understand ROS 2 nodes and topics so I can run basic robot programs.\nWhy this priority: This is the foundational knowledge required to work with any ROS 2 system, and without understanding these core concepts, the learner cannot progress to more advanced topics.\nIndependent Test: Can be fully tested by successfully running a basic publisher/subscriber example and explaining the communication patterns.\nAcceptance Scenarios:\n- Given a basic understanding of programming concepts, When the user reads the ROS 2 fundamentals chapter, Then they can explain the roles of nodes and topics\n- Given a working ROS 2 environment, When the user runs the provided publisher/subscriber example, Then they can observe the message passing between nodes\nUser Story 2 - Python Integration with rclpy (Priority: P2)\nAs an AI developer, I want to use rclpy to connect Python agents to robot controllers.\nWhy this priority: Python is the primary language for AI development, so connecting AI agents to ROS 2 systems is critical for the target audience.\nIndependent Test: Can be fully tested by successfully running a Python script that communicates with ROS 2 and demonstrates control concepts.\nAcceptance Scenarios:\n- Given a ROS 2 environment with rclpy installed, When the user runs the Python examples, Then they can see the interaction between Python code and robot controllers\nUser Story 3 - URDF for Humanoid Robots (Priority: P3)\nAs a humanoid robotics learner, I want to understand URDF so I can model a robot body.\nWhy this priority: Understanding robot modeling is essential for working with humanoid robots, which is the main focus of this curriculum.\nIndependent Test: Can be fully tested by successfully loading and modifying a basic humanoid URDF model.\nAcceptance Scenarios:\n- Given a basic URDF file, When the user modifies it following the tutorial, Then they can create a valid robot body model\nEdge Cases\n- ROS 2 distribution mismatch\n- Missing dependencies during setup\n- URDF syntax errors causing launch failures\n- Python environment misconfiguration\nRequirements (mandatory)\nFunctional Requirements\n- FR-001: The module MUST explain ROS 2 nodes, topics, and services accurately.\n- FR-002: The module MUST include executable rclpy examples.\n- FR-003: The module MUST include a valid humanoid URDF example.\n- FR-004: The module MUST avoid ROS 1 concepts.\nKey Entities\n- ROS 2 Nodes\n- Topics and Services\n- rclpy Python scripts\n- URDF files (links, joints, hierarchy)\n- Code blocks and CLI commands\n- Diagrams (conceptual or ASCII)\nSuccess Criteria (mandatory)\nMeasurable Outcomes\n- SC-001: Users can successfully run a ROS 2 publisher/subscriber example.\n- SC-002: Users can modify and execute a Python rclpy node.\n- SC-003: Users can explain the role of nodes and topics.\n- SC-004: Users can understand and edit a basic humanoid URDF model.\nOut of Scope\n- ROS 1\n- Simulation tools (Gazebo / Isaac)\n- Hardware deployment\nContent Structure\n- Chapter 1: ROS 2 Fundamentals for Humanoid Robots\n- Chapter 2: Nodes, Topics, Services & rclpy Integration\n- Chapter 3: URDF Basics for Humanoids\n--------------------------------------------------------------------------------"
  },
  {
    "url": "https://Muham251.github.io/ai-hackathon-1/docs/module-1/#functional-requirements",
    "text": "Feature Specification: Module 1 \u2014 The Robotic Nervous System (ROS 2)\nFeature Branch: module-1\nCreated: December 7, 2025\nStatus: Draft\nInput: User description: \"Module: 1 \u2014 The Robotic Nervous System (ROS 2)\"\nUser Scenarios & Testing (mandatory)\nUser Story 1 - ROS 2 Fundamentals (Priority: P1)\nAs a beginner robotics student, I want to understand ROS 2 nodes and topics so I can run basic robot programs.\nWhy this priority: This is the foundational knowledge required to work with any ROS 2 system, and without understanding these core concepts, the learner cannot progress to more advanced topics.\nIndependent Test: Can be fully tested by successfully running a basic publisher/subscriber example and explaining the communication patterns.\nAcceptance Scenarios:\n- Given a basic understanding of programming concepts, When the user reads the ROS 2 fundamentals chapter, Then they can explain the roles of nodes and topics\n- Given a working ROS 2 environment, When the user runs the provided publisher/subscriber example, Then they can observe the message passing between nodes\nUser Story 2 - Python Integration with rclpy (Priority: P2)\nAs an AI developer, I want to use rclpy to connect Python agents to robot controllers.\nWhy this priority: Python is the primary language for AI development, so connecting AI agents to ROS 2 systems is critical for the target audience.\nIndependent Test: Can be fully tested by successfully running a Python script that communicates with ROS 2 and demonstrates control concepts.\nAcceptance Scenarios:\n- Given a ROS 2 environment with rclpy installed, When the user runs the Python examples, Then they can see the interaction between Python code and robot controllers\nUser Story 3 - URDF for Humanoid Robots (Priority: P3)\nAs a humanoid robotics learner, I want to understand URDF so I can model a robot body.\nWhy this priority: Understanding robot modeling is essential for working with humanoid robots, which is the main focus of this curriculum.\nIndependent Test: Can be fully tested by successfully loading and modifying a basic humanoid URDF model.\nAcceptance Scenarios:\n- Given a basic URDF file, When the user modifies it following the tutorial, Then they can create a valid robot body model\nEdge Cases\n- ROS 2 distribution mismatch\n- Missing dependencies during setup\n- URDF syntax errors causing launch failures\n- Python environment misconfiguration\nRequirements (mandatory)\nFunctional Requirements\n- FR-001: The module MUST explain ROS 2 nodes, topics, and services accurately.\n- FR-002: The module MUST include executable rclpy examples.\n- FR-003: The module MUST include a valid humanoid URDF example.\n- FR-004: The module MUST avoid ROS 1 concepts.\nKey Entities\n- ROS 2 Nodes\n- Topics and Services\n- rclpy Python scripts\n- URDF files (links, joints, hierarchy)\n- Code blocks and CLI commands\n- Diagrams (conceptual or ASCII)\nSuccess Criteria (mandatory)\nMeasurable Outcomes\n- SC-001: Users can successfully run a ROS 2 publisher/subscriber example.\n- SC-002: Users can modify and execute a Python rclpy node.\n- SC-003: Users can explain the role of nodes and topics.\n- SC-004: Users can understand and edit a basic humanoid URDF model.\nOut of Scope\n- ROS 1\n- Simulation tools (Gazebo / Isaac)\n- Hardware deployment\nContent Structure\n- Chapter 1: ROS 2 Fundamentals for Humanoid Robots\n- Chapter 2: Nodes, Topics, Services & rclpy Integration\n- Chapter 3: URDF Basics for Humanoids\n--------------------------------------------------------------------------------"
  },
  {
    "url": "https://Muham251.github.io/ai-hackathon-1/docs/module-1/#key-entities",
    "text": "Feature Specification: Module 1 \u2014 The Robotic Nervous System (ROS 2)\nFeature Branch: module-1\nCreated: December 7, 2025\nStatus: Draft\nInput: User description: \"Module: 1 \u2014 The Robotic Nervous System (ROS 2)\"\nUser Scenarios & Testing (mandatory)\nUser Story 1 - ROS 2 Fundamentals (Priority: P1)\nAs a beginner robotics student, I want to understand ROS 2 nodes and topics so I can run basic robot programs.\nWhy this priority: This is the foundational knowledge required to work with any ROS 2 system, and without understanding these core concepts, the learner cannot progress to more advanced topics.\nIndependent Test: Can be fully tested by successfully running a basic publisher/subscriber example and explaining the communication patterns.\nAcceptance Scenarios:\n- Given a basic understanding of programming concepts, When the user reads the ROS 2 fundamentals chapter, Then they can explain the roles of nodes and topics\n- Given a working ROS 2 environment, When the user runs the provided publisher/subscriber example, Then they can observe the message passing between nodes\nUser Story 2 - Python Integration with rclpy (Priority: P2)\nAs an AI developer, I want to use rclpy to connect Python agents to robot controllers.\nWhy this priority: Python is the primary language for AI development, so connecting AI agents to ROS 2 systems is critical for the target audience.\nIndependent Test: Can be fully tested by successfully running a Python script that communicates with ROS 2 and demonstrates control concepts.\nAcceptance Scenarios:\n- Given a ROS 2 environment with rclpy installed, When the user runs the Python examples, Then they can see the interaction between Python code and robot controllers\nUser Story 3 - URDF for Humanoid Robots (Priority: P3)\nAs a humanoid robotics learner, I want to understand URDF so I can model a robot body.\nWhy this priority: Understanding robot modeling is essential for working with humanoid robots, which is the main focus of this curriculum.\nIndependent Test: Can be fully tested by successfully loading and modifying a basic humanoid URDF model.\nAcceptance Scenarios:\n- Given a basic URDF file, When the user modifies it following the tutorial, Then they can create a valid robot body model\nEdge Cases\n- ROS 2 distribution mismatch\n- Missing dependencies during setup\n- URDF syntax errors causing launch failures\n- Python environment misconfiguration\nRequirements (mandatory)\nFunctional Requirements\n- FR-001: The module MUST explain ROS 2 nodes, topics, and services accurately.\n- FR-002: The module MUST include executable rclpy examples.\n- FR-003: The module MUST include a valid humanoid URDF example.\n- FR-004: The module MUST avoid ROS 1 concepts.\nKey Entities\n- ROS 2 Nodes\n- Topics and Services\n- rclpy Python scripts\n- URDF files (links, joints, hierarchy)\n- Code blocks and CLI commands\n- Diagrams (conceptual or ASCII)\nSuccess Criteria (mandatory)\nMeasurable Outcomes\n- SC-001: Users can successfully run a ROS 2 publisher/subscriber example.\n- SC-002: Users can modify and execute a Python rclpy node.\n- SC-003: Users can explain the role of nodes and topics.\n- SC-004: Users can understand and edit a basic humanoid URDF model.\nOut of Scope\n- ROS 1\n- Simulation tools (Gazebo / Isaac)\n- Hardware deployment\nContent Structure\n- Chapter 1: ROS 2 Fundamentals for Humanoid Robots\n- Chapter 2: Nodes, Topics, Services & rclpy Integration\n- Chapter 3: URDF Basics for Humanoids\n--------------------------------------------------------------------------------"
  },
  {
    "url": "https://Muham251.github.io/ai-hackathon-1/docs/module-1/#success-criteria-mandatory",
    "text": "Feature Specification: Module 1 \u2014 The Robotic Nervous System (ROS 2)\nFeature Branch: module-1\nCreated: December 7, 2025\nStatus: Draft\nInput: User description: \"Module: 1 \u2014 The Robotic Nervous System (ROS 2)\"\nUser Scenarios & Testing (mandatory)\nUser Story 1 - ROS 2 Fundamentals (Priority: P1)\nAs a beginner robotics student, I want to understand ROS 2 nodes and topics so I can run basic robot programs.\nWhy this priority: This is the foundational knowledge required to work with any ROS 2 system, and without understanding these core concepts, the learner cannot progress to more advanced topics.\nIndependent Test: Can be fully tested by successfully running a basic publisher/subscriber example and explaining the communication patterns.\nAcceptance Scenarios:\n- Given a basic understanding of programming concepts, When the user reads the ROS 2 fundamentals chapter, Then they can explain the roles of nodes and topics\n- Given a working ROS 2 environment, When the user runs the provided publisher/subscriber example, Then they can observe the message passing between nodes\nUser Story 2 - Python Integration with rclpy (Priority: P2)\nAs an AI developer, I want to use rclpy to connect Python agents to robot controllers.\nWhy this priority: Python is the primary language for AI development, so connecting AI agents to ROS 2 systems is critical for the target audience.\nIndependent Test: Can be fully tested by successfully running a Python script that communicates with ROS 2 and demonstrates control concepts.\nAcceptance Scenarios:\n- Given a ROS 2 environment with rclpy installed, When the user runs the Python examples, Then they can see the interaction between Python code and robot controllers\nUser Story 3 - URDF for Humanoid Robots (Priority: P3)\nAs a humanoid robotics learner, I want to understand URDF so I can model a robot body.\nWhy this priority: Understanding robot modeling is essential for working with humanoid robots, which is the main focus of this curriculum.\nIndependent Test: Can be fully tested by successfully loading and modifying a basic humanoid URDF model.\nAcceptance Scenarios:\n- Given a basic URDF file, When the user modifies it following the tutorial, Then they can create a valid robot body model\nEdge Cases\n- ROS 2 distribution mismatch\n- Missing dependencies during setup\n- URDF syntax errors causing launch failures\n- Python environment misconfiguration\nRequirements (mandatory)\nFunctional Requirements\n- FR-001: The module MUST explain ROS 2 nodes, topics, and services accurately.\n- FR-002: The module MUST include executable rclpy examples.\n- FR-003: The module MUST include a valid humanoid URDF example.\n- FR-004: The module MUST avoid ROS 1 concepts.\nKey Entities\n- ROS 2 Nodes\n- Topics and Services\n- rclpy Python scripts\n- URDF files (links, joints, hierarchy)\n- Code blocks and CLI commands\n- Diagrams (conceptual or ASCII)\nSuccess Criteria (mandatory)\nMeasurable Outcomes\n- SC-001: Users can successfully run a ROS 2 publisher/subscriber example.\n- SC-002: Users can modify and execute a Python rclpy node.\n- SC-003: Users can explain the role of nodes and topics.\n- SC-004: Users can understand and edit a basic humanoid URDF model.\nOut of Scope\n- ROS 1\n- Simulation tools (Gazebo / Isaac)\n- Hardware deployment\nContent Structure\n- Chapter 1: ROS 2 Fundamentals for Humanoid Robots\n- Chapter 2: Nodes, Topics, Services & rclpy Integration\n- Chapter 3: URDF Basics for Humanoids\n--------------------------------------------------------------------------------"
  },
  {
    "url": "https://Muham251.github.io/ai-hackathon-1/docs/module-1/#measurable-outcomes",
    "text": "Feature Specification: Module 1 \u2014 The Robotic Nervous System (ROS 2)\nFeature Branch: module-1\nCreated: December 7, 2025\nStatus: Draft\nInput: User description: \"Module: 1 \u2014 The Robotic Nervous System (ROS 2)\"\nUser Scenarios & Testing (mandatory)\nUser Story 1 - ROS 2 Fundamentals (Priority: P1)\nAs a beginner robotics student, I want to understand ROS 2 nodes and topics so I can run basic robot programs.\nWhy this priority: This is the foundational knowledge required to work with any ROS 2 system, and without understanding these core concepts, the learner cannot progress to more advanced topics.\nIndependent Test: Can be fully tested by successfully running a basic publisher/subscriber example and explaining the communication patterns.\nAcceptance Scenarios:\n- Given a basic understanding of programming concepts, When the user reads the ROS 2 fundamentals chapter, Then they can explain the roles of nodes and topics\n- Given a working ROS 2 environment, When the user runs the provided publisher/subscriber example, Then they can observe the message passing between nodes\nUser Story 2 - Python Integration with rclpy (Priority: P2)\nAs an AI developer, I want to use rclpy to connect Python agents to robot controllers.\nWhy this priority: Python is the primary language for AI development, so connecting AI agents to ROS 2 systems is critical for the target audience.\nIndependent Test: Can be fully tested by successfully running a Python script that communicates with ROS 2 and demonstrates control concepts.\nAcceptance Scenarios:\n- Given a ROS 2 environment with rclpy installed, When the user runs the Python examples, Then they can see the interaction between Python code and robot controllers\nUser Story 3 - URDF for Humanoid Robots (Priority: P3)\nAs a humanoid robotics learner, I want to understand URDF so I can model a robot body.\nWhy this priority: Understanding robot modeling is essential for working with humanoid robots, which is the main focus of this curriculum.\nIndependent Test: Can be fully tested by successfully loading and modifying a basic humanoid URDF model.\nAcceptance Scenarios:\n- Given a basic URDF file, When the user modifies it following the tutorial, Then they can create a valid robot body model\nEdge Cases\n- ROS 2 distribution mismatch\n- Missing dependencies during setup\n- URDF syntax errors causing launch failures\n- Python environment misconfiguration\nRequirements (mandatory)\nFunctional Requirements\n- FR-001: The module MUST explain ROS 2 nodes, topics, and services accurately.\n- FR-002: The module MUST include executable rclpy examples.\n- FR-003: The module MUST include a valid humanoid URDF example.\n- FR-004: The module MUST avoid ROS 1 concepts.\nKey Entities\n- ROS 2 Nodes\n- Topics and Services\n- rclpy Python scripts\n- URDF files (links, joints, hierarchy)\n- Code blocks and CLI commands\n- Diagrams (conceptual or ASCII)\nSuccess Criteria (mandatory)\nMeasurable Outcomes\n- SC-001: Users can successfully run a ROS 2 publisher/subscriber example.\n- SC-002: Users can modify and execute a Python rclpy node.\n- SC-003: Users can explain the role of nodes and topics.\n- SC-004: Users can understand and edit a basic humanoid URDF model.\nOut of Scope\n- ROS 1\n- Simulation tools (Gazebo / Isaac)\n- Hardware deployment\nContent Structure\n- Chapter 1: ROS 2 Fundamentals for Humanoid Robots\n- Chapter 2: Nodes, Topics, Services & rclpy Integration\n- Chapter 3: URDF Basics for Humanoids\n--------------------------------------------------------------------------------"
  },
  {
    "url": "https://Muham251.github.io/ai-hackathon-1/docs/module-1/#out-of-scope",
    "text": "Feature Specification: Module 1 \u2014 The Robotic Nervous System (ROS 2)\nFeature Branch: module-1\nCreated: December 7, 2025\nStatus: Draft\nInput: User description: \"Module: 1 \u2014 The Robotic Nervous System (ROS 2)\"\nUser Scenarios & Testing (mandatory)\nUser Story 1 - ROS 2 Fundamentals (Priority: P1)\nAs a beginner robotics student, I want to understand ROS 2 nodes and topics so I can run basic robot programs.\nWhy this priority: This is the foundational knowledge required to work with any ROS 2 system, and without understanding these core concepts, the learner cannot progress to more advanced topics.\nIndependent Test: Can be fully tested by successfully running a basic publisher/subscriber example and explaining the communication patterns.\nAcceptance Scenarios:\n- Given a basic understanding of programming concepts, When the user reads the ROS 2 fundamentals chapter, Then they can explain the roles of nodes and topics\n- Given a working ROS 2 environment, When the user runs the provided publisher/subscriber example, Then they can observe the message passing between nodes\nUser Story 2 - Python Integration with rclpy (Priority: P2)\nAs an AI developer, I want to use rclpy to connect Python agents to robot controllers.\nWhy this priority: Python is the primary language for AI development, so connecting AI agents to ROS 2 systems is critical for the target audience.\nIndependent Test: Can be fully tested by successfully running a Python script that communicates with ROS 2 and demonstrates control concepts.\nAcceptance Scenarios:\n- Given a ROS 2 environment with rclpy installed, When the user runs the Python examples, Then they can see the interaction between Python code and robot controllers\nUser Story 3 - URDF for Humanoid Robots (Priority: P3)\nAs a humanoid robotics learner, I want to understand URDF so I can model a robot body.\nWhy this priority: Understanding robot modeling is essential for working with humanoid robots, which is the main focus of this curriculum.\nIndependent Test: Can be fully tested by successfully loading and modifying a basic humanoid URDF model.\nAcceptance Scenarios:\n- Given a basic URDF file, When the user modifies it following the tutorial, Then they can create a valid robot body model\nEdge Cases\n- ROS 2 distribution mismatch\n- Missing dependencies during setup\n- URDF syntax errors causing launch failures\n- Python environment misconfiguration\nRequirements (mandatory)\nFunctional Requirements\n- FR-001: The module MUST explain ROS 2 nodes, topics, and services accurately.\n- FR-002: The module MUST include executable rclpy examples.\n- FR-003: The module MUST include a valid humanoid URDF example.\n- FR-004: The module MUST avoid ROS 1 concepts.\nKey Entities\n- ROS 2 Nodes\n- Topics and Services\n- rclpy Python scripts\n- URDF files (links, joints, hierarchy)\n- Code blocks and CLI commands\n- Diagrams (conceptual or ASCII)\nSuccess Criteria (mandatory)\nMeasurable Outcomes\n- SC-001: Users can successfully run a ROS 2 publisher/subscriber example.\n- SC-002: Users can modify and execute a Python rclpy node.\n- SC-003: Users can explain the role of nodes and topics.\n- SC-004: Users can understand and edit a basic humanoid URDF model.\nOut of Scope\n- ROS 1\n- Simulation tools (Gazebo / Isaac)\n- Hardware deployment\nContent Structure\n- Chapter 1: ROS 2 Fundamentals for Humanoid Robots\n- Chapter 2: Nodes, Topics, Services & rclpy Integration\n- Chapter 3: URDF Basics for Humanoids\n--------------------------------------------------------------------------------"
  },
  {
    "url": "https://Muham251.github.io/ai-hackathon-1/docs/module-1/#content-structure",
    "text": "Feature Specification: Module 1 \u2014 The Robotic Nervous System (ROS 2)\nFeature Branch: module-1\nCreated: December 7, 2025\nStatus: Draft\nInput: User description: \"Module: 1 \u2014 The Robotic Nervous System (ROS 2)\"\nUser Scenarios & Testing (mandatory)\nUser Story 1 - ROS 2 Fundamentals (Priority: P1)\nAs a beginner robotics student, I want to understand ROS 2 nodes and topics so I can run basic robot programs.\nWhy this priority: This is the foundational knowledge required to work with any ROS 2 system, and without understanding these core concepts, the learner cannot progress to more advanced topics.\nIndependent Test: Can be fully tested by successfully running a basic publisher/subscriber example and explaining the communication patterns.\nAcceptance Scenarios:\n- Given a basic understanding of programming concepts, When the user reads the ROS 2 fundamentals chapter, Then they can explain the roles of nodes and topics\n- Given a working ROS 2 environment, When the user runs the provided publisher/subscriber example, Then they can observe the message passing between nodes\nUser Story 2 - Python Integration with rclpy (Priority: P2)\nAs an AI developer, I want to use rclpy to connect Python agents to robot controllers.\nWhy this priority: Python is the primary language for AI development, so connecting AI agents to ROS 2 systems is critical for the target audience.\nIndependent Test: Can be fully tested by successfully running a Python script that communicates with ROS 2 and demonstrates control concepts.\nAcceptance Scenarios:\n- Given a ROS 2 environment with rclpy installed, When the user runs the Python examples, Then they can see the interaction between Python code and robot controllers\nUser Story 3 - URDF for Humanoid Robots (Priority: P3)\nAs a humanoid robotics learner, I want to understand URDF so I can model a robot body.\nWhy this priority: Understanding robot modeling is essential for working with humanoid robots, which is the main focus of this curriculum.\nIndependent Test: Can be fully tested by successfully loading and modifying a basic humanoid URDF model.\nAcceptance Scenarios:\n- Given a basic URDF file, When the user modifies it following the tutorial, Then they can create a valid robot body model\nEdge Cases\n- ROS 2 distribution mismatch\n- Missing dependencies during setup\n- URDF syntax errors causing launch failures\n- Python environment misconfiguration\nRequirements (mandatory)\nFunctional Requirements\n- FR-001: The module MUST explain ROS 2 nodes, topics, and services accurately.\n- FR-002: The module MUST include executable rclpy examples.\n- FR-003: The module MUST include a valid humanoid URDF example.\n- FR-004: The module MUST avoid ROS 1 concepts.\nKey Entities\n- ROS 2 Nodes\n- Topics and Services\n- rclpy Python scripts\n- URDF files (links, joints, hierarchy)\n- Code blocks and CLI commands\n- Diagrams (conceptual or ASCII)\nSuccess Criteria (mandatory)\nMeasurable Outcomes\n- SC-001: Users can successfully run a ROS 2 publisher/subscriber example.\n- SC-002: Users can modify and execute a Python rclpy node.\n- SC-003: Users can explain the role of nodes and topics.\n- SC-004: Users can understand and edit a basic humanoid URDF model.\nOut of Scope\n- ROS 1\n- Simulation tools (Gazebo / Isaac)\n- Hardware deployment\nContent Structure\n- Chapter 1: ROS 2 Fundamentals for Humanoid Robots\n- Chapter 2: Nodes, Topics, Services & rclpy Integration\n- Chapter 3: URDF Basics for Humanoids\n--------------------------------------------------------------------------------"
  },
  {
    "url": "https://Muham251.github.io/ai-hackathon-1/docs/intro#getting-started",
    "text": "Tutorial Intro\nLet's discover Docusaurus in less than 5 minutes.\nGetting Started\nGet started by creating a new site.\nOr try Docusaurus immediately with docusaurus.new.\nWhat you'll need\n- Node.js version 20.0 or above:\n- When installing Node.js, you are recommended to check all checkboxes related to dependencies.\nGenerate a new site\nGenerate a new Docusaurus site using the classic template.\nThe classic template will automatically be added to your project after you run the command:\nnpm init docusaurus@latest my-website classic\nYou can type this command into Command Prompt, Powershell, Terminal, or any other integrated terminal of your code editor.\nThe command also installs all necessary dependencies you need to run Docusaurus.\nStart your site\nRun the development server:\ncd my-website\nnpm run start\nThe cd\ncommand changes the directory you're working with. In order to work with your newly created Docusaurus site, you'll need to navigate the terminal there.\nThe npm run start\ncommand builds your website locally and serves it through a development server, ready for you to view at http://localhost:3000/.\nOpen docs/intro.md\n(this page) and edit some lines: the site reloads automatically and displays your changes.\n--------------------------------------------------------------------------------"
  },
  {
    "url": "https://Muham251.github.io/ai-hackathon-1/docs/intro#what-youll-need",
    "text": "Tutorial Intro\nLet's discover Docusaurus in less than 5 minutes.\nGetting Started\nGet started by creating a new site.\nOr try Docusaurus immediately with docusaurus.new.\nWhat you'll need\n- Node.js version 20.0 or above:\n- When installing Node.js, you are recommended to check all checkboxes related to dependencies.\nGenerate a new site\nGenerate a new Docusaurus site using the classic template.\nThe classic template will automatically be added to your project after you run the command:\nnpm init docusaurus@latest my-website classic\nYou can type this command into Command Prompt, Powershell, Terminal, or any other integrated terminal of your code editor.\nThe command also installs all necessary dependencies you need to run Docusaurus.\nStart your site\nRun the development server:\ncd my-website\nnpm run start\nThe cd\ncommand changes the directory you're working with. In order to work with your newly created Docusaurus site, you'll need to navigate the terminal there.\nThe npm run start\ncommand builds your website locally and serves it through a development server, ready for you to view at http://localhost:3000/.\nOpen docs/intro.md\n(this page) and edit some lines: the site reloads automatically and displays your changes.\n--------------------------------------------------------------------------------"
  },
  {
    "url": "https://Muham251.github.io/ai-hackathon-1/docs/intro#generate-a-new-site",
    "text": "Tutorial Intro\nLet's discover Docusaurus in less than 5 minutes.\nGetting Started\nGet started by creating a new site.\nOr try Docusaurus immediately with docusaurus.new.\nWhat you'll need\n- Node.js version 20.0 or above:\n- When installing Node.js, you are recommended to check all checkboxes related to dependencies.\nGenerate a new site\nGenerate a new Docusaurus site using the classic template.\nThe classic template will automatically be added to your project after you run the command:\nnpm init docusaurus@latest my-website classic\nYou can type this command into Command Prompt, Powershell, Terminal, or any other integrated terminal of your code editor.\nThe command also installs all necessary dependencies you need to run Docusaurus.\nStart your site\nRun the development server:\ncd my-website\nnpm run start\nThe cd\ncommand changes the directory you're working with. In order to work with your newly created Docusaurus site, you'll need to navigate the terminal there.\nThe npm run start\ncommand builds your website locally and serves it through a development server, ready for you to view at http://localhost:3000/.\nOpen docs/intro.md\n(this page) and edit some lines: the site reloads automatically and displays your changes.\n--------------------------------------------------------------------------------"
  },
  {
    "url": "https://Muham251.github.io/ai-hackathon-1/docs/intro#start-your-site",
    "text": "Tutorial Intro\nLet's discover Docusaurus in less than 5 minutes.\nGetting Started\nGet started by creating a new site.\nOr try Docusaurus immediately with docusaurus.new.\nWhat you'll need\n- Node.js version 20.0 or above:\n- When installing Node.js, you are recommended to check all checkboxes related to dependencies.\nGenerate a new site\nGenerate a new Docusaurus site using the classic template.\nThe classic template will automatically be added to your project after you run the command:\nnpm init docusaurus@latest my-website classic\nYou can type this command into Command Prompt, Powershell, Terminal, or any other integrated terminal of your code editor.\nThe command also installs all necessary dependencies you need to run Docusaurus.\nStart your site\nRun the development server:\ncd my-website\nnpm run start\nThe cd\ncommand changes the directory you're working with. In order to work with your newly created Docusaurus site, you'll need to navigate the terminal there.\nThe npm run start\ncommand builds your website locally and serves it through a development server, ready for you to view at http://localhost:3000/.\nOpen docs/intro.md\n(this page) and edit some lines: the site reloads automatically and displays your changes.\n--------------------------------------------------------------------------------"
  }
]