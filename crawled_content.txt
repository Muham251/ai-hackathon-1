URL: https://Muham251.github.io/ai-hackathon-1/
Content:
Easy to Use
Docusaurus was designed from the ground up to be easily installed and used to get your website up and running quickly.
Focus on What Matters
Docusaurus lets you focus on your docs, and we'll do the chores. Go ahead and move your docs into the docs
directory.
Powered by React
Extend or customize your website layout by reusing React. Docusaurus can be extended while reusing the same header and footer.
--------------------------------------------------------------------------------
URL: https://Muham251.github.io/ai-hackathon-1/#__docusaurus_skipToContent_fallback
Content:
Easy to Use
Docusaurus was designed from the ground up to be easily installed and used to get your website up and running quickly.
Focus on What Matters
Docusaurus lets you focus on your docs, and we'll do the chores. Go ahead and move your docs into the docs
directory.
Powered by React
Extend or customize your website layout by reusing React. Docusaurus can be extended while reusing the same header and footer.
--------------------------------------------------------------------------------
URL: https://Muham251.github.io/ai-hackathon-1/docs/intro
Content:
Tutorial Intro
Let's discover Docusaurus in less than 5 minutes.
Getting Started
Get started by creating a new site.
Or try Docusaurus immediately with docusaurus.new.
What you'll need
- Node.js version 20.0 or above:
- When installing Node.js, you are recommended to check all checkboxes related to dependencies.
Generate a new site
Generate a new Docusaurus site using the classic template.
The classic template will automatically be added to your project after you run the command:
npm init docusaurus@latest my-website classic
You can type this command into Command Prompt, Powershell, Terminal, or any other integrated terminal of your code editor.
The command also installs all necessary dependencies you need to run Docusaurus.
Start your site
Run the development server:
cd my-website
npm run start
The cd
command changes the directory you're working with. In order to work with your newly created Docusaurus site, you'll need to navigate the terminal there.
The npm run start
command builds your website locally and serves it through a development server, ready for you to view at http://localhost:3000/.
Open docs/intro.md
(this page) and edit some lines: the site reloads automatically and displays your changes.
--------------------------------------------------------------------------------
URL: https://Muham251.github.io/ai-hackathon-1/docs/intro#__docusaurus_skipToContent_fallback
Content:
Tutorial Intro
Let's discover Docusaurus in less than 5 minutes.
Getting Started
Get started by creating a new site.
Or try Docusaurus immediately with docusaurus.new.
What you'll need
- Node.js version 20.0 or above:
- When installing Node.js, you are recommended to check all checkboxes related to dependencies.
Generate a new site
Generate a new Docusaurus site using the classic template.
The classic template will automatically be added to your project after you run the command:
npm init docusaurus@latest my-website classic
You can type this command into Command Prompt, Powershell, Terminal, or any other integrated terminal of your code editor.
The command also installs all necessary dependencies you need to run Docusaurus.
Start your site
Run the development server:
cd my-website
npm run start
The cd
command changes the directory you're working with. In order to work with your newly created Docusaurus site, you'll need to navigate the terminal there.
The npm run start
command builds your website locally and serves it through a development server, ready for you to view at http://localhost:3000/.
Open docs/intro.md
(this page) and edit some lines: the site reloads automatically and displays your changes.
--------------------------------------------------------------------------------
URL: https://Muham251.github.io/ai-hackathon-1/docs/module-1/
Content:
Feature Specification: Module 1 ‚Äî The Robotic Nervous System (ROS 2)
Feature Branch: module-1
Created: December 7, 2025
Status: Draft
Input: User description: "Module: 1 ‚Äî The Robotic Nervous System (ROS 2)"
User Scenarios & Testing (mandatory)
User Story 1 - ROS 2 Fundamentals (Priority: P1)
As a beginner robotics student, I want to understand ROS 2 nodes and topics so I can run basic robot programs.
Why this priority: This is the foundational knowledge required to work with any ROS 2 system, and without understanding these core concepts, the learner cannot progress to more advanced topics.
Independent Test: Can be fully tested by successfully running a basic publisher/subscriber example and explaining the communication patterns.
Acceptance Scenarios:
- Given a basic understanding of programming concepts, When the user reads the ROS 2 fundamentals chapter, Then they can explain the roles of nodes and topics
- Given a working ROS 2 environment, When the user runs the provided publisher/subscriber example, Then they can observe the message passing between nodes
User Story 2 - Python Integration with rclpy (Priority: P2)
As an AI developer, I want to use rclpy to connect Python agents to robot controllers.
Why this priority: Python is the primary language for AI development, so connecting AI agents to ROS 2 systems is critical for the target audience.
Independent Test: Can be fully tested by successfully running a Python script that communicates with ROS 2 and demonstrates control concepts.
Acceptance Scenarios:
- Given a ROS 2 environment with rclpy installed, When the user runs the Python examples, Then they can see the interaction between Python code and robot controllers
User Story 3 - URDF for Humanoid Robots (Priority: P3)
As a humanoid robotics learner, I want to understand URDF so I can model a robot body.
Why this priority: Understanding robot modeling is essential for working with humanoid robots, which is the main focus of this curriculum.
Independent Test: Can be fully tested by successfully loading and modifying a basic humanoid URDF model.
Acceptance Scenarios:
- Given a basic URDF file, When the user modifies it following the tutorial, Then they can create a valid robot body model
Edge Cases
- ROS 2 distribution mismatch
- Missing dependencies during setup
- URDF syntax errors causing launch failures
- Python environment misconfiguration
Requirements (mandatory)
Functional Requirements
- FR-001: The module MUST explain ROS 2 nodes, topics, and services accurately.
- FR-002: The module MUST include executable rclpy examples.
- FR-003: The module MUST include a valid humanoid URDF example.
- FR-004: The module MUST avoid ROS 1 concepts.
Key Entities
- ROS 2 Nodes
- Topics and Services
- rclpy Python scripts
- URDF files (links, joints, hierarchy)
- Code blocks and CLI commands
- Diagrams (conceptual or ASCII)
Success Criteria (mandatory)
Measurable Outcomes
- SC-001: Users can successfully run a ROS 2 publisher/subscriber example.
- SC-002: Users can modify and execute a Python rclpy node.
- SC-003: Users can explain the role of nodes and topics.
- SC-004: Users can understand and edit a basic humanoid URDF model.
Out of Scope
- ROS 1
- Simulation tools (Gazebo / Isaac)
- Hardware deployment
Content Structure
- Chapter 1: ROS 2 Fundamentals for Humanoid Robots
- Chapter 2: Nodes, Topics, Services & rclpy Integration
- Chapter 3: URDF Basics for Humanoids
--------------------------------------------------------------------------------
URL: https://Muham251.github.io/ai-hackathon-1/docs/module-1/#__docusaurus_skipToContent_fallback
Content:
Feature Specification: Module 1 ‚Äî The Robotic Nervous System (ROS 2)
Feature Branch: module-1
Created: December 7, 2025
Status: Draft
Input: User description: "Module: 1 ‚Äî The Robotic Nervous System (ROS 2)"
User Scenarios & Testing (mandatory)
User Story 1 - ROS 2 Fundamentals (Priority: P1)
As a beginner robotics student, I want to understand ROS 2 nodes and topics so I can run basic robot programs.
Why this priority: This is the foundational knowledge required to work with any ROS 2 system, and without understanding these core concepts, the learner cannot progress to more advanced topics.
Independent Test: Can be fully tested by successfully running a basic publisher/subscriber example and explaining the communication patterns.
Acceptance Scenarios:
- Given a basic understanding of programming concepts, When the user reads the ROS 2 fundamentals chapter, Then they can explain the roles of nodes and topics
- Given a working ROS 2 environment, When the user runs the provided publisher/subscriber example, Then they can observe the message passing between nodes
User Story 2 - Python Integration with rclpy (Priority: P2)
As an AI developer, I want to use rclpy to connect Python agents to robot controllers.
Why this priority: Python is the primary language for AI development, so connecting AI agents to ROS 2 systems is critical for the target audience.
Independent Test: Can be fully tested by successfully running a Python script that communicates with ROS 2 and demonstrates control concepts.
Acceptance Scenarios:
- Given a ROS 2 environment with rclpy installed, When the user runs the Python examples, Then they can see the interaction between Python code and robot controllers
User Story 3 - URDF for Humanoid Robots (Priority: P3)
As a humanoid robotics learner, I want to understand URDF so I can model a robot body.
Why this priority: Understanding robot modeling is essential for working with humanoid robots, which is the main focus of this curriculum.
Independent Test: Can be fully tested by successfully loading and modifying a basic humanoid URDF model.
Acceptance Scenarios:
- Given a basic URDF file, When the user modifies it following the tutorial, Then they can create a valid robot body model
Edge Cases
- ROS 2 distribution mismatch
- Missing dependencies during setup
- URDF syntax errors causing launch failures
- Python environment misconfiguration
Requirements (mandatory)
Functional Requirements
- FR-001: The module MUST explain ROS 2 nodes, topics, and services accurately.
- FR-002: The module MUST include executable rclpy examples.
- FR-003: The module MUST include a valid humanoid URDF example.
- FR-004: The module MUST avoid ROS 1 concepts.
Key Entities
- ROS 2 Nodes
- Topics and Services
- rclpy Python scripts
- URDF files (links, joints, hierarchy)
- Code blocks and CLI commands
- Diagrams (conceptual or ASCII)
Success Criteria (mandatory)
Measurable Outcomes
- SC-001: Users can successfully run a ROS 2 publisher/subscriber example.
- SC-002: Users can modify and execute a Python rclpy node.
- SC-003: Users can explain the role of nodes and topics.
- SC-004: Users can understand and edit a basic humanoid URDF model.
Out of Scope
- ROS 1
- Simulation tools (Gazebo / Isaac)
- Hardware deployment
Content Structure
- Chapter 1: ROS 2 Fundamentals for Humanoid Robots
- Chapter 2: Nodes, Topics, Services & rclpy Integration
- Chapter 3: URDF Basics for Humanoids
--------------------------------------------------------------------------------
URL: https://Muham251.github.io/ai-hackathon-1/docs/module-1/nodes-topics-services-rclpy
Content:
ROS 2 Fundamentals & Python
This chapter covers the three fundamental communication concepts in ROS 2: Nodes, Topics, and Services. We'll also see how to implement them using rclpy
, the official Python client library for ROS 2.
1. ROS 2 Nodes
A Node is the smallest, most fundamental unit of a ROS 2 system. Think of a node as a single, independent process in your robot's software architecture. Each node should have a single, well-defined purpose.
For a humanoid robot, you might have nodes for:
- Reading sensor data from an IMU.
- Controlling the motors in the right leg.
- Processing camera images.
- Planning footsteps for walking.
Nodes are what allow ROS 2 systems to be modular and fault-tolerant. If one node crashes, the rest of the system can continue to run.
Example: A Simple rclpy
Node
Here is the "hello world" of rclpy
: a simple node that initializes itself, prints a message, and then shuts down.
# 'hello_node.py'
import rclpy
from rclpy.node import Node
def main(args=None):
# 1. Initialize the rclpy library
rclpy.init(args=args)
# 2. Create a Node
# The node is named 'hello_ros_node'
node = Node('hello_ros_node')
# 3. From this point, the node is running.
# You can add your logic here.
node.get_logger().info('Hello, ROS 2 World!')
# 4. The node is destroyed when the 'with' statement exits.
# This is important for cleanup.
node.destroy_node()
# 5. Shutdown the rclpy library
rclpy.shutdown()
if __name__ == '__main__':
main()
To run this code:
- Save it as
hello_node.py
. - Make sure you have a ROS 2 environment sourced.
- Run
python hello_node.py
.
2. ROS 2 Topics (Publish/Subscribe)
Topics are the primary mechanism for one-way, asynchronous communication in ROS 2. Nodes can publish messages to a topic, and other nodes can subscribe to that topic to receive those messages. This is a decoupled communication model; publishers and subscribers don't need to know about each other.
- Use Case: Continuously streaming data, like sensor readings or motor commands.
Diagram: Publisher-Subscriber Model
graph TD
A[Publisher Node] -- Publishes message --> B(Topic: /sensor_data);
B -- Message is delivered --> C[Subscriber Node 1];
B -- Message is delivered --> D[Subscriber Node 2];
Example: A Simple Publisher and Subscriber
First, we define a publisher node that sends a simple string message every second.
# 'publisher_node.py'
import rclpy
from rclpy.node import Node
from std_msgs.msg import String
class SimplePublisher(Node):
def __init__(self):
super().__init__('simple_publisher')
# Create a publisher on the 'chatter' topic
self.publisher_ = self.create_publisher(String, 'chatter', 10)
self.timer = self.create_timer(1.0, self.timer_callback)
self.i = 0
def timer_callback(self):
msg = String()
msg.data = f'Hello from publisher: {self.i}'
self.publisher_.publish(msg)
self.get_logger().info(f'Publishing: "{msg.data}"')
self.i += 1
def main(args=None):
rclpy.init(args=args)
publisher = SimplePublisher()
rclpy.spin(publisher) # Keep the node alive
publisher.destroy_node()
rclpy.shutdown()
if __name__ == '__main__':
main()
Next, a subscriber node that listens to the chatter
topic.
# 'subscriber_node.py'
import rclpy
from rclpy.node import Node
from std_msgs.msg import String
class SimpleSubscriber(Node):
def __init__(self):
super().__init__('simple_subscriber')
# Create a subscription to the 'chatter' topic
self.subscription = self.create_subscription(
String,
'chatter',
self.listener_callback,
10)
self.subscription # prevent unused variable warning
def listener_callback(self, msg):
self.get_logger().info(f'I heard: "{msg.data}"')
def main(args=None):
rclpy.init(args=args)
subscriber = SimpleSubscriber()
rclpy.spin(subscriber)
subscriber.destroy_node()
rclpy.shutdown()
if __name__ == '__main__':
main()
To run this example:
- Open two terminals with your ROS 2 environment sourced.
- In the first terminal, run
python publisher_node.py
. - In the second terminal, run
python subscriber_node.py
. You will see the messages from the publisher being received.
3. ROS 2 Services (Request/Response)
Services are for two-way, synchronous communication. A Service Server node provides a service, and a Service Client node can send a request and wait for a response. This is a tightly coupled, blocking communication model.
- Use Case: Triggering a specific action that has a clear start and end, like "open gripper" or "calculate inverse kinematics".
Diagram: Service Client/Server Model
graph TD
A[Service Client Node] -- Sends Request --> B(Service: /add_two_ints);
B -- Delivers Request --> C[Service Server Node];
C -- Processes and sends Response --> B;
B -- Delivers Response --> A;
Example: A Simple Service Server and Client
First, the server node that offers a service to add two integers. ROS 2 has a standard service definition for this, example_interfaces/srv/AddTwoInts
.
# 'service_server_node.py'
import rclpy
from rclpy.node import Node
from example_interfaces.srv import AddTwoInts
class SimpleServiceServer(Node):
def __init__(self):
super().__init__('simple_service_server')
# Create a service named 'add_two_ints'
self.srv = self.create_service(AddTwoInts, 'add_two_ints', self.add_two_ints_callback)
def add_two_ints_callback(self, request, response):
response.sum = request.a + request.b
self.get_logger().info(f'Incoming request: a={request.a}, b={request.b}. Returning sum={response.sum}')
return response
def main(args=None):
rclpy.init(args=args)
server = SimpleServiceServer()
rclpy.spin(server)
server.destroy_node()
rclpy.shutdown()
if __name__ == '__main__':
main()
Next, the client node that calls the service.
# 'service_client_node.py'
import rclpy
from rclpy.node import Node
from example_interfaces.srv import AddTwoInts
import sys
class SimpleServiceClient(Node):
def __init__(self):
super().__init__('simple_service_client')
# Create a client for the 'add_two_ints' service
self.client = self.create_client(AddTwoInts, 'add_two_ints')
while not self.client.wait_for_service(timeout_sec=1.0):
self.get_logger().info('Service not available, waiting again...')
self.req = AddTwoInts.Request()
def send_request(self, a, b):
self.req.a = a
self.req.b = b
self.future = self.client.call_async(self.req)
rclpy.spin_until_future_complete(self, self.future)
return self.future.result()
def main(args=None):
rclpy.init(args=args)
if len(sys.argv) != 3:
print("Usage: python service_client_node.py <int1> <int2>")
return
client = SimpleServiceClient()
response = client.send_request(int(sys.argv[1]), int(sys.argv[2]))
if response:
client.get_logger().info(
f'Result of add_two_ints: for {sys.argv[1]} + {sys.argv[2]} = {response.sum}')
else:
client.get_logger().error('Service call failed')
client.destroy_node()
rclpy.shutdown()
if __name__ == '__main__':
main()
To run this example:
- Open two terminals with your ROS 2 environment sourced.
- In the first, run
python service_server_node.py
. - In the second, run
python service_client_node.py 5 10
. The client will send the request and print the response.
--------------------------------------------------------------------------------
URL: https://Muham251.github.io/ai-hackathon-1/docs/module-1/nodes-topics-services-rclpy#__docusaurus_skipToContent_fallback
Content:
ROS 2 Fundamentals & Python
This chapter covers the three fundamental communication concepts in ROS 2: Nodes, Topics, and Services. We'll also see how to implement them using rclpy
, the official Python client library for ROS 2.
1. ROS 2 Nodes
A Node is the smallest, most fundamental unit of a ROS 2 system. Think of a node as a single, independent process in your robot's software architecture. Each node should have a single, well-defined purpose.
For a humanoid robot, you might have nodes for:
- Reading sensor data from an IMU.
- Controlling the motors in the right leg.
- Processing camera images.
- Planning footsteps for walking.
Nodes are what allow ROS 2 systems to be modular and fault-tolerant. If one node crashes, the rest of the system can continue to run.
Example: A Simple rclpy
Node
Here is the "hello world" of rclpy
: a simple node that initializes itself, prints a message, and then shuts down.
# 'hello_node.py'
import rclpy
from rclpy.node import Node
def main(args=None):
# 1. Initialize the rclpy library
rclpy.init(args=args)
# 2. Create a Node
# The node is named 'hello_ros_node'
node = Node('hello_ros_node')
# 3. From this point, the node is running.
# You can add your logic here.
node.get_logger().info('Hello, ROS 2 World!')
# 4. The node is destroyed when the 'with' statement exits.
# This is important for cleanup.
node.destroy_node()
# 5. Shutdown the rclpy library
rclpy.shutdown()
if __name__ == '__main__':
main()
To run this code:
- Save it as
hello_node.py
. - Make sure you have a ROS 2 environment sourced.
- Run
python hello_node.py
.
2. ROS 2 Topics (Publish/Subscribe)
Topics are the primary mechanism for one-way, asynchronous communication in ROS 2. Nodes can publish messages to a topic, and other nodes can subscribe to that topic to receive those messages. This is a decoupled communication model; publishers and subscribers don't need to know about each other.
- Use Case: Continuously streaming data, like sensor readings or motor commands.
Diagram: Publisher-Subscriber Model
graph TD
A[Publisher Node] -- Publishes message --> B(Topic: /sensor_data);
B -- Message is delivered --> C[Subscriber Node 1];
B -- Message is delivered --> D[Subscriber Node 2];
Example: A Simple Publisher and Subscriber
First, we define a publisher node that sends a simple string message every second.
# 'publisher_node.py'
import rclpy
from rclpy.node import Node
from std_msgs.msg import String
class SimplePublisher(Node):
def __init__(self):
super().__init__('simple_publisher')
# Create a publisher on the 'chatter' topic
self.publisher_ = self.create_publisher(String, 'chatter', 10)
self.timer = self.create_timer(1.0, self.timer_callback)
self.i = 0
def timer_callback(self):
msg = String()
msg.data = f'Hello from publisher: {self.i}'
self.publisher_.publish(msg)
self.get_logger().info(f'Publishing: "{msg.data}"')
self.i += 1
def main(args=None):
rclpy.init(args=args)
publisher = SimplePublisher()
rclpy.spin(publisher) # Keep the node alive
publisher.destroy_node()
rclpy.shutdown()
if __name__ == '__main__':
main()
Next, a subscriber node that listens to the chatter
topic.
# 'subscriber_node.py'
import rclpy
from rclpy.node import Node
from std_msgs.msg import String
class SimpleSubscriber(Node):
def __init__(self):
super().__init__('simple_subscriber')
# Create a subscription to the 'chatter' topic
self.subscription = self.create_subscription(
String,
'chatter',
self.listener_callback,
10)
self.subscription # prevent unused variable warning
def listener_callback(self, msg):
self.get_logger().info(f'I heard: "{msg.data}"')
def main(args=None):
rclpy.init(args=args)
subscriber = SimpleSubscriber()
rclpy.spin(subscriber)
subscriber.destroy_node()
rclpy.shutdown()
if __name__ == '__main__':
main()
To run this example:
- Open two terminals with your ROS 2 environment sourced.
- In the first terminal, run
python publisher_node.py
. - In the second terminal, run
python subscriber_node.py
. You will see the messages from the publisher being received.
3. ROS 2 Services (Request/Response)
Services are for two-way, synchronous communication. A Service Server node provides a service, and a Service Client node can send a request and wait for a response. This is a tightly coupled, blocking communication model.
- Use Case: Triggering a specific action that has a clear start and end, like "open gripper" or "calculate inverse kinematics".
Diagram: Service Client/Server Model
graph TD
A[Service Client Node] -- Sends Request --> B(Service: /add_two_ints);
B -- Delivers Request --> C[Service Server Node];
C -- Processes and sends Response --> B;
B -- Delivers Response --> A;
Example: A Simple Service Server and Client
First, the server node that offers a service to add two integers. ROS 2 has a standard service definition for this, example_interfaces/srv/AddTwoInts
.
# 'service_server_node.py'
import rclpy
from rclpy.node import Node
from example_interfaces.srv import AddTwoInts
class SimpleServiceServer(Node):
def __init__(self):
super().__init__('simple_service_server')
# Create a service named 'add_two_ints'
self.srv = self.create_service(AddTwoInts, 'add_two_ints', self.add_two_ints_callback)
def add_two_ints_callback(self, request, response):
response.sum = request.a + request.b
self.get_logger().info(f'Incoming request: a={request.a}, b={request.b}. Returning sum={response.sum}')
return response
def main(args=None):
rclpy.init(args=args)
server = SimpleServiceServer()
rclpy.spin(server)
server.destroy_node()
rclpy.shutdown()
if __name__ == '__main__':
main()
Next, the client node that calls the service.
# 'service_client_node.py'
import rclpy
from rclpy.node import Node
from example_interfaces.srv import AddTwoInts
import sys
class SimpleServiceClient(Node):
def __init__(self):
super().__init__('simple_service_client')
# Create a client for the 'add_two_ints' service
self.client = self.create_client(AddTwoInts, 'add_two_ints')
while not self.client.wait_for_service(timeout_sec=1.0):
self.get_logger().info('Service not available, waiting again...')
self.req = AddTwoInts.Request()
def send_request(self, a, b):
self.req.a = a
self.req.b = b
self.future = self.client.call_async(self.req)
rclpy.spin_until_future_complete(self, self.future)
return self.future.result()
def main(args=None):
rclpy.init(args=args)
if len(sys.argv) != 3:
print("Usage: python service_client_node.py <int1> <int2>")
return
client = SimpleServiceClient()
response = client.send_request(int(sys.argv[1]), int(sys.argv[2]))
if response:
client.get_logger().info(
f'Result of add_two_ints: for {sys.argv[1]} + {sys.argv[2]} = {response.sum}')
else:
client.get_logger().error('Service call failed')
client.destroy_node()
rclpy.shutdown()
if __name__ == '__main__':
main()
To run this example:
- Open two terminals with your ROS 2 environment sourced.
- In the first, run
python service_server_node.py
. - In the second, run
python service_client_node.py 5 10
. The client will send the request and print the response.
--------------------------------------------------------------------------------
URL: https://Muham251.github.io/ai-hackathon-1/docs/module-1/ros-2-basics-for-humanoid-control
Content:
Module 1: The Robotic Nervous System (ROS 2)
Welcome to Module 1! This module introduces the Robot Operating System (ROS 2), the fundamental software framework that acts as the nervous system for modern robots, including humanoids. As we journey into controlling complex humanoid robots, understanding ROS 2 is the essential first step.
Target Audience
This module is designed for students and developers who are new to robotics but have some programming experience, particularly in Python. If you're aiming to understand how to make a humanoid robot walk, talk, or interact with its environment, you're in the right place.
What is ROS 2?
ROS 2 is a flexible framework for writing robot software. It is a set of software libraries and tools that help you build robot applications. Think of it as the middleware that handles all the complex communication between different parts of your robot's software. From low-level motor control to high-level planning and perception, ROS 2 provides the tools to manage it all.
For a humanoid robot, this is crucial. A humanoid may have dozens of motors (actuators), a suite of sensors (cameras, IMUs, force sensors), and multiple computers running different software modules. ROS 2 allows these components to communicate with each other in a standardized way.
Learning Objectives
By the end of this module, you will be able to:
- Explain the core components of the ROS 2 architecture.
- Create and run ROS 2 nodes, topics, and services.
- Write a simple Python agent that communicates with a ROS 2 system.
- Understand and write a basic URDF file for a humanoid robot model.
Module Chapters
This module is divided into the following chapters:
- ROS 2 Fundamentals (Nodes, Topics, Services): A deep dive into the core communication patterns in ROS 2.
- URDF Basics for Humanoid Models: An introduction to the Unified Robot Description Format (URDF) for modeling your humanoid.
Let's get started!
--------------------------------------------------------------------------------
URL: https://Muham251.github.io/ai-hackathon-1/docs/module-1/ros-2-basics-for-humanoid-control#__docusaurus_skipToContent_fallback
Content:
Module 1: The Robotic Nervous System (ROS 2)
Welcome to Module 1! This module introduces the Robot Operating System (ROS 2), the fundamental software framework that acts as the nervous system for modern robots, including humanoids. As we journey into controlling complex humanoid robots, understanding ROS 2 is the essential first step.
Target Audience
This module is designed for students and developers who are new to robotics but have some programming experience, particularly in Python. If you're aiming to understand how to make a humanoid robot walk, talk, or interact with its environment, you're in the right place.
What is ROS 2?
ROS 2 is a flexible framework for writing robot software. It is a set of software libraries and tools that help you build robot applications. Think of it as the middleware that handles all the complex communication between different parts of your robot's software. From low-level motor control to high-level planning and perception, ROS 2 provides the tools to manage it all.
For a humanoid robot, this is crucial. A humanoid may have dozens of motors (actuators), a suite of sensors (cameras, IMUs, force sensors), and multiple computers running different software modules. ROS 2 allows these components to communicate with each other in a standardized way.
Learning Objectives
By the end of this module, you will be able to:
- Explain the core components of the ROS 2 architecture.
- Create and run ROS 2 nodes, topics, and services.
- Write a simple Python agent that communicates with a ROS 2 system.
- Understand and write a basic URDF file for a humanoid robot model.
Module Chapters
This module is divided into the following chapters:
- ROS 2 Fundamentals (Nodes, Topics, Services): A deep dive into the core communication patterns in ROS 2.
- URDF Basics for Humanoid Models: An introduction to the Unified Robot Description Format (URDF) for modeling your humanoid.
Let's get started!
--------------------------------------------------------------------------------
URL: https://Muham251.github.io/ai-hackathon-1/docs/module-1/urdf-essentials
Content:
URDF Basics for Humanoid Models
The Unified Robot Description Format (URDF) is an XML format used in ROS to describe all the physical elements of a robot model. For a humanoid, the URDF file is its digital blueprint. It defines the robot's body parts, how they are connected, and their physical properties.
A URDF file describes the robot as a tree of links connected by joints.
<link>
: A physical part of the robot (e.g., a forearm, a foot, a torso).<joint>
: The connection between two links. It defines how one link moves relative to another (e.g., rotating like an elbow, sliding, or being fixed).
Key URDF Tags
The <robot>
Tag
Every URDF file must start and end with the <robot>
tag. It's the root element, and you give your robot a name here.
<robot name="simple_humanoid">
... all your links and joints go here ...
</robot>
The <link>
Tag
Each link has a name and can contain three important sub-tags:
<visual>
: Defines the appearance of the link (shape, size, color, texture). This is what you see in simulation tools like RViz.<collision>
: Defines the collision geometry of the link. This is what the physics engine uses to calculate collisions with other objects. It's often a simpler shape than the visual one for performance reasons.<inertial>
: Defines the dynamic properties of the link: its mass, center of mass, and inertia matrix. This is crucial for realistic physics simulation.
<link name="torso">
<visual>
<geometry>
<box size="0.2 0.3 0.5"/>
</geometry>
<material name="grey">
<color rgba="0.5 0.5 0.5 1"/>
</material>
</visual>
<collision>
<geometry>
<box size="0.2 0.3 0.5"/>
</geometry>
</collision>
<inertial>
<mass value="10"/>
<inertia ixx="0.4" ixy="0.0" ixz="0.0" iyy="0.4" iyz="0.0" izz="0.2"/>
</inertial>
</link>
The <joint>
Tag
Each joint connects two links, called the parent and the child. It also defines the motion allowed between them.
name
: The name of the joint (e.g., "left_shoulder_joint").type
: The type of motion. Common types are:revolute
: Rotates around an axis (like an elbow). Requires<limit>
tags for upper and lower angles.continuous
: Rotates around an axis with no angle limits.prismatic
: Slides along an axis (like a piston).fixed
: No motion is allowed between the two links.
<parent link="..." />
: The name of the parent link.<child link="..." />
: The name of the child link.<origin xyz="..." rpy="..." />
: The transform (position and orientation) from the parent link's origin to the joint's origin.<axis xyz="..." />
: The axis of rotation or translation forrevolute
andprismatic
joints.
<joint name="neck_joint" type="revolute">
<parent link="torso"/>
<child link="head"/>
<origin xyz="0 0 0.25" rpy="0 0 0"/>
<axis xyz="0 0 1"/>
<limit lower="-0.5" upper="0.5" effort="10" velocity="1.0"/>
</joint>
Simple Humanoid URDF Example
This example describes a very simple humanoid robot. It has a torso, a head, and a single arm with a shoulder and elbow joint. This demonstrates the parent-child tree structure. The "torso" is the root link of our tree.
<!-- simple_humanoid.urdf -->
<robot name="simple_humanoid">
<!-- A. Materials (for color) -->
<material name="blue">
<color rgba="0.0 0.0 0.8 1.0"/>
</material>
<material name="white">
<color rgba="1.0 1.0 1.0 1.0"/>
</material>
<!-- B. Base Link (Torso) -->
<link name="torso">
<visual>
<geometry><box size="0.2 0.3 0.5"/></geometry>
<origin xyz="0 0 0" rpy="0 0 0"/>
<material name="blue"/>
</visual>
<collision><geometry><box size="0.2 0.3 0.5"/></geometry></collision>
<inertial>
<mass value="10"/>
<inertia ixx="1.0" ixy="0" ixz="0" iyy="1.0" iyz="0" izz="1.0"/>
</inertial>
</link>
<!-- C. Head Link -->
<link name="head">
<visual>
<geometry><sphere radius="0.1"/></geometry>
<origin xyz="0 0 0" rpy="0 0 0"/>
<material name="white"/>
</visual>
<collision><geometry><sphere radius="0.1"/></geometry></collision>
<inertial>
<mass value="1"/>
<inertia ixx="0.1" ixy="0" ixz="0" iyy="0.1" iyz="0" izz="0.1"/>
</inertial>
</link>
<!-- D. Neck Joint (Connects Torso to Head) -->
<joint name="neck_joint" type="revolute">
<parent link="torso"/>
<child link="head"/>
<origin xyz="0 0 0.30" rpy="0 0 0"/> <!-- Position of head relative to torso -->
<axis xyz="0 0 1"/>
<limit lower="-0.7" upper="0.7" effort="5" velocity="0.5"/>
</joint>
<!-- E. Right Arm -->
<link name="right_upper_arm">
<visual>
<geometry><cylinder length="0.3" radius="0.05"/></geometry>
<origin xyz="0 0 -0.15" rpy="0 0 0"/>
<material name="white"/>
</visual>
<collision><geometry><cylinder length="0.3" radius="0.05"/></geometry></collision>
<inertial>
<mass value="2"/>
<inertia ixx="0.1" ixy="0" ixz="0" iyy="0.1" iyz="0" izz="0.1"/>
</inertial>
</link>
<!-- F. Right Shoulder Joint -->
<joint name="right_shoulder_joint" type="revolute">
<parent link="torso"/>
<child link="right_upper_arm"/>
<origin xyz="0 -0.20 0.15" rpy="1.5707 0 0"/> <!-- Position of arm relative to torso -->
<axis xyz="0 0 1"/>
<limit lower="-1.57" upper="1.57" effort="10" velocity="1.0"/>
</joint>
</robot>
How to Use This URDF
- Save the code above as
simple_humanoid.urdf
. - You can check its validity using the
check_urdf
command:check_urdf simple_humanoid.urdf
. - To visualize it, you can use ROS tools like RViz. This typically requires a small launch file to publish the robot's state, which is a topic for a later module.
This simple example provides the foundation. A full humanoid model would extend this tree with more links and joints for the legs, a more complex arm, and hands.
--------------------------------------------------------------------------------
URL: https://Muham251.github.io/ai-hackathon-1/docs/module-1/urdf-essentials#__docusaurus_skipToContent_fallback
Content:
URDF Basics for Humanoid Models
The Unified Robot Description Format (URDF) is an XML format used in ROS to describe all the physical elements of a robot model. For a humanoid, the URDF file is its digital blueprint. It defines the robot's body parts, how they are connected, and their physical properties.
A URDF file describes the robot as a tree of links connected by joints.
<link>
: A physical part of the robot (e.g., a forearm, a foot, a torso).<joint>
: The connection between two links. It defines how one link moves relative to another (e.g., rotating like an elbow, sliding, or being fixed).
Key URDF Tags
The <robot>
Tag
Every URDF file must start and end with the <robot>
tag. It's the root element, and you give your robot a name here.
<robot name="simple_humanoid">
... all your links and joints go here ...
</robot>
The <link>
Tag
Each link has a name and can contain three important sub-tags:
<visual>
: Defines the appearance of the link (shape, size, color, texture). This is what you see in simulation tools like RViz.<collision>
: Defines the collision geometry of the link. This is what the physics engine uses to calculate collisions with other objects. It's often a simpler shape than the visual one for performance reasons.<inertial>
: Defines the dynamic properties of the link: its mass, center of mass, and inertia matrix. This is crucial for realistic physics simulation.
<link name="torso">
<visual>
<geometry>
<box size="0.2 0.3 0.5"/>
</geometry>
<material name="grey">
<color rgba="0.5 0.5 0.5 1"/>
</material>
</visual>
<collision>
<geometry>
<box size="0.2 0.3 0.5"/>
</geometry>
</collision>
<inertial>
<mass value="10"/>
<inertia ixx="0.4" ixy="0.0" ixz="0.0" iyy="0.4" iyz="0.0" izz="0.2"/>
</inertial>
</link>
The <joint>
Tag
Each joint connects two links, called the parent and the child. It also defines the motion allowed between them.
name
: The name of the joint (e.g., "left_shoulder_joint").type
: The type of motion. Common types are:revolute
: Rotates around an axis (like an elbow). Requires<limit>
tags for upper and lower angles.continuous
: Rotates around an axis with no angle limits.prismatic
: Slides along an axis (like a piston).fixed
: No motion is allowed between the two links.
<parent link="..." />
: The name of the parent link.<child link="..." />
: The name of the child link.<origin xyz="..." rpy="..." />
: The transform (position and orientation) from the parent link's origin to the joint's origin.<axis xyz="..." />
: The axis of rotation or translation forrevolute
andprismatic
joints.
<joint name="neck_joint" type="revolute">
<parent link="torso"/>
<child link="head"/>
<origin xyz="0 0 0.25" rpy="0 0 0"/>
<axis xyz="0 0 1"/>
<limit lower="-0.5" upper="0.5" effort="10" velocity="1.0"/>
</joint>
Simple Humanoid URDF Example
This example describes a very simple humanoid robot. It has a torso, a head, and a single arm with a shoulder and elbow joint. This demonstrates the parent-child tree structure. The "torso" is the root link of our tree.
<!-- simple_humanoid.urdf -->
<robot name="simple_humanoid">
<!-- A. Materials (for color) -->
<material name="blue">
<color rgba="0.0 0.0 0.8 1.0"/>
</material>
<material name="white">
<color rgba="1.0 1.0 1.0 1.0"/>
</material>
<!-- B. Base Link (Torso) -->
<link name="torso">
<visual>
<geometry><box size="0.2 0.3 0.5"/></geometry>
<origin xyz="0 0 0" rpy="0 0 0"/>
<material name="blue"/>
</visual>
<collision><geometry><box size="0.2 0.3 0.5"/></geometry></collision>
<inertial>
<mass value="10"/>
<inertia ixx="1.0" ixy="0" ixz="0" iyy="1.0" iyz="0" izz="1.0"/>
</inertial>
</link>
<!-- C. Head Link -->
<link name="head">
<visual>
<geometry><sphere radius="0.1"/></geometry>
<origin xyz="0 0 0" rpy="0 0 0"/>
<material name="white"/>
</visual>
<collision><geometry><sphere radius="0.1"/></geometry></collision>
<inertial>
<mass value="1"/>
<inertia ixx="0.1" ixy="0" ixz="0" iyy="0.1" iyz="0" izz="0.1"/>
</inertial>
</link>
<!-- D. Neck Joint (Connects Torso to Head) -->
<joint name="neck_joint" type="revolute">
<parent link="torso"/>
<child link="head"/>
<origin xyz="0 0 0.30" rpy="0 0 0"/> <!-- Position of head relative to torso -->
<axis xyz="0 0 1"/>
<limit lower="-0.7" upper="0.7" effort="5" velocity="0.5"/>
</joint>
<!-- E. Right Arm -->
<link name="right_upper_arm">
<visual>
<geometry><cylinder length="0.3" radius="0.05"/></geometry>
<origin xyz="0 0 -0.15" rpy="0 0 0"/>
<material name="white"/>
</visual>
<collision><geometry><cylinder length="0.3" radius="0.05"/></geometry></collision>
<inertial>
<mass value="2"/>
<inertia ixx="0.1" ixy="0" ixz="0" iyy="0.1" iyz="0" izz="0.1"/>
</inertial>
</link>
<!-- F. Right Shoulder Joint -->
<joint name="right_shoulder_joint" type="revolute">
<parent link="torso"/>
<child link="right_upper_arm"/>
<origin xyz="0 -0.20 0.15" rpy="1.5707 0 0"/> <!-- Position of arm relative to torso -->
<axis xyz="0 0 1"/>
<limit lower="-1.57" upper="1.57" effort="10" velocity="1.0"/>
</joint>
</robot>
How to Use This URDF
- Save the code above as
simple_humanoid.urdf
. - You can check its validity using the
check_urdf
command:check_urdf simple_humanoid.urdf
. - To visualize it, you can use ROS tools like RViz. This typically requires a small launch file to publish the robot's state, which is a topic for a later module.
This simple example provides the foundation. A full humanoid model would extend this tree with more links and joints for the legs, a more complex arm, and hands.
--------------------------------------------------------------------------------
URL: https://Muham251.github.io/ai-hackathon-1/docs/category/tutorial---basics
Content:
üìÑÔ∏è Create a Page
Add Markdown or React files to src/pages to create a standalone page:
üìÑÔ∏è Create a Document
Documents are groups of pages connected through:
üìÑÔ∏è Create a Blog Post
Docusaurus creates a page for each blog post, but also a blog index page, a tag system, an RSS feed...
üìÑÔ∏è Markdown Features
Docusaurus supports Markdown and a few additional features.
üìÑÔ∏è Deploy your site
Docusaurus is a static-site-generator (also called Jamstack).
üìÑÔ∏è Congratulations!
You have just learned the basics of Docusaurus and made some changes to the initial template.
--------------------------------------------------------------------------------
URL: https://Muham251.github.io/ai-hackathon-1/docs/category/tutorial---basics#__docusaurus_skipToContent_fallback
Content:
üìÑÔ∏è Create a Page
Add Markdown or React files to src/pages to create a standalone page:
üìÑÔ∏è Create a Document
Documents are groups of pages connected through:
üìÑÔ∏è Create a Blog Post
Docusaurus creates a page for each blog post, but also a blog index page, a tag system, an RSS feed...
üìÑÔ∏è Markdown Features
Docusaurus supports Markdown and a few additional features.
üìÑÔ∏è Deploy your site
Docusaurus is a static-site-generator (also called Jamstack).
üìÑÔ∏è Congratulations!
You have just learned the basics of Docusaurus and made some changes to the initial template.
--------------------------------------------------------------------------------
URL: https://Muham251.github.io/ai-hackathon-1/docs/tutorial-basics/create-a-page
Content:
Create a Page
Add Markdown or React files to src/pages
to create a standalone page:
src/pages/index.js
‚Üílocalhost:3000/
src/pages/foo.md
‚Üílocalhost:3000/foo
src/pages/foo/bar.js
‚Üílocalhost:3000/foo/bar
Create your first React Page
Create a file at src/pages/my-react-page.js
:
src/pages/my-react-page.js
import React from 'react';
import Layout from '@theme/Layout';
export default function MyReactPage() {
return (
<Layout>
<h1>My React page</h1>
<p>This is a React page</p>
</Layout>
);
}
A new page is now available at http://localhost:3000/my-react-page.
Create your first Markdown Page
Create a file at src/pages/my-markdown-page.md
:
src/pages/my-markdown-page.md
# My Markdown page
This is a Markdown page
A new page is now available at http://localhost:3000/my-markdown-page.
--------------------------------------------------------------------------------
URL: https://Muham251.github.io/ai-hackathon-1/docs/tutorial-basics/create-a-page#__docusaurus_skipToContent_fallback
Content:
Create a Page
Add Markdown or React files to src/pages
to create a standalone page:
src/pages/index.js
‚Üílocalhost:3000/
src/pages/foo.md
‚Üílocalhost:3000/foo
src/pages/foo/bar.js
‚Üílocalhost:3000/foo/bar
Create your first React Page
Create a file at src/pages/my-react-page.js
:
src/pages/my-react-page.js
import React from 'react';
import Layout from '@theme/Layout';
export default function MyReactPage() {
return (
<Layout>
<h1>My React page</h1>
<p>This is a React page</p>
</Layout>
);
}
A new page is now available at http://localhost:3000/my-react-page.
Create your first Markdown Page
Create a file at src/pages/my-markdown-page.md
:
src/pages/my-markdown-page.md
# My Markdown page
This is a Markdown page
A new page is now available at http://localhost:3000/my-markdown-page.
--------------------------------------------------------------------------------
URL: https://Muham251.github.io/ai-hackathon-1/docs/tutorial-basics/create-a-document
Content:
Create a Document
Documents are groups of pages connected through:
- a sidebar
- previous/next navigation
- versioning
Create your first Doc
Create a Markdown file at docs/hello.md
:
docs/hello.md
# Hello
This is my **first Docusaurus document**!
A new document is now available at http://localhost:3000/docs/hello.
Configure the Sidebar
Docusaurus automatically creates a sidebar from the docs
folder.
Add metadata to customize the sidebar label and position:
docs/hello.md
---
sidebar_label: 'Hi!'
sidebar_position: 3
---
# Hello
This is my **first Docusaurus document**!
It is also possible to create your sidebar explicitly in sidebars.js
:
sidebars.js
export default {
tutorialSidebar: [
'intro',
'hello',
{
type: 'category',
label: 'Tutorial',
items: ['tutorial-basics/create-a-document'],
},
],
};
--------------------------------------------------------------------------------
URL: https://Muham251.github.io/ai-hackathon-1/docs/tutorial-basics/create-a-document#__docusaurus_skipToContent_fallback
Content:
Create a Document
Documents are groups of pages connected through:
- a sidebar
- previous/next navigation
- versioning
Create your first Doc
Create a Markdown file at docs/hello.md
:
docs/hello.md
# Hello
This is my **first Docusaurus document**!
A new document is now available at http://localhost:3000/docs/hello.
Configure the Sidebar
Docusaurus automatically creates a sidebar from the docs
folder.
Add metadata to customize the sidebar label and position:
docs/hello.md
---
sidebar_label: 'Hi!'
sidebar_position: 3
---
# Hello
This is my **first Docusaurus document**!
It is also possible to create your sidebar explicitly in sidebars.js
:
sidebars.js
export default {
tutorialSidebar: [
'intro',
'hello',
{
type: 'category',
label: 'Tutorial',
items: ['tutorial-basics/create-a-document'],
},
],
};
--------------------------------------------------------------------------------
URL: https://Muham251.github.io/ai-hackathon-1/docs/tutorial-basics/create-a-blog-post
Content:
Create a Blog Post
Docusaurus creates a page for each blog post, but also a blog index page, a tag system, an RSS feed...
Create your first Post
Create a file at blog/2021-02-28-greetings.md
:
blog/2021-02-28-greetings.md
---
slug: greetings
title: Greetings!
authors:
- name: Joel Marcey
title: Co-creator of Docusaurus 1
url: https://github.com/JoelMarcey
image_url: https://github.com/JoelMarcey.png
- name: S√©bastien Lorber
title: Docusaurus maintainer
url: https://sebastienlorber.com
image_url: https://github.com/slorber.png
tags: [greetings]
---
Congratulations, you have made your first post!
Feel free to play around and edit this post as much as you like.
A new blog post is now available at http://localhost:3000/blog/greetings.
--------------------------------------------------------------------------------
URL: https://Muham251.github.io/ai-hackathon-1/docs/tutorial-basics/create-a-blog-post#__docusaurus_skipToContent_fallback
Content:
Create a Blog Post
Docusaurus creates a page for each blog post, but also a blog index page, a tag system, an RSS feed...
Create your first Post
Create a file at blog/2021-02-28-greetings.md
:
blog/2021-02-28-greetings.md
---
slug: greetings
title: Greetings!
authors:
- name: Joel Marcey
title: Co-creator of Docusaurus 1
url: https://github.com/JoelMarcey
image_url: https://github.com/JoelMarcey.png
- name: S√©bastien Lorber
title: Docusaurus maintainer
url: https://sebastienlorber.com
image_url: https://github.com/slorber.png
tags: [greetings]
---
Congratulations, you have made your first post!
Feel free to play around and edit this post as much as you like.
A new blog post is now available at http://localhost:3000/blog/greetings.
--------------------------------------------------------------------------------
URL: https://Muham251.github.io/ai-hackathon-1/docs/tutorial-basics/markdown-features
Content:
Markdown Features
Docusaurus supports Markdown and a few additional features.
Front Matter
Markdown documents have metadata at the top called Front Matter:
---
id: my-doc-id
title: My document title
description: My document description
slug: /my-custom-url
---
## Markdown heading
Markdown text with [links](./hello.md)
Links
Regular Markdown links are supported, using url paths or relative file paths.
Let's see how to [Create a page](/create-a-page).
Let's see how to [Create a page](./create-a-page.md).
Result: Let's see how to Create a page.
Images
Regular Markdown images are supported.
You can use absolute paths to reference images in the static directory (static/img/docusaurus.png
):
![Docusaurus logo](/img/docusaurus.png)
You can reference images relative to the current file as well. This is particularly useful to colocate images close to the Markdown files using them:
![Docusaurus logo](./img/docusaurus.png)
Code Blocks
Markdown code blocks are supported with Syntax highlighting.
```jsx title="src/components/HelloDocusaurus.js"
function HelloDocusaurus() {
return <h1>Hello, Docusaurus!</h1>;
}
```
function HelloDocusaurus() {
return <h1>Hello, Docusaurus!</h1>;
}
Admonitions
Docusaurus has a special syntax to create admonitions and callouts:
:::tip[My tip]
Use this awesome feature option
:::
:::danger[Take care]
This action is dangerous
:::
Use this awesome feature option
This action is dangerous
MDX and React Components
MDX can make your documentation more interactive and allows using any React components inside Markdown:
export const Highlight = ({children, color}) => (
<span
style={{
backgroundColor: color,
borderRadius: '20px',
color: '#fff',
padding: '10px',
cursor: 'pointer',
}}
onClick={() => {
alert(`You clicked the color ${color} with label ${children}`)
}}>
{children}
</span>
);
This is <Highlight color="#25c2a0">Docusaurus green</Highlight> !
This is <Highlight color="#1877F2">Facebook blue</Highlight> !
This is Docusaurus green !
This is Facebook blue !
--------------------------------------------------------------------------------
URL: https://Muham251.github.io/ai-hackathon-1/docs/tutorial-basics/markdown-features#__docusaurus_skipToContent_fallback
Content:
Markdown Features
Docusaurus supports Markdown and a few additional features.
Front Matter
Markdown documents have metadata at the top called Front Matter:
---
id: my-doc-id
title: My document title
description: My document description
slug: /my-custom-url
---
## Markdown heading
Markdown text with [links](./hello.md)
Links
Regular Markdown links are supported, using url paths or relative file paths.
Let's see how to [Create a page](/create-a-page).
Let's see how to [Create a page](./create-a-page.md).
Result: Let's see how to Create a page.
Images
Regular Markdown images are supported.
You can use absolute paths to reference images in the static directory (static/img/docusaurus.png
):
![Docusaurus logo](/img/docusaurus.png)
You can reference images relative to the current file as well. This is particularly useful to colocate images close to the Markdown files using them:
![Docusaurus logo](./img/docusaurus.png)
Code Blocks
Markdown code blocks are supported with Syntax highlighting.
```jsx title="src/components/HelloDocusaurus.js"
function HelloDocusaurus() {
return <h1>Hello, Docusaurus!</h1>;
}
```
function HelloDocusaurus() {
return <h1>Hello, Docusaurus!</h1>;
}
Admonitions
Docusaurus has a special syntax to create admonitions and callouts:
:::tip[My tip]
Use this awesome feature option
:::
:::danger[Take care]
This action is dangerous
:::
Use this awesome feature option
This action is dangerous
MDX and React Components
MDX can make your documentation more interactive and allows using any React components inside Markdown:
export const Highlight = ({children, color}) => (
<span
style={{
backgroundColor: color,
borderRadius: '20px',
color: '#fff',
padding: '10px',
cursor: 'pointer',
}}
onClick={() => {
alert(`You clicked the color ${color} with label ${children}`)
}}>
{children}
</span>
);
This is <Highlight color="#25c2a0">Docusaurus green</Highlight> !
This is <Highlight color="#1877F2">Facebook blue</Highlight> !
This is Docusaurus green !
This is Facebook blue !
--------------------------------------------------------------------------------
URL: https://Muham251.github.io/ai-hackathon-1/docs/tutorial-basics/deploy-your-site
Content:
Deploy your site
Docusaurus is a static-site-generator (also called Jamstack).
It builds your site as simple static HTML, JavaScript and CSS files.
Build your site
Build your site for production:
npm run build
The static files are generated in the build
folder.
Deploy your site
Test your production build locally:
npm run serve
The build
folder is now served at http://localhost:3000/.
You can now deploy the build
folder almost anywhere easily, for free or very small cost (read the Deployment Guide).
--------------------------------------------------------------------------------
URL: https://Muham251.github.io/ai-hackathon-1/docs/tutorial-basics/deploy-your-site#__docusaurus_skipToContent_fallback
Content:
Deploy your site
Docusaurus is a static-site-generator (also called Jamstack).
It builds your site as simple static HTML, JavaScript and CSS files.
Build your site
Build your site for production:
npm run build
The static files are generated in the build
folder.
Deploy your site
Test your production build locally:
npm run serve
The build
folder is now served at http://localhost:3000/.
You can now deploy the build
folder almost anywhere easily, for free or very small cost (read the Deployment Guide).
--------------------------------------------------------------------------------
URL: https://Muham251.github.io/ai-hackathon-1/docs/tutorial-basics/congratulations
Content:
Congratulations!
You have just learned the basics of Docusaurus and made some changes to the initial template.
Docusaurus has much more to offer!
Have 5 more minutes? Take a look at versioning and i18n.
Anything unclear or buggy in this tutorial? Please report it!
What's next?
- Read the official documentation
- Modify your site configuration with
docusaurus.config.js
- Add navbar and footer items with
themeConfig
- Add a custom Design and Layout
- Add a search bar
- Find inspirations in the Docusaurus showcase
- Get involved in the Docusaurus Community
--------------------------------------------------------------------------------
URL: https://Muham251.github.io/ai-hackathon-1/docs/tutorial-basics/congratulations#__docusaurus_skipToContent_fallback
Content:
Congratulations!
You have just learned the basics of Docusaurus and made some changes to the initial template.
Docusaurus has much more to offer!
Have 5 more minutes? Take a look at versioning and i18n.
Anything unclear or buggy in this tutorial? Please report it!
What's next?
- Read the official documentation
- Modify your site configuration with
docusaurus.config.js
- Add navbar and footer items with
themeConfig
- Add a custom Design and Layout
- Add a search bar
- Find inspirations in the Docusaurus showcase
- Get involved in the Docusaurus Community
--------------------------------------------------------------------------------
URL: https://Muham251.github.io/ai-hackathon-1/docs/module-2/
Content:
Spec: Module 2: The Digital Twin (Gazebo & Unity)
1. Business Understanding
1.1. Goal
The goal of this module is to educate students on creating and utilizing digital twins for humanoid robotics. The focus is on mastering physics simulation, environment construction, and sensor simulation within Gazebo and leveraging Unity for high-fidelity rendering.
1.2. Justification
Digital twins are fundamental to modern robotics development, enabling safe, cost-effective, and efficient testing of robot behaviors before deployment. This module provides the essential skills for simulating robots and their interactions with the environment, which is a cornerstone of Physical AI.
1.3. Target Audience
- Primary: Beginner to intermediate students in Physical AI and Humanoid Robotics.
- Secondary: Hobbyists and developers looking to expand their skills into robotics simulation.
2. Functional Requirements
2.1. In Scope (Features)
- Gazebo Physics Simulation:
- Detailed explanation of core physics concepts: gravity, friction, and collision models.
- A tutorial on building a simple environment (e.g., a room with obstacles) using SDF.
- A demonstration of a humanoid robot interacting with the environment.
- Unity for High-Fidelity Rendering:
- An overview of Unity's rendering pipeline and its application in robotics.
- A guide to setting up a simple Unity scene for robotic visualization.
- Sensor Simulation:
- A guide to simulating common robotic sensors: LiDAR, Depth Cameras, and IMUs in both Gazebo and Unity.
- Configuration examples for each sensor.
- Runnable Examples:
- The module must include 2‚Äì3 complete, runnable code examples demonstrating the concepts.
- Format: The final output must be a Docusaurus-compatible Markdown file.
2.2. Out of Scope
- Full-scale game development or advanced Unity features.
- Complex humanoid control algorithms (this is reserved for Module 3).
- Networked or multi-agent simulations.
2.3. Success Criteria
- Knowledge: Students can clearly explain the role of physics engines (Gazebo) and rendering engines (Unity) in creating a digital twin.
- Practical Skill (Gazebo): Students can successfully build a simple Gazebo world and simulate a humanoid robot within it.
- Practical Skill (Unity): Students can set up a Unity scene to render a robot model.
- Practical Skill (Sensors): Students are able to add and configure virtual LiDAR, Depth Camera, and IMU sensors to a simulated robot.
- Completion: The module contains 2-3 fully functional examples that run without errors.
2.4. Constraints
- Word Count: 2000‚Äì3000 words.
- Format: Docusaurus Markdown, including formatted code blocks for SDF, C#, and Python/C++.
- Sources: Content must be based on official documentation from Gazebo, Unity, and relevant sensor simulation libraries.
- Timeline: 1 week for completion.
3. UX/UI (Reader Experience)
3.1. Content Structure
The module will be structured into clear, logical chapters to guide the reader progressively.
- Chapter 1: Gazebo Physics & Environments
- Introduction to Gazebo as a physics simulator.
- Core concepts: gravity, collisions, and inertia.
- Tutorial: Creating a
.world
file with ground plane and basic shapes. - Example: Spawning a humanoid model and observing basic physical interactions.
- Chapter 2: High-Fidelity Rendering with Unity
- The role of Unity in robotics for high-quality visualization.
- Setting up a Unity project for robotics (e.g., using ROS-Unity connectors).
- Example: Importing a URDF model and creating a simple visualization scene.
- Chapter 3: Simulating the Senses
- Introduction to sensor simulation.
- Simulating LiDAR and visualizing point clouds.
- Simulating Depth Cameras and interpreting depth images.
- Simulating an IMU to get orientation and acceleration data.
- Example: A single robot model equipped with all three sensors, with configuration snippets for each.
4. Technical Considerations
4.1. Key Decisions
- Simulation Tools: Gazebo will be used for robust physics simulation, while Unity will be highlighted for its superior rendering capabilities. The text will explain the trade-offs.
- Code Examples: Examples will be provided as self-contained code blocks that can be easily copied and run.
- Structure: The content will follow the chapter structure defined in section 3.1.
4.2. External Dependencies
- The module assumes readers have access to and a basic understanding of a Linux environment (for Gazebo/ROS).
- Readers will need to have Gazebo and Unity Hub (with a recent Unity version) installed.
- Code examples may rely on ROS 2,
rclpy
, orroscpp
for integration, building on concepts from Module 1.
--------------------------------------------------------------------------------
URL: https://Muham251.github.io/ai-hackathon-1/docs/module-2/#__docusaurus_skipToContent_fallback
Content:
Spec: Module 2: The Digital Twin (Gazebo & Unity)
1. Business Understanding
1.1. Goal
The goal of this module is to educate students on creating and utilizing digital twins for humanoid robotics. The focus is on mastering physics simulation, environment construction, and sensor simulation within Gazebo and leveraging Unity for high-fidelity rendering.
1.2. Justification
Digital twins are fundamental to modern robotics development, enabling safe, cost-effective, and efficient testing of robot behaviors before deployment. This module provides the essential skills for simulating robots and their interactions with the environment, which is a cornerstone of Physical AI.
1.3. Target Audience
- Primary: Beginner to intermediate students in Physical AI and Humanoid Robotics.
- Secondary: Hobbyists and developers looking to expand their skills into robotics simulation.
2. Functional Requirements
2.1. In Scope (Features)
- Gazebo Physics Simulation:
- Detailed explanation of core physics concepts: gravity, friction, and collision models.
- A tutorial on building a simple environment (e.g., a room with obstacles) using SDF.
- A demonstration of a humanoid robot interacting with the environment.
- Unity for High-Fidelity Rendering:
- An overview of Unity's rendering pipeline and its application in robotics.
- A guide to setting up a simple Unity scene for robotic visualization.
- Sensor Simulation:
- A guide to simulating common robotic sensors: LiDAR, Depth Cameras, and IMUs in both Gazebo and Unity.
- Configuration examples for each sensor.
- Runnable Examples:
- The module must include 2‚Äì3 complete, runnable code examples demonstrating the concepts.
- Format: The final output must be a Docusaurus-compatible Markdown file.
2.2. Out of Scope
- Full-scale game development or advanced Unity features.
- Complex humanoid control algorithms (this is reserved for Module 3).
- Networked or multi-agent simulations.
2.3. Success Criteria
- Knowledge: Students can clearly explain the role of physics engines (Gazebo) and rendering engines (Unity) in creating a digital twin.
- Practical Skill (Gazebo): Students can successfully build a simple Gazebo world and simulate a humanoid robot within it.
- Practical Skill (Unity): Students can set up a Unity scene to render a robot model.
- Practical Skill (Sensors): Students are able to add and configure virtual LiDAR, Depth Camera, and IMU sensors to a simulated robot.
- Completion: The module contains 2-3 fully functional examples that run without errors.
2.4. Constraints
- Word Count: 2000‚Äì3000 words.
- Format: Docusaurus Markdown, including formatted code blocks for SDF, C#, and Python/C++.
- Sources: Content must be based on official documentation from Gazebo, Unity, and relevant sensor simulation libraries.
- Timeline: 1 week for completion.
3. UX/UI (Reader Experience)
3.1. Content Structure
The module will be structured into clear, logical chapters to guide the reader progressively.
- Chapter 1: Gazebo Physics & Environments
- Introduction to Gazebo as a physics simulator.
- Core concepts: gravity, collisions, and inertia.
- Tutorial: Creating a
.world
file with ground plane and basic shapes. - Example: Spawning a humanoid model and observing basic physical interactions.
- Chapter 2: High-Fidelity Rendering with Unity
- The role of Unity in robotics for high-quality visualization.
- Setting up a Unity project for robotics (e.g., using ROS-Unity connectors).
- Example: Importing a URDF model and creating a simple visualization scene.
- Chapter 3: Simulating the Senses
- Introduction to sensor simulation.
- Simulating LiDAR and visualizing point clouds.
- Simulating Depth Cameras and interpreting depth images.
- Simulating an IMU to get orientation and acceleration data.
- Example: A single robot model equipped with all three sensors, with configuration snippets for each.
4. Technical Considerations
4.1. Key Decisions
- Simulation Tools: Gazebo will be used for robust physics simulation, while Unity will be highlighted for its superior rendering capabilities. The text will explain the trade-offs.
- Code Examples: Examples will be provided as self-contained code blocks that can be easily copied and run.
- Structure: The content will follow the chapter structure defined in section 3.1.
4.2. External Dependencies
- The module assumes readers have access to and a basic understanding of a Linux environment (for Gazebo/ROS).
- Readers will need to have Gazebo and Unity Hub (with a recent Unity version) installed.
- Code examples may rely on ROS 2,
rclpy
, orroscpp
for integration, building on concepts from Module 1.
--------------------------------------------------------------------------------
URL: https://Muham251.github.io/ai-hackathon-1/docs/module-2/gazebo-physics-environments
Content:
Chapter 1: Gazebo Physics & Environments
- Introduction to Gazebo as a physics simulator.
- Core concepts: gravity, collisions, and inertia.
- Tutorial: Creating a
.world
file with ground plane and basic shapes. - Example: Spawning a humanoid model and observing basic physical interactions.
--------------------------------------------------------------------------------
URL: https://Muham251.github.io/ai-hackathon-1/docs/module-2/gazebo-physics-environments#__docusaurus_skipToContent_fallback
Content:
Chapter 1: Gazebo Physics & Environments
- Introduction to Gazebo as a physics simulator.
- Core concepts: gravity, collisions, and inertia.
- Tutorial: Creating a
.world
file with ground plane and basic shapes. - Example: Spawning a humanoid model and observing basic physical interactions.
--------------------------------------------------------------------------------
URL: https://Muham251.github.io/ai-hackathon-1/docs/module-2/high-fidelity-rendering-with-unity
Content:
Chapter 2: High-Fidelity Rendering with Unity
- The role of Unity in robotics for high-quality visualization.
- Setting up a Unity project for robotics (e.g., using ROS-Unity connectors).
- Example: Importing a URDF model and creating a simple visualization scene.
--------------------------------------------------------------------------------
URL: https://Muham251.github.io/ai-hackathon-1/docs/module-2/high-fidelity-rendering-with-unity#__docusaurus_skipToContent_fallback
Content:
Chapter 2: High-Fidelity Rendering with Unity
- The role of Unity in robotics for high-quality visualization.
- Setting up a Unity project for robotics (e.g., using ROS-Unity connectors).
- Example: Importing a URDF model and creating a simple visualization scene.
--------------------------------------------------------------------------------
URL: https://Muham251.github.io/ai-hackathon-1/docs/module-2/simulating-the-senses
Content:
Chapter 3: Simulating the Senses
- Introduction to sensor simulation.
- Simulating LiDAR and visualizing point clouds.
- Simulating Depth Cameras and interpreting depth images.
- Simulating an IMU to get orientation and acceleration data.
- Example: A single robot model equipped with all three sensors, with configuration snippets for each.
--------------------------------------------------------------------------------
URL: https://Muham251.github.io/ai-hackathon-1/docs/module-2/simulating-the-senses#__docusaurus_skipToContent_fallback
Content:
Chapter 3: Simulating the Senses
- Introduction to sensor simulation.
- Simulating LiDAR and visualizing point clouds.
- Simulating Depth Cameras and interpreting depth images.
- Simulating an IMU to get orientation and acceleration data.
- Example: A single robot model equipped with all three sensors, with configuration snippets for each.
--------------------------------------------------------------------------------
URL: https://Muham251.github.io/ai-hackathon-1/docs/category/tutorial---extras
Content:
üìÑÔ∏è Manage Docs VersionsDocusaurus can manage multiple versions of your docs.üìÑÔ∏è Translate your siteLet's translate docs/intro.md to French.
--------------------------------------------------------------------------------
URL: https://Muham251.github.io/ai-hackathon-1/docs/category/tutorial---extras#__docusaurus_skipToContent_fallback
Content:
üìÑÔ∏è Manage Docs VersionsDocusaurus can manage multiple versions of your docs.üìÑÔ∏è Translate your siteLet's translate docs/intro.md to French.
--------------------------------------------------------------------------------
URL: https://Muham251.github.io/ai-hackathon-1/docs/tutorial-extras/manage-docs-versions
Content:
Manage Docs Versions
Docusaurus can manage multiple versions of your docs.
Create a docs version
Release a version 1.0 of your project:
npm run docusaurus docs:version 1.0
The docs
folder is copied into versioned_docs/version-1.0
and versions.json
is created.
Your docs now have 2 versions:
1.0
athttp://localhost:3000/docs/
for the version 1.0 docscurrent
athttp://localhost:3000/docs/next/
for the upcoming, unreleased docs
Add a Version Dropdown
To navigate seamlessly across versions, add a version dropdown.
Modify the docusaurus.config.js
file:
docusaurus.config.js
export default {
themeConfig: {
navbar: {
items: [
{
type: 'docsVersionDropdown',
},
],
},
},
};
The docs version dropdown appears in your navbar:
Update an existing version
It is possible to edit versioned docs in their respective folder:
versioned_docs/version-1.0/hello.md
updateshttp://localhost:3000/docs/hello
docs/hello.md
updateshttp://localhost:3000/docs/next/hello
--------------------------------------------------------------------------------
URL: https://Muham251.github.io/ai-hackathon-1/docs/tutorial-extras/manage-docs-versions#__docusaurus_skipToContent_fallback
Content:
Manage Docs Versions
Docusaurus can manage multiple versions of your docs.
Create a docs version
Release a version 1.0 of your project:
npm run docusaurus docs:version 1.0
The docs
folder is copied into versioned_docs/version-1.0
and versions.json
is created.
Your docs now have 2 versions:
1.0
athttp://localhost:3000/docs/
for the version 1.0 docscurrent
athttp://localhost:3000/docs/next/
for the upcoming, unreleased docs
Add a Version Dropdown
To navigate seamlessly across versions, add a version dropdown.
Modify the docusaurus.config.js
file:
docusaurus.config.js
export default {
themeConfig: {
navbar: {
items: [
{
type: 'docsVersionDropdown',
},
],
},
},
};
The docs version dropdown appears in your navbar:
Update an existing version
It is possible to edit versioned docs in their respective folder:
versioned_docs/version-1.0/hello.md
updateshttp://localhost:3000/docs/hello
docs/hello.md
updateshttp://localhost:3000/docs/next/hello
--------------------------------------------------------------------------------
URL: https://Muham251.github.io/ai-hackathon-1/docs/tutorial-extras/translate-your-site
Content:
Translate your site
Let's translate docs/intro.md
to French.
Configure i18n
Modify docusaurus.config.js
to add support for the fr
locale:
docusaurus.config.js
export default {
i18n: {
defaultLocale: 'en',
locales: ['en', 'fr'],
},
};
Translate a doc
Copy the docs/intro.md
file to the i18n/fr
folder:
mkdir -p i18n/fr/docusaurus-plugin-content-docs/current/
cp docs/intro.md i18n/fr/docusaurus-plugin-content-docs/current/intro.md
Translate i18n/fr/docusaurus-plugin-content-docs/current/intro.md
in French.
Start your localized site
Start your site on the French locale:
npm run start -- --locale fr
Your localized site is accessible at http://localhost:3000/fr/ and the Getting Started
page is translated.
caution
In development, you can only use one locale at a time.
Add a Locale Dropdown
To navigate seamlessly across languages, add a locale dropdown.
Modify the docusaurus.config.js
file:
docusaurus.config.js
export default {
themeConfig: {
navbar: {
items: [
{
type: 'localeDropdown',
},
],
},
},
};
The locale dropdown now appears in your navbar:
Build your localized site
Build your site for a specific locale:
npm run build -- --locale fr
Or build your site to include all the locales at once:
npm run build
--------------------------------------------------------------------------------
URL: https://Muham251.github.io/ai-hackathon-1/docs/tutorial-extras/translate-your-site#__docusaurus_skipToContent_fallback
Content:
Translate your site
Let's translate docs/intro.md
to French.
Configure i18n
Modify docusaurus.config.js
to add support for the fr
locale:
docusaurus.config.js
export default {
i18n: {
defaultLocale: 'en',
locales: ['en', 'fr'],
},
};
Translate a doc
Copy the docs/intro.md
file to the i18n/fr
folder:
mkdir -p i18n/fr/docusaurus-plugin-content-docs/current/
cp docs/intro.md i18n/fr/docusaurus-plugin-content-docs/current/intro.md
Translate i18n/fr/docusaurus-plugin-content-docs/current/intro.md
in French.
Start your localized site
Start your site on the French locale:
npm run start -- --locale fr
Your localized site is accessible at http://localhost:3000/fr/ and the Getting Started
page is translated.
caution
In development, you can only use one locale at a time.
Add a Locale Dropdown
To navigate seamlessly across languages, add a locale dropdown.
Modify the docusaurus.config.js
file:
docusaurus.config.js
export default {
themeConfig: {
navbar: {
items: [
{
type: 'localeDropdown',
},
],
},
},
};
The locale dropdown now appears in your navbar:
Build your localized site
Build your site for a specific locale:
npm run build -- --locale fr
Or build your site to include all the locales at once:
npm run build
--------------------------------------------------------------------------------
URL: https://Muham251.github.io/ai-hackathon-1/docs/module-3/
Content:
Spec: Module 3: The AI-Robot Brain (NVIDIA Isaac‚Ñ¢)
1. Business Understanding
1.1. Goal
The objective of this module is to educate beginner to intermediate robotics students on leveraging the NVIDIA Isaac‚Ñ¢ ecosystem to build a robot's core intelligence. The curriculum will focus on Isaac Sim for photorealistic simulation, Isaac ROS for perception and navigation (specifically VSLAM), and Nav2 for autonomous path planning.
1.2. Justification
The NVIDIA Isaac‚Ñ¢ platform is at the forefront of AI-driven robotics, offering powerful tools for developing and testing intelligent robots in realistic virtual environments. This module provides learners with critical, industry-relevant skills in simulation, perception, and navigation, forming the foundation of a robot's "brain."
1.3. Target Audience
- Primary: Beginner to intermediate students in robotics and AI who have a foundational understanding of ROS and simulation concepts.
- Secondary: Developers and engineers looking to get started with the NVIDIA Isaac‚Ñ¢ toolchain for robotics projects.
2. Functional Requirements
2.1. In Scope (Features)
- Isaac Sim Fundamentals:
- An introduction to creating scenes in Isaac Sim.
- A guide on generating synthetic data (e.g., camera images, LiDAR data) for training and testing.
- Isaac ROS Workflow:
- A conceptual overview of the Visual SLAM (VSLAM) and navigation pipeline provided by Isaac ROS.
- A tutorial on setting up and running the Isaac ROS navigation stack on a simulated robot.
- Nav2 Path Planning:
- A practical example demonstrating how to use Nav2 for path planning with a humanoid robot in Isaac Sim.
- Runnable Demos:
- The module must include 2‚Äì3 complete, runnable demonstrations that users can execute.
- Format: The final content will be delivered as a Docusaurus-compatible Markdown file.
2.2. Out of Scope
- A complete, end-to-end humanoid control pipeline (e.g., manipulation, complex behaviors).
- The development or deep-dive analysis of custom SLAM or navigation algorithms.
- Advanced Isaac Sim features like custom Python scripting or detailed RTX rendering settings.
2.3. Success Criteria
- Understanding: Learners can explain the significance of photorealistic simulation and synthetic data generation for robotics.
- Explanation: Learners can describe the high-level workflow of the Isaac ROS VSLAM and navigation stack.
- Application: Learners can successfully launch and run a basic Nav2 path-planning demonstration within Isaac Sim.
- Completion: The module is published with 2‚Äì3 fully functional and validated demos.
2.4. Constraints
- Word Count: Approximately 2000‚Äì3000 words.
- Format: Docusaurus Markdown with appropriate code blocks for commands and configurations.
- Sources: All technical information must be derived from and cite official NVIDIA Isaac‚Ñ¢ documentation.
- Timeline: The module is to be completed within a 1-week timeframe.
3. UX/UI (Reader Experience)
3.1. Content Structure
The module will be organized into a clear, three-chapter structure to guide the learner from environment creation to autonomous navigation.
- Chapter 1: Isaac Sim Basics: Scenes & Synthetic Data
- Introduction to the Isaac Sim interface and its core concepts.
- Tutorial: Setting up a basic scene with a robot and obstacles.
- Guide: Generating and exporting synthetic sensor data from the simulation.
- Chapter 2: Isaac ROS: VSLAM & Navigation Workflow
- Overview of the Isaac ROS ecosystem and its advantages.
- Step-by-step guide to launching the Isaac ROS Docker container.
- Tutorial: Running the VSLAM and navigation stack on a robot within Isaac Sim.
- Chapter 3: Nav2: Path Planning for Humanoids
- Introduction to Nav2 and its role in autonomous navigation.
- Example: Sending a navigation goal to a humanoid robot and having it execute a path using Nav2.
4. Technical Considerations
4.1. Key Decisions
- Ecosystem Focus: This module will exclusively use the NVIDIA Isaac‚Ñ¢ toolchain to provide a cohesive learning experience.
- Runnable Demos: Demos will be provided as shell scripts or clear, step-by-step command sequences to ensure they are easy to run.
- Configuration over Code: The focus will be on configuring and running existing Isaac ROS packages rather than writing new code.
4.2. External Dependencies
- Hardware: A computer with a compatible NVIDIA GPU is required.
- Software: Users must have NVIDIA Isaac Sim, Docker, and NVIDIA Container Toolkit installed.
- Prerequisites: A foundational knowledge of ROS 2 concepts is assumed (as taught in Module 1).
--------------------------------------------------------------------------------
URL: https://Muham251.github.io/ai-hackathon-1/docs/module-3/#__docusaurus_skipToContent_fallback
Content:
Spec: Module 3: The AI-Robot Brain (NVIDIA Isaac‚Ñ¢)
1. Business Understanding
1.1. Goal
The objective of this module is to educate beginner to intermediate robotics students on leveraging the NVIDIA Isaac‚Ñ¢ ecosystem to build a robot's core intelligence. The curriculum will focus on Isaac Sim for photorealistic simulation, Isaac ROS for perception and navigation (specifically VSLAM), and Nav2 for autonomous path planning.
1.2. Justification
The NVIDIA Isaac‚Ñ¢ platform is at the forefront of AI-driven robotics, offering powerful tools for developing and testing intelligent robots in realistic virtual environments. This module provides learners with critical, industry-relevant skills in simulation, perception, and navigation, forming the foundation of a robot's "brain."
1.3. Target Audience
- Primary: Beginner to intermediate students in robotics and AI who have a foundational understanding of ROS and simulation concepts.
- Secondary: Developers and engineers looking to get started with the NVIDIA Isaac‚Ñ¢ toolchain for robotics projects.
2. Functional Requirements
2.1. In Scope (Features)
- Isaac Sim Fundamentals:
- An introduction to creating scenes in Isaac Sim.
- A guide on generating synthetic data (e.g., camera images, LiDAR data) for training and testing.
- Isaac ROS Workflow:
- A conceptual overview of the Visual SLAM (VSLAM) and navigation pipeline provided by Isaac ROS.
- A tutorial on setting up and running the Isaac ROS navigation stack on a simulated robot.
- Nav2 Path Planning:
- A practical example demonstrating how to use Nav2 for path planning with a humanoid robot in Isaac Sim.
- Runnable Demos:
- The module must include 2‚Äì3 complete, runnable demonstrations that users can execute.
- Format: The final content will be delivered as a Docusaurus-compatible Markdown file.
2.2. Out of Scope
- A complete, end-to-end humanoid control pipeline (e.g., manipulation, complex behaviors).
- The development or deep-dive analysis of custom SLAM or navigation algorithms.
- Advanced Isaac Sim features like custom Python scripting or detailed RTX rendering settings.
2.3. Success Criteria
- Understanding: Learners can explain the significance of photorealistic simulation and synthetic data generation for robotics.
- Explanation: Learners can describe the high-level workflow of the Isaac ROS VSLAM and navigation stack.
- Application: Learners can successfully launch and run a basic Nav2 path-planning demonstration within Isaac Sim.
- Completion: The module is published with 2‚Äì3 fully functional and validated demos.
2.4. Constraints
- Word Count: Approximately 2000‚Äì3000 words.
- Format: Docusaurus Markdown with appropriate code blocks for commands and configurations.
- Sources: All technical information must be derived from and cite official NVIDIA Isaac‚Ñ¢ documentation.
- Timeline: The module is to be completed within a 1-week timeframe.
3. UX/UI (Reader Experience)
3.1. Content Structure
The module will be organized into a clear, three-chapter structure to guide the learner from environment creation to autonomous navigation.
- Chapter 1: Isaac Sim Basics: Scenes & Synthetic Data
- Introduction to the Isaac Sim interface and its core concepts.
- Tutorial: Setting up a basic scene with a robot and obstacles.
- Guide: Generating and exporting synthetic sensor data from the simulation.
- Chapter 2: Isaac ROS: VSLAM & Navigation Workflow
- Overview of the Isaac ROS ecosystem and its advantages.
- Step-by-step guide to launching the Isaac ROS Docker container.
- Tutorial: Running the VSLAM and navigation stack on a robot within Isaac Sim.
- Chapter 3: Nav2: Path Planning for Humanoids
- Introduction to Nav2 and its role in autonomous navigation.
- Example: Sending a navigation goal to a humanoid robot and having it execute a path using Nav2.
4. Technical Considerations
4.1. Key Decisions
- Ecosystem Focus: This module will exclusively use the NVIDIA Isaac‚Ñ¢ toolchain to provide a cohesive learning experience.
- Runnable Demos: Demos will be provided as shell scripts or clear, step-by-step command sequences to ensure they are easy to run.
- Configuration over Code: The focus will be on configuring and running existing Isaac ROS packages rather than writing new code.
4.2. External Dependencies
- Hardware: A computer with a compatible NVIDIA GPU is required.
- Software: Users must have NVIDIA Isaac Sim, Docker, and NVIDIA Container Toolkit installed.
- Prerequisites: A foundational knowledge of ROS 2 concepts is assumed (as taught in Module 1).
--------------------------------------------------------------------------------
URL: https://Muham251.github.io/ai-hackathon-1/docs/module-3/isaac-sim-basics
Content:
Chapter 1: Isaac Sim Basics: Scenes & Synthetic Data
- Introduction to the Isaac Sim interface and its core concepts.
- Tutorial: Setting up a basic scene with a robot and obstacles.
- Guide: Generating and exporting synthetic sensor data from the simulation.
--------------------------------------------------------------------------------
URL: https://Muham251.github.io/ai-hackathon-1/docs/module-3/isaac-sim-basics#__docusaurus_skipToContent_fallback
Content:
Chapter 1: Isaac Sim Basics: Scenes & Synthetic Data
- Introduction to the Isaac Sim interface and its core concepts.
- Tutorial: Setting up a basic scene with a robot and obstacles.
- Guide: Generating and exporting synthetic sensor data from the simulation.
--------------------------------------------------------------------------------
URL: https://Muham251.github.io/ai-hackathon-1/docs/module-3/isaac-ros-vslam-navigation
Content:
Module 3: The AI-Robot Brain (NVIDIA Isaac‚Ñ¢)Chapter 2: Isaac ROS: VSLAM & Navigation WorkflowChapter 2: Isaac ROS: VSLAM & Navigation Workflow Overview of the Isaac ROS ecosystem and its advantages. Step-by-step guide to launching the Isaac ROS Docker container. Tutorial: Running the VSLAM and navigation stack on a robot within Isaac Sim.
--------------------------------------------------------------------------------
URL: https://Muham251.github.io/ai-hackathon-1/docs/module-3/isaac-ros-vslam-navigation#__docusaurus_skipToContent_fallback
Content:
Module 3: The AI-Robot Brain (NVIDIA Isaac‚Ñ¢)Chapter 2: Isaac ROS: VSLAM & Navigation WorkflowChapter 2: Isaac ROS: VSLAM & Navigation Workflow Overview of the Isaac ROS ecosystem and its advantages. Step-by-step guide to launching the Isaac ROS Docker container. Tutorial: Running the VSLAM and navigation stack on a robot within Isaac Sim.
--------------------------------------------------------------------------------
URL: https://Muham251.github.io/ai-hackathon-1/docs/module-3/nav2-path-planning
Content:
Module 3: The AI-Robot Brain (NVIDIA Isaac‚Ñ¢)Chapter 3: Nav2: Path Planning for HumanoidsChapter 3: Nav2: Path Planning for Humanoids Introduction to Nav2 and its role in autonomous navigation. Example: Sending a navigation goal to a humanoid robot and having it execute a path using Nav2.
--------------------------------------------------------------------------------
URL: https://Muham251.github.io/ai-hackathon-1/docs/module-3/nav2-path-planning#__docusaurus_skipToContent_fallback
Content:
Module 3: The AI-Robot Brain (NVIDIA Isaac‚Ñ¢)Chapter 3: Nav2: Path Planning for HumanoidsChapter 3: Nav2: Path Planning for Humanoids Introduction to Nav2 and its role in autonomous navigation. Example: Sending a navigation goal to a humanoid robot and having it execute a path using Nav2.
--------------------------------------------------------------------------------
URL: https://Muham251.github.io/ai-hackathon-1/docs/module-4/
Content:
Spec: Module 4: Vision-Language-Action (VLA)
1. Business Understanding
1.1. Goal
The primary goal of this module is to teach students how to build a complete Vision-Language-Action (VLA) pipeline. This involves integrating Large Language Models (LLMs) with robotics, using Whisper for voice command input, employing LLMs for cognitive planning, and executing tasks through ROS 2 actions.
1.2. Justification
VLA is a transformative paradigm in robotics, enabling more natural human-robot interaction and a higher degree of autonomy. This module provides a crucial bridge between theoretical AI concepts (like LLMs) and practical, physical robotics, equipping students with highly sought-after skills in creating intelligent, responsive robots.
1.3. Target Audience
- Primary: Beginner to intermediate students in robotics and AI.
- Secondary: Developers and researchers interested in the practical application of LLMs in autonomous systems.
2. Functional Requirements
2.1. In Scope (Features)
- VLA Pipeline Overview: A clear, conceptual explanation of the entire Vision-Language-Action pipeline, from voice input to robot action.
- Voice Command Processing: A tutorial on using OpenAI Whisper to transcribe spoken language into text and extract actionable commands.
- LLM-based Planning: A guide on how to prompt an LLM to translate a high-level natural language task (e.g., "get me the apple") into a sequence of concrete ROS 2 action goals.
- Mini-Capstone Project: A complete, runnable example demonstrating the full VLA loop. A user will issue a voice command, and a simulated humanoid robot will plan and execute a simple multi-step task involving navigation, perception, and manipulation.
2.2. Out of Scope
- The development of a production-grade, highly reliable robotics stack.
- Training or fine-tuning custom Large Language Models.
- Complex, long-horizon tasks or navigation in dynamic, multi-room environments.
2.3. Success Criteria
- Understanding: Students can draw a diagram of the VLA pipeline and explain the role of each component (Voice, LLM, ROS 2).
- Application (Whisper): Students are able to run a demo that successfully converts a spoken command into a text string.
- Application (LLM Planning): Students can explain how a natural language goal is decomposed into a JSON or YAML sequence of ROS 2 actions by an LLM.
- Completion: The mini-capstone example runs end-to-end: a voice command triggers a sequence of autonomous actions in the simulated robot.
2.4. Constraints
- Word Count: 2000‚Äì3000 words.
- Format: Docusaurus Markdown, including code snippets for Python (for Whisper and LLM interaction) and configuration files (for ROS 2 actions).
- Sources: Content will be based on official OpenAI Whisper documentation, ROS 2 action design patterns, and foundational VLA research.
- Timeline: 1 week.
3. UX/UI (Reader Experience)
3.1. Content Structure
The module will be structured to guide the learner from individual components to a fully integrated system.
- Chapter 1: Voice-to-Action: Whisper and Command Extraction
- Introduction to Whisper for speech-to-text.
- Tutorial: Setting up Whisper and running a simple voice transcription demo.
- Guide: Extracting key intents and entities from the transcribed text (e.g., turning "Can you please find the red block" into
{action: "find", object: "red block"}
).
- Chapter 2: Cognitive Planning with LLMs
- The role of LLMs as a "reasoning engine" for robots.
- Tutorial: Prompt engineering techniques to make an LLM translate a task into a machine-readable plan (a sequence of ROS 2 actions).
- Example: Converting
{action: "find", object: "red block"}
into a plan like[{navigate: "table"}, {detect: "red_block"}]
.
- Chapter 3: Mini-Capstone: The Full VLA Loop
- An overview of the complete system architecture.
- A step-by-step guide to launching all components (Whisper listener, LLM planner, ROS 2 action servers).
- A full demonstration of an autonomous task: e.g., "Go to the table, find the blue cup, and pick it up."
4. Technical Considerations
4.1. Key Decisions
- LLM Abstraction: We will use a major LLM provider's API (e.g., OpenAI, Anthropic, or Google) to avoid the complexity of self-hosting. The focus is on the integration, not the model itself.
- ROS 2 Actions: The module will use the standard ROS 2 action interface for all robot tasks (e.g.,
NavigateToPose
,DetectObject
,PickAndPlace
). This ensures a modular and scalable design. - Focus on Integration: The code provided will be primarily "glue code" written in Python, connecting the various components of the VLA pipeline.
4.2. External Dependencies
- Software: Python 3, ROS 2, and the OpenAI Whisper library.
- APIs: An API key for an LLM provider will be required for the cognitive planning chapter.
- Hardware: A microphone for voice commands.
- Prerequisites: Completion of prior modules, including ROS 2 basics, simulation, and navigation.
--------------------------------------------------------------------------------
URL: https://Muham251.github.io/ai-hackathon-1/docs/module-4/#__docusaurus_skipToContent_fallback
Content:
Spec: Module 4: Vision-Language-Action (VLA)
1. Business Understanding
1.1. Goal
The primary goal of this module is to teach students how to build a complete Vision-Language-Action (VLA) pipeline. This involves integrating Large Language Models (LLMs) with robotics, using Whisper for voice command input, employing LLMs for cognitive planning, and executing tasks through ROS 2 actions.
1.2. Justification
VLA is a transformative paradigm in robotics, enabling more natural human-robot interaction and a higher degree of autonomy. This module provides a crucial bridge between theoretical AI concepts (like LLMs) and practical, physical robotics, equipping students with highly sought-after skills in creating intelligent, responsive robots.
1.3. Target Audience
- Primary: Beginner to intermediate students in robotics and AI.
- Secondary: Developers and researchers interested in the practical application of LLMs in autonomous systems.
2. Functional Requirements
2.1. In Scope (Features)
- VLA Pipeline Overview: A clear, conceptual explanation of the entire Vision-Language-Action pipeline, from voice input to robot action.
- Voice Command Processing: A tutorial on using OpenAI Whisper to transcribe spoken language into text and extract actionable commands.
- LLM-based Planning: A guide on how to prompt an LLM to translate a high-level natural language task (e.g., "get me the apple") into a sequence of concrete ROS 2 action goals.
- Mini-Capstone Project: A complete, runnable example demonstrating the full VLA loop. A user will issue a voice command, and a simulated humanoid robot will plan and execute a simple multi-step task involving navigation, perception, and manipulation.
2.2. Out of Scope
- The development of a production-grade, highly reliable robotics stack.
- Training or fine-tuning custom Large Language Models.
- Complex, long-horizon tasks or navigation in dynamic, multi-room environments.
2.3. Success Criteria
- Understanding: Students can draw a diagram of the VLA pipeline and explain the role of each component (Voice, LLM, ROS 2).
- Application (Whisper): Students are able to run a demo that successfully converts a spoken command into a text string.
- Application (LLM Planning): Students can explain how a natural language goal is decomposed into a JSON or YAML sequence of ROS 2 actions by an LLM.
- Completion: The mini-capstone example runs end-to-end: a voice command triggers a sequence of autonomous actions in the simulated robot.
2.4. Constraints
- Word Count: 2000‚Äì3000 words.
- Format: Docusaurus Markdown, including code snippets for Python (for Whisper and LLM interaction) and configuration files (for ROS 2 actions).
- Sources: Content will be based on official OpenAI Whisper documentation, ROS 2 action design patterns, and foundational VLA research.
- Timeline: 1 week.
3. UX/UI (Reader Experience)
3.1. Content Structure
The module will be structured to guide the learner from individual components to a fully integrated system.
- Chapter 1: Voice-to-Action: Whisper and Command Extraction
- Introduction to Whisper for speech-to-text.
- Tutorial: Setting up Whisper and running a simple voice transcription demo.
- Guide: Extracting key intents and entities from the transcribed text (e.g., turning "Can you please find the red block" into
{action: "find", object: "red block"}
).
- Chapter 2: Cognitive Planning with LLMs
- The role of LLMs as a "reasoning engine" for robots.
- Tutorial: Prompt engineering techniques to make an LLM translate a task into a machine-readable plan (a sequence of ROS 2 actions).
- Example: Converting
{action: "find", object: "red block"}
into a plan like[{navigate: "table"}, {detect: "red_block"}]
.
- Chapter 3: Mini-Capstone: The Full VLA Loop
- An overview of the complete system architecture.
- A step-by-step guide to launching all components (Whisper listener, LLM planner, ROS 2 action servers).
- A full demonstration of an autonomous task: e.g., "Go to the table, find the blue cup, and pick it up."
4. Technical Considerations
4.1. Key Decisions
- LLM Abstraction: We will use a major LLM provider's API (e.g., OpenAI, Anthropic, or Google) to avoid the complexity of self-hosting. The focus is on the integration, not the model itself.
- ROS 2 Actions: The module will use the standard ROS 2 action interface for all robot tasks (e.g.,
NavigateToPose
,DetectObject
,PickAndPlace
). This ensures a modular and scalable design. - Focus on Integration: The code provided will be primarily "glue code" written in Python, connecting the various components of the VLA pipeline.
4.2. External Dependencies
- Software: Python 3, ROS 2, and the OpenAI Whisper library.
- APIs: An API key for an LLM provider will be required for the cognitive planning chapter.
- Hardware: A microphone for voice commands.
- Prerequisites: Completion of prior modules, including ROS 2 basics, simulation, and navigation.
--------------------------------------------------------------------------------
URL: https://Muham251.github.io/ai-hackathon-1/docs/module-4/voice-to-action
Content:
Chapter 1: Voice-to-Action: Whisper and Command Extraction
- Introduction to Whisper for speech-to-text.
- Tutorial: Setting up Whisper and running a simple voice transcription demo.
- Guide: Extracting key intents and entities from the transcribed text (e.g., turning "Can you please find the red block" into
{action: "find", object: "red block"}
).
--------------------------------------------------------------------------------
URL: https://Muham251.github.io/ai-hackathon-1/docs/module-4/voice-to-action#__docusaurus_skipToContent_fallback
Content:
Chapter 1: Voice-to-Action: Whisper and Command Extraction
- Introduction to Whisper for speech-to-text.
- Tutorial: Setting up Whisper and running a simple voice transcription demo.
- Guide: Extracting key intents and entities from the transcribed text (e.g., turning "Can you please find the red block" into
{action: "find", object: "red block"}
).
--------------------------------------------------------------------------------
URL: https://Muham251.github.io/ai-hackathon-1/docs/module-4/cognitive-planning-with-llms
Content:
Chapter 2: Cognitive Planning with LLMs
- The role of LLMs as a "reasoning engine" for robots.
- Tutorial: Prompt engineering techniques to make an LLM translate a task into a machine-readable plan (a sequence of ROS 2 actions).
- Example: Converting
{action: "find", object: "red block"}
into a plan like[{navigate: "table"}, {detect: "red_block"}]
.
--------------------------------------------------------------------------------
URL: https://Muham251.github.io/ai-hackathon-1/docs/module-4/cognitive-planning-with-llms#__docusaurus_skipToContent_fallback
Content:
Chapter 2: Cognitive Planning with LLMs
- The role of LLMs as a "reasoning engine" for robots.
- Tutorial: Prompt engineering techniques to make an LLM translate a task into a machine-readable plan (a sequence of ROS 2 actions).
- Example: Converting
{action: "find", object: "red block"}
into a plan like[{navigate: "table"}, {detect: "red_block"}]
.
--------------------------------------------------------------------------------
URL: https://Muham251.github.io/ai-hackathon-1/docs/module-4/mini-capstone
Content:
Chapter 3: Mini-Capstone: The Full VLA Loop
- An overview of the complete system architecture.
- A step-by-step guide to launching all components (Whisper listener, LLM planner, ROS 2 action servers).
- A full demonstration of an autonomous task: e.g., "Go to the table, find the blue cup, and pick it up."
--------------------------------------------------------------------------------
URL: https://Muham251.github.io/ai-hackathon-1/docs/module-4/mini-capstone#__docusaurus_skipToContent_fallback
Content:
Chapter 3: Mini-Capstone: The Full VLA Loop
- An overview of the complete system architecture.
- A step-by-step guide to launching all components (Whisper listener, LLM planner, ROS 2 action servers).
- A full demonstration of an autonomous task: e.g., "Go to the table, find the blue cup, and pick it up."
--------------------------------------------------------------------------------
URL: https://Muham251.github.io/ai-hackathon-1/docs/module-4/#1-business-understanding
Content:
Spec: Module 4: Vision-Language-Action (VLA)
1. Business Understanding
1.1. Goal
The primary goal of this module is to teach students how to build a complete Vision-Language-Action (VLA) pipeline. This involves integrating Large Language Models (LLMs) with robotics, using Whisper for voice command input, employing LLMs for cognitive planning, and executing tasks through ROS 2 actions.
1.2. Justification
VLA is a transformative paradigm in robotics, enabling more natural human-robot interaction and a higher degree of autonomy. This module provides a crucial bridge between theoretical AI concepts (like LLMs) and practical, physical robotics, equipping students with highly sought-after skills in creating intelligent, responsive robots.
1.3. Target Audience
- Primary: Beginner to intermediate students in robotics and AI.
- Secondary: Developers and researchers interested in the practical application of LLMs in autonomous systems.
2. Functional Requirements
2.1. In Scope (Features)
- VLA Pipeline Overview: A clear, conceptual explanation of the entire Vision-Language-Action pipeline, from voice input to robot action.
- Voice Command Processing: A tutorial on using OpenAI Whisper to transcribe spoken language into text and extract actionable commands.
- LLM-based Planning: A guide on how to prompt an LLM to translate a high-level natural language task (e.g., "get me the apple") into a sequence of concrete ROS 2 action goals.
- Mini-Capstone Project: A complete, runnable example demonstrating the full VLA loop. A user will issue a voice command, and a simulated humanoid robot will plan and execute a simple multi-step task involving navigation, perception, and manipulation.
2.2. Out of Scope
- The development of a production-grade, highly reliable robotics stack.
- Training or fine-tuning custom Large Language Models.
- Complex, long-horizon tasks or navigation in dynamic, multi-room environments.
2.3. Success Criteria
- Understanding: Students can draw a diagram of the VLA pipeline and explain the role of each component (Voice, LLM, ROS 2).
- Application (Whisper): Students are able to run a demo that successfully converts a spoken command into a text string.
- Application (LLM Planning): Students can explain how a natural language goal is decomposed into a JSON or YAML sequence of ROS 2 actions by an LLM.
- Completion: The mini-capstone example runs end-to-end: a voice command triggers a sequence of autonomous actions in the simulated robot.
2.4. Constraints
- Word Count: 2000‚Äì3000 words.
- Format: Docusaurus Markdown, including code snippets for Python (for Whisper and LLM interaction) and configuration files (for ROS 2 actions).
- Sources: Content will be based on official OpenAI Whisper documentation, ROS 2 action design patterns, and foundational VLA research.
- Timeline: 1 week.
3. UX/UI (Reader Experience)
3.1. Content Structure
The module will be structured to guide the learner from individual components to a fully integrated system.
- Chapter 1: Voice-to-Action: Whisper and Command Extraction
- Introduction to Whisper for speech-to-text.
- Tutorial: Setting up Whisper and running a simple voice transcription demo.
- Guide: Extracting key intents and entities from the transcribed text (e.g., turning "Can you please find the red block" into
{action: "find", object: "red block"}
).
- Chapter 2: Cognitive Planning with LLMs
- The role of LLMs as a "reasoning engine" for robots.
- Tutorial: Prompt engineering techniques to make an LLM translate a task into a machine-readable plan (a sequence of ROS 2 actions).
- Example: Converting
{action: "find", object: "red block"}
into a plan like[{navigate: "table"}, {detect: "red_block"}]
.
- Chapter 3: Mini-Capstone: The Full VLA Loop
- An overview of the complete system architecture.
- A step-by-step guide to launching all components (Whisper listener, LLM planner, ROS 2 action servers).
- A full demonstration of an autonomous task: e.g., "Go to the table, find the blue cup, and pick it up."
4. Technical Considerations
4.1. Key Decisions
- LLM Abstraction: We will use a major LLM provider's API (e.g., OpenAI, Anthropic, or Google) to avoid the complexity of self-hosting. The focus is on the integration, not the model itself.
- ROS 2 Actions: The module will use the standard ROS 2 action interface for all robot tasks (e.g.,
NavigateToPose
,DetectObject
,PickAndPlace
). This ensures a modular and scalable design. - Focus on Integration: The code provided will be primarily "glue code" written in Python, connecting the various components of the VLA pipeline.
4.2. External Dependencies
- Software: Python 3, ROS 2, and the OpenAI Whisper library.
- APIs: An API key for an LLM provider will be required for the cognitive planning chapter.
- Hardware: A microphone for voice commands.
- Prerequisites: Completion of prior modules, including ROS 2 basics, simulation, and navigation.
--------------------------------------------------------------------------------
URL: https://Muham251.github.io/ai-hackathon-1/docs/module-4/#11-goal
Content:
Spec: Module 4: Vision-Language-Action (VLA)
1. Business Understanding
1.1. Goal
The primary goal of this module is to teach students how to build a complete Vision-Language-Action (VLA) pipeline. This involves integrating Large Language Models (LLMs) with robotics, using Whisper for voice command input, employing LLMs for cognitive planning, and executing tasks through ROS 2 actions.
1.2. Justification
VLA is a transformative paradigm in robotics, enabling more natural human-robot interaction and a higher degree of autonomy. This module provides a crucial bridge between theoretical AI concepts (like LLMs) and practical, physical robotics, equipping students with highly sought-after skills in creating intelligent, responsive robots.
1.3. Target Audience
- Primary: Beginner to intermediate students in robotics and AI.
- Secondary: Developers and researchers interested in the practical application of LLMs in autonomous systems.
2. Functional Requirements
2.1. In Scope (Features)
- VLA Pipeline Overview: A clear, conceptual explanation of the entire Vision-Language-Action pipeline, from voice input to robot action.
- Voice Command Processing: A tutorial on using OpenAI Whisper to transcribe spoken language into text and extract actionable commands.
- LLM-based Planning: A guide on how to prompt an LLM to translate a high-level natural language task (e.g., "get me the apple") into a sequence of concrete ROS 2 action goals.
- Mini-Capstone Project: A complete, runnable example demonstrating the full VLA loop. A user will issue a voice command, and a simulated humanoid robot will plan and execute a simple multi-step task involving navigation, perception, and manipulation.
2.2. Out of Scope
- The development of a production-grade, highly reliable robotics stack.
- Training or fine-tuning custom Large Language Models.
- Complex, long-horizon tasks or navigation in dynamic, multi-room environments.
2.3. Success Criteria
- Understanding: Students can draw a diagram of the VLA pipeline and explain the role of each component (Voice, LLM, ROS 2).
- Application (Whisper): Students are able to run a demo that successfully converts a spoken command into a text string.
- Application (LLM Planning): Students can explain how a natural language goal is decomposed into a JSON or YAML sequence of ROS 2 actions by an LLM.
- Completion: The mini-capstone example runs end-to-end: a voice command triggers a sequence of autonomous actions in the simulated robot.
2.4. Constraints
- Word Count: 2000‚Äì3000 words.
- Format: Docusaurus Markdown, including code snippets for Python (for Whisper and LLM interaction) and configuration files (for ROS 2 actions).
- Sources: Content will be based on official OpenAI Whisper documentation, ROS 2 action design patterns, and foundational VLA research.
- Timeline: 1 week.
3. UX/UI (Reader Experience)
3.1. Content Structure
The module will be structured to guide the learner from individual components to a fully integrated system.
- Chapter 1: Voice-to-Action: Whisper and Command Extraction
- Introduction to Whisper for speech-to-text.
- Tutorial: Setting up Whisper and running a simple voice transcription demo.
- Guide: Extracting key intents and entities from the transcribed text (e.g., turning "Can you please find the red block" into
{action: "find", object: "red block"}
).
- Chapter 2: Cognitive Planning with LLMs
- The role of LLMs as a "reasoning engine" for robots.
- Tutorial: Prompt engineering techniques to make an LLM translate a task into a machine-readable plan (a sequence of ROS 2 actions).
- Example: Converting
{action: "find", object: "red block"}
into a plan like[{navigate: "table"}, {detect: "red_block"}]
.
- Chapter 3: Mini-Capstone: The Full VLA Loop
- An overview of the complete system architecture.
- A step-by-step guide to launching all components (Whisper listener, LLM planner, ROS 2 action servers).
- A full demonstration of an autonomous task: e.g., "Go to the table, find the blue cup, and pick it up."
4. Technical Considerations
4.1. Key Decisions
- LLM Abstraction: We will use a major LLM provider's API (e.g., OpenAI, Anthropic, or Google) to avoid the complexity of self-hosting. The focus is on the integration, not the model itself.
- ROS 2 Actions: The module will use the standard ROS 2 action interface for all robot tasks (e.g.,
NavigateToPose
,DetectObject
,PickAndPlace
). This ensures a modular and scalable design. - Focus on Integration: The code provided will be primarily "glue code" written in Python, connecting the various components of the VLA pipeline.
4.2. External Dependencies
- Software: Python 3, ROS 2, and the OpenAI Whisper library.
- APIs: An API key for an LLM provider will be required for the cognitive planning chapter.
- Hardware: A microphone for voice commands.
- Prerequisites: Completion of prior modules, including ROS 2 basics, simulation, and navigation.
--------------------------------------------------------------------------------
URL: https://Muham251.github.io/ai-hackathon-1/docs/module-4/#12-justification
Content:
Spec: Module 4: Vision-Language-Action (VLA)
1. Business Understanding
1.1. Goal
The primary goal of this module is to teach students how to build a complete Vision-Language-Action (VLA) pipeline. This involves integrating Large Language Models (LLMs) with robotics, using Whisper for voice command input, employing LLMs for cognitive planning, and executing tasks through ROS 2 actions.
1.2. Justification
VLA is a transformative paradigm in robotics, enabling more natural human-robot interaction and a higher degree of autonomy. This module provides a crucial bridge between theoretical AI concepts (like LLMs) and practical, physical robotics, equipping students with highly sought-after skills in creating intelligent, responsive robots.
1.3. Target Audience
- Primary: Beginner to intermediate students in robotics and AI.
- Secondary: Developers and researchers interested in the practical application of LLMs in autonomous systems.
2. Functional Requirements
2.1. In Scope (Features)
- VLA Pipeline Overview: A clear, conceptual explanation of the entire Vision-Language-Action pipeline, from voice input to robot action.
- Voice Command Processing: A tutorial on using OpenAI Whisper to transcribe spoken language into text and extract actionable commands.
- LLM-based Planning: A guide on how to prompt an LLM to translate a high-level natural language task (e.g., "get me the apple") into a sequence of concrete ROS 2 action goals.
- Mini-Capstone Project: A complete, runnable example demonstrating the full VLA loop. A user will issue a voice command, and a simulated humanoid robot will plan and execute a simple multi-step task involving navigation, perception, and manipulation.
2.2. Out of Scope
- The development of a production-grade, highly reliable robotics stack.
- Training or fine-tuning custom Large Language Models.
- Complex, long-horizon tasks or navigation in dynamic, multi-room environments.
2.3. Success Criteria
- Understanding: Students can draw a diagram of the VLA pipeline and explain the role of each component (Voice, LLM, ROS 2).
- Application (Whisper): Students are able to run a demo that successfully converts a spoken command into a text string.
- Application (LLM Planning): Students can explain how a natural language goal is decomposed into a JSON or YAML sequence of ROS 2 actions by an LLM.
- Completion: The mini-capstone example runs end-to-end: a voice command triggers a sequence of autonomous actions in the simulated robot.
2.4. Constraints
- Word Count: 2000‚Äì3000 words.
- Format: Docusaurus Markdown, including code snippets for Python (for Whisper and LLM interaction) and configuration files (for ROS 2 actions).
- Sources: Content will be based on official OpenAI Whisper documentation, ROS 2 action design patterns, and foundational VLA research.
- Timeline: 1 week.
3. UX/UI (Reader Experience)
3.1. Content Structure
The module will be structured to guide the learner from individual components to a fully integrated system.
- Chapter 1: Voice-to-Action: Whisper and Command Extraction
- Introduction to Whisper for speech-to-text.
- Tutorial: Setting up Whisper and running a simple voice transcription demo.
- Guide: Extracting key intents and entities from the transcribed text (e.g., turning "Can you please find the red block" into
{action: "find", object: "red block"}
).
- Chapter 2: Cognitive Planning with LLMs
- The role of LLMs as a "reasoning engine" for robots.
- Tutorial: Prompt engineering techniques to make an LLM translate a task into a machine-readable plan (a sequence of ROS 2 actions).
- Example: Converting
{action: "find", object: "red block"}
into a plan like[{navigate: "table"}, {detect: "red_block"}]
.
- Chapter 3: Mini-Capstone: The Full VLA Loop
- An overview of the complete system architecture.
- A step-by-step guide to launching all components (Whisper listener, LLM planner, ROS 2 action servers).
- A full demonstration of an autonomous task: e.g., "Go to the table, find the blue cup, and pick it up."
4. Technical Considerations
4.1. Key Decisions
- LLM Abstraction: We will use a major LLM provider's API (e.g., OpenAI, Anthropic, or Google) to avoid the complexity of self-hosting. The focus is on the integration, not the model itself.
- ROS 2 Actions: The module will use the standard ROS 2 action interface for all robot tasks (e.g.,
NavigateToPose
,DetectObject
,PickAndPlace
). This ensures a modular and scalable design. - Focus on Integration: The code provided will be primarily "glue code" written in Python, connecting the various components of the VLA pipeline.
4.2. External Dependencies
- Software: Python 3, ROS 2, and the OpenAI Whisper library.
- APIs: An API key for an LLM provider will be required for the cognitive planning chapter.
- Hardware: A microphone for voice commands.
- Prerequisites: Completion of prior modules, including ROS 2 basics, simulation, and navigation.
--------------------------------------------------------------------------------
URL: https://Muham251.github.io/ai-hackathon-1/docs/module-4/#13-target-audience
Content:
Spec: Module 4: Vision-Language-Action (VLA)
1. Business Understanding
1.1. Goal
The primary goal of this module is to teach students how to build a complete Vision-Language-Action (VLA) pipeline. This involves integrating Large Language Models (LLMs) with robotics, using Whisper for voice command input, employing LLMs for cognitive planning, and executing tasks through ROS 2 actions.
1.2. Justification
VLA is a transformative paradigm in robotics, enabling more natural human-robot interaction and a higher degree of autonomy. This module provides a crucial bridge between theoretical AI concepts (like LLMs) and practical, physical robotics, equipping students with highly sought-after skills in creating intelligent, responsive robots.
1.3. Target Audience
- Primary: Beginner to intermediate students in robotics and AI.
- Secondary: Developers and researchers interested in the practical application of LLMs in autonomous systems.
2. Functional Requirements
2.1. In Scope (Features)
- VLA Pipeline Overview: A clear, conceptual explanation of the entire Vision-Language-Action pipeline, from voice input to robot action.
- Voice Command Processing: A tutorial on using OpenAI Whisper to transcribe spoken language into text and extract actionable commands.
- LLM-based Planning: A guide on how to prompt an LLM to translate a high-level natural language task (e.g., "get me the apple") into a sequence of concrete ROS 2 action goals.
- Mini-Capstone Project: A complete, runnable example demonstrating the full VLA loop. A user will issue a voice command, and a simulated humanoid robot will plan and execute a simple multi-step task involving navigation, perception, and manipulation.
2.2. Out of Scope
- The development of a production-grade, highly reliable robotics stack.
- Training or fine-tuning custom Large Language Models.
- Complex, long-horizon tasks or navigation in dynamic, multi-room environments.
2.3. Success Criteria
- Understanding: Students can draw a diagram of the VLA pipeline and explain the role of each component (Voice, LLM, ROS 2).
- Application (Whisper): Students are able to run a demo that successfully converts a spoken command into a text string.
- Application (LLM Planning): Students can explain how a natural language goal is decomposed into a JSON or YAML sequence of ROS 2 actions by an LLM.
- Completion: The mini-capstone example runs end-to-end: a voice command triggers a sequence of autonomous actions in the simulated robot.
2.4. Constraints
- Word Count: 2000‚Äì3000 words.
- Format: Docusaurus Markdown, including code snippets for Python (for Whisper and LLM interaction) and configuration files (for ROS 2 actions).
- Sources: Content will be based on official OpenAI Whisper documentation, ROS 2 action design patterns, and foundational VLA research.
- Timeline: 1 week.
3. UX/UI (Reader Experience)
3.1. Content Structure
The module will be structured to guide the learner from individual components to a fully integrated system.
- Chapter 1: Voice-to-Action: Whisper and Command Extraction
- Introduction to Whisper for speech-to-text.
- Tutorial: Setting up Whisper and running a simple voice transcription demo.
- Guide: Extracting key intents and entities from the transcribed text (e.g., turning "Can you please find the red block" into
{action: "find", object: "red block"}
).
- Chapter 2: Cognitive Planning with LLMs
- The role of LLMs as a "reasoning engine" for robots.
- Tutorial: Prompt engineering techniques to make an LLM translate a task into a machine-readable plan (a sequence of ROS 2 actions).
- Example: Converting
{action: "find", object: "red block"}
into a plan like[{navigate: "table"}, {detect: "red_block"}]
.
- Chapter 3: Mini-Capstone: The Full VLA Loop
- An overview of the complete system architecture.
- A step-by-step guide to launching all components (Whisper listener, LLM planner, ROS 2 action servers).
- A full demonstration of an autonomous task: e.g., "Go to the table, find the blue cup, and pick it up."
4. Technical Considerations
4.1. Key Decisions
- LLM Abstraction: We will use a major LLM provider's API (e.g., OpenAI, Anthropic, or Google) to avoid the complexity of self-hosting. The focus is on the integration, not the model itself.
- ROS 2 Actions: The module will use the standard ROS 2 action interface for all robot tasks (e.g.,
NavigateToPose
,DetectObject
,PickAndPlace
). This ensures a modular and scalable design. - Focus on Integration: The code provided will be primarily "glue code" written in Python, connecting the various components of the VLA pipeline.
4.2. External Dependencies
- Software: Python 3, ROS 2, and the OpenAI Whisper library.
- APIs: An API key for an LLM provider will be required for the cognitive planning chapter.
- Hardware: A microphone for voice commands.
- Prerequisites: Completion of prior modules, including ROS 2 basics, simulation, and navigation.
--------------------------------------------------------------------------------
URL: https://Muham251.github.io/ai-hackathon-1/docs/module-4/#2-functional-requirements
Content:
Spec: Module 4: Vision-Language-Action (VLA)
1. Business Understanding
1.1. Goal
The primary goal of this module is to teach students how to build a complete Vision-Language-Action (VLA) pipeline. This involves integrating Large Language Models (LLMs) with robotics, using Whisper for voice command input, employing LLMs for cognitive planning, and executing tasks through ROS 2 actions.
1.2. Justification
VLA is a transformative paradigm in robotics, enabling more natural human-robot interaction and a higher degree of autonomy. This module provides a crucial bridge between theoretical AI concepts (like LLMs) and practical, physical robotics, equipping students with highly sought-after skills in creating intelligent, responsive robots.
1.3. Target Audience
- Primary: Beginner to intermediate students in robotics and AI.
- Secondary: Developers and researchers interested in the practical application of LLMs in autonomous systems.
2. Functional Requirements
2.1. In Scope (Features)
- VLA Pipeline Overview: A clear, conceptual explanation of the entire Vision-Language-Action pipeline, from voice input to robot action.
- Voice Command Processing: A tutorial on using OpenAI Whisper to transcribe spoken language into text and extract actionable commands.
- LLM-based Planning: A guide on how to prompt an LLM to translate a high-level natural language task (e.g., "get me the apple") into a sequence of concrete ROS 2 action goals.
- Mini-Capstone Project: A complete, runnable example demonstrating the full VLA loop. A user will issue a voice command, and a simulated humanoid robot will plan and execute a simple multi-step task involving navigation, perception, and manipulation.
2.2. Out of Scope
- The development of a production-grade, highly reliable robotics stack.
- Training or fine-tuning custom Large Language Models.
- Complex, long-horizon tasks or navigation in dynamic, multi-room environments.
2.3. Success Criteria
- Understanding: Students can draw a diagram of the VLA pipeline and explain the role of each component (Voice, LLM, ROS 2).
- Application (Whisper): Students are able to run a demo that successfully converts a spoken command into a text string.
- Application (LLM Planning): Students can explain how a natural language goal is decomposed into a JSON or YAML sequence of ROS 2 actions by an LLM.
- Completion: The mini-capstone example runs end-to-end: a voice command triggers a sequence of autonomous actions in the simulated robot.
2.4. Constraints
- Word Count: 2000‚Äì3000 words.
- Format: Docusaurus Markdown, including code snippets for Python (for Whisper and LLM interaction) and configuration files (for ROS 2 actions).
- Sources: Content will be based on official OpenAI Whisper documentation, ROS 2 action design patterns, and foundational VLA research.
- Timeline: 1 week.
3. UX/UI (Reader Experience)
3.1. Content Structure
The module will be structured to guide the learner from individual components to a fully integrated system.
- Chapter 1: Voice-to-Action: Whisper and Command Extraction
- Introduction to Whisper for speech-to-text.
- Tutorial: Setting up Whisper and running a simple voice transcription demo.
- Guide: Extracting key intents and entities from the transcribed text (e.g., turning "Can you please find the red block" into
{action: "find", object: "red block"}
).
- Chapter 2: Cognitive Planning with LLMs
- The role of LLMs as a "reasoning engine" for robots.
- Tutorial: Prompt engineering techniques to make an LLM translate a task into a machine-readable plan (a sequence of ROS 2 actions).
- Example: Converting
{action: "find", object: "red block"}
into a plan like[{navigate: "table"}, {detect: "red_block"}]
.
- Chapter 3: Mini-Capstone: The Full VLA Loop
- An overview of the complete system architecture.
- A step-by-step guide to launching all components (Whisper listener, LLM planner, ROS 2 action servers).
- A full demonstration of an autonomous task: e.g., "Go to the table, find the blue cup, and pick it up."
4. Technical Considerations
4.1. Key Decisions
- LLM Abstraction: We will use a major LLM provider's API (e.g., OpenAI, Anthropic, or Google) to avoid the complexity of self-hosting. The focus is on the integration, not the model itself.
- ROS 2 Actions: The module will use the standard ROS 2 action interface for all robot tasks (e.g.,
NavigateToPose
,DetectObject
,PickAndPlace
). This ensures a modular and scalable design. - Focus on Integration: The code provided will be primarily "glue code" written in Python, connecting the various components of the VLA pipeline.
4.2. External Dependencies
- Software: Python 3, ROS 2, and the OpenAI Whisper library.
- APIs: An API key for an LLM provider will be required for the cognitive planning chapter.
- Hardware: A microphone for voice commands.
- Prerequisites: Completion of prior modules, including ROS 2 basics, simulation, and navigation.
--------------------------------------------------------------------------------
URL: https://Muham251.github.io/ai-hackathon-1/docs/module-4/#21-in-scope-features
Content:
Spec: Module 4: Vision-Language-Action (VLA)
1. Business Understanding
1.1. Goal
The primary goal of this module is to teach students how to build a complete Vision-Language-Action (VLA) pipeline. This involves integrating Large Language Models (LLMs) with robotics, using Whisper for voice command input, employing LLMs for cognitive planning, and executing tasks through ROS 2 actions.
1.2. Justification
VLA is a transformative paradigm in robotics, enabling more natural human-robot interaction and a higher degree of autonomy. This module provides a crucial bridge between theoretical AI concepts (like LLMs) and practical, physical robotics, equipping students with highly sought-after skills in creating intelligent, responsive robots.
1.3. Target Audience
- Primary: Beginner to intermediate students in robotics and AI.
- Secondary: Developers and researchers interested in the practical application of LLMs in autonomous systems.
2. Functional Requirements
2.1. In Scope (Features)
- VLA Pipeline Overview: A clear, conceptual explanation of the entire Vision-Language-Action pipeline, from voice input to robot action.
- Voice Command Processing: A tutorial on using OpenAI Whisper to transcribe spoken language into text and extract actionable commands.
- LLM-based Planning: A guide on how to prompt an LLM to translate a high-level natural language task (e.g., "get me the apple") into a sequence of concrete ROS 2 action goals.
- Mini-Capstone Project: A complete, runnable example demonstrating the full VLA loop. A user will issue a voice command, and a simulated humanoid robot will plan and execute a simple multi-step task involving navigation, perception, and manipulation.
2.2. Out of Scope
- The development of a production-grade, highly reliable robotics stack.
- Training or fine-tuning custom Large Language Models.
- Complex, long-horizon tasks or navigation in dynamic, multi-room environments.
2.3. Success Criteria
- Understanding: Students can draw a diagram of the VLA pipeline and explain the role of each component (Voice, LLM, ROS 2).
- Application (Whisper): Students are able to run a demo that successfully converts a spoken command into a text string.
- Application (LLM Planning): Students can explain how a natural language goal is decomposed into a JSON or YAML sequence of ROS 2 actions by an LLM.
- Completion: The mini-capstone example runs end-to-end: a voice command triggers a sequence of autonomous actions in the simulated robot.
2.4. Constraints
- Word Count: 2000‚Äì3000 words.
- Format: Docusaurus Markdown, including code snippets for Python (for Whisper and LLM interaction) and configuration files (for ROS 2 actions).
- Sources: Content will be based on official OpenAI Whisper documentation, ROS 2 action design patterns, and foundational VLA research.
- Timeline: 1 week.
3. UX/UI (Reader Experience)
3.1. Content Structure
The module will be structured to guide the learner from individual components to a fully integrated system.
- Chapter 1: Voice-to-Action: Whisper and Command Extraction
- Introduction to Whisper for speech-to-text.
- Tutorial: Setting up Whisper and running a simple voice transcription demo.
- Guide: Extracting key intents and entities from the transcribed text (e.g., turning "Can you please find the red block" into
{action: "find", object: "red block"}
).
- Chapter 2: Cognitive Planning with LLMs
- The role of LLMs as a "reasoning engine" for robots.
- Tutorial: Prompt engineering techniques to make an LLM translate a task into a machine-readable plan (a sequence of ROS 2 actions).
- Example: Converting
{action: "find", object: "red block"}
into a plan like[{navigate: "table"}, {detect: "red_block"}]
.
- Chapter 3: Mini-Capstone: The Full VLA Loop
- An overview of the complete system architecture.
- A step-by-step guide to launching all components (Whisper listener, LLM planner, ROS 2 action servers).
- A full demonstration of an autonomous task: e.g., "Go to the table, find the blue cup, and pick it up."
4. Technical Considerations
4.1. Key Decisions
- LLM Abstraction: We will use a major LLM provider's API (e.g., OpenAI, Anthropic, or Google) to avoid the complexity of self-hosting. The focus is on the integration, not the model itself.
- ROS 2 Actions: The module will use the standard ROS 2 action interface for all robot tasks (e.g.,
NavigateToPose
,DetectObject
,PickAndPlace
). This ensures a modular and scalable design. - Focus on Integration: The code provided will be primarily "glue code" written in Python, connecting the various components of the VLA pipeline.
4.2. External Dependencies
- Software: Python 3, ROS 2, and the OpenAI Whisper library.
- APIs: An API key for an LLM provider will be required for the cognitive planning chapter.
- Hardware: A microphone for voice commands.
- Prerequisites: Completion of prior modules, including ROS 2 basics, simulation, and navigation.
--------------------------------------------------------------------------------
URL: https://Muham251.github.io/ai-hackathon-1/docs/module-4/#22-out-of-scope
Content:
Spec: Module 4: Vision-Language-Action (VLA)
1. Business Understanding
1.1. Goal
The primary goal of this module is to teach students how to build a complete Vision-Language-Action (VLA) pipeline. This involves integrating Large Language Models (LLMs) with robotics, using Whisper for voice command input, employing LLMs for cognitive planning, and executing tasks through ROS 2 actions.
1.2. Justification
VLA is a transformative paradigm in robotics, enabling more natural human-robot interaction and a higher degree of autonomy. This module provides a crucial bridge between theoretical AI concepts (like LLMs) and practical, physical robotics, equipping students with highly sought-after skills in creating intelligent, responsive robots.
1.3. Target Audience
- Primary: Beginner to intermediate students in robotics and AI.
- Secondary: Developers and researchers interested in the practical application of LLMs in autonomous systems.
2. Functional Requirements
2.1. In Scope (Features)
- VLA Pipeline Overview: A clear, conceptual explanation of the entire Vision-Language-Action pipeline, from voice input to robot action.
- Voice Command Processing: A tutorial on using OpenAI Whisper to transcribe spoken language into text and extract actionable commands.
- LLM-based Planning: A guide on how to prompt an LLM to translate a high-level natural language task (e.g., "get me the apple") into a sequence of concrete ROS 2 action goals.
- Mini-Capstone Project: A complete, runnable example demonstrating the full VLA loop. A user will issue a voice command, and a simulated humanoid robot will plan and execute a simple multi-step task involving navigation, perception, and manipulation.
2.2. Out of Scope
- The development of a production-grade, highly reliable robotics stack.
- Training or fine-tuning custom Large Language Models.
- Complex, long-horizon tasks or navigation in dynamic, multi-room environments.
2.3. Success Criteria
- Understanding: Students can draw a diagram of the VLA pipeline and explain the role of each component (Voice, LLM, ROS 2).
- Application (Whisper): Students are able to run a demo that successfully converts a spoken command into a text string.
- Application (LLM Planning): Students can explain how a natural language goal is decomposed into a JSON or YAML sequence of ROS 2 actions by an LLM.
- Completion: The mini-capstone example runs end-to-end: a voice command triggers a sequence of autonomous actions in the simulated robot.
2.4. Constraints
- Word Count: 2000‚Äì3000 words.
- Format: Docusaurus Markdown, including code snippets for Python (for Whisper and LLM interaction) and configuration files (for ROS 2 actions).
- Sources: Content will be based on official OpenAI Whisper documentation, ROS 2 action design patterns, and foundational VLA research.
- Timeline: 1 week.
3. UX/UI (Reader Experience)
3.1. Content Structure
The module will be structured to guide the learner from individual components to a fully integrated system.
- Chapter 1: Voice-to-Action: Whisper and Command Extraction
- Introduction to Whisper for speech-to-text.
- Tutorial: Setting up Whisper and running a simple voice transcription demo.
- Guide: Extracting key intents and entities from the transcribed text (e.g., turning "Can you please find the red block" into
{action: "find", object: "red block"}
).
- Chapter 2: Cognitive Planning with LLMs
- The role of LLMs as a "reasoning engine" for robots.
- Tutorial: Prompt engineering techniques to make an LLM translate a task into a machine-readable plan (a sequence of ROS 2 actions).
- Example: Converting
{action: "find", object: "red block"}
into a plan like[{navigate: "table"}, {detect: "red_block"}]
.
- Chapter 3: Mini-Capstone: The Full VLA Loop
- An overview of the complete system architecture.
- A step-by-step guide to launching all components (Whisper listener, LLM planner, ROS 2 action servers).
- A full demonstration of an autonomous task: e.g., "Go to the table, find the blue cup, and pick it up."
4. Technical Considerations
4.1. Key Decisions
- LLM Abstraction: We will use a major LLM provider's API (e.g., OpenAI, Anthropic, or Google) to avoid the complexity of self-hosting. The focus is on the integration, not the model itself.
- ROS 2 Actions: The module will use the standard ROS 2 action interface for all robot tasks (e.g.,
NavigateToPose
,DetectObject
,PickAndPlace
). This ensures a modular and scalable design. - Focus on Integration: The code provided will be primarily "glue code" written in Python, connecting the various components of the VLA pipeline.
4.2. External Dependencies
- Software: Python 3, ROS 2, and the OpenAI Whisper library.
- APIs: An API key for an LLM provider will be required for the cognitive planning chapter.
- Hardware: A microphone for voice commands.
- Prerequisites: Completion of prior modules, including ROS 2 basics, simulation, and navigation.
--------------------------------------------------------------------------------
URL: https://Muham251.github.io/ai-hackathon-1/docs/module-4/#23-success-criteria
Content:
Spec: Module 4: Vision-Language-Action (VLA)
1. Business Understanding
1.1. Goal
The primary goal of this module is to teach students how to build a complete Vision-Language-Action (VLA) pipeline. This involves integrating Large Language Models (LLMs) with robotics, using Whisper for voice command input, employing LLMs for cognitive planning, and executing tasks through ROS 2 actions.
1.2. Justification
VLA is a transformative paradigm in robotics, enabling more natural human-robot interaction and a higher degree of autonomy. This module provides a crucial bridge between theoretical AI concepts (like LLMs) and practical, physical robotics, equipping students with highly sought-after skills in creating intelligent, responsive robots.
1.3. Target Audience
- Primary: Beginner to intermediate students in robotics and AI.
- Secondary: Developers and researchers interested in the practical application of LLMs in autonomous systems.
2. Functional Requirements
2.1. In Scope (Features)
- VLA Pipeline Overview: A clear, conceptual explanation of the entire Vision-Language-Action pipeline, from voice input to robot action.
- Voice Command Processing: A tutorial on using OpenAI Whisper to transcribe spoken language into text and extract actionable commands.
- LLM-based Planning: A guide on how to prompt an LLM to translate a high-level natural language task (e.g., "get me the apple") into a sequence of concrete ROS 2 action goals.
- Mini-Capstone Project: A complete, runnable example demonstrating the full VLA loop. A user will issue a voice command, and a simulated humanoid robot will plan and execute a simple multi-step task involving navigation, perception, and manipulation.
2.2. Out of Scope
- The development of a production-grade, highly reliable robotics stack.
- Training or fine-tuning custom Large Language Models.
- Complex, long-horizon tasks or navigation in dynamic, multi-room environments.
2.3. Success Criteria
- Understanding: Students can draw a diagram of the VLA pipeline and explain the role of each component (Voice, LLM, ROS 2).
- Application (Whisper): Students are able to run a demo that successfully converts a spoken command into a text string.
- Application (LLM Planning): Students can explain how a natural language goal is decomposed into a JSON or YAML sequence of ROS 2 actions by an LLM.
- Completion: The mini-capstone example runs end-to-end: a voice command triggers a sequence of autonomous actions in the simulated robot.
2.4. Constraints
- Word Count: 2000‚Äì3000 words.
- Format: Docusaurus Markdown, including code snippets for Python (for Whisper and LLM interaction) and configuration files (for ROS 2 actions).
- Sources: Content will be based on official OpenAI Whisper documentation, ROS 2 action design patterns, and foundational VLA research.
- Timeline: 1 week.
3. UX/UI (Reader Experience)
3.1. Content Structure
The module will be structured to guide the learner from individual components to a fully integrated system.
- Chapter 1: Voice-to-Action: Whisper and Command Extraction
- Introduction to Whisper for speech-to-text.
- Tutorial: Setting up Whisper and running a simple voice transcription demo.
- Guide: Extracting key intents and entities from the transcribed text (e.g., turning "Can you please find the red block" into
{action: "find", object: "red block"}
).
- Chapter 2: Cognitive Planning with LLMs
- The role of LLMs as a "reasoning engine" for robots.
- Tutorial: Prompt engineering techniques to make an LLM translate a task into a machine-readable plan (a sequence of ROS 2 actions).
- Example: Converting
{action: "find", object: "red block"}
into a plan like[{navigate: "table"}, {detect: "red_block"}]
.
- Chapter 3: Mini-Capstone: The Full VLA Loop
- An overview of the complete system architecture.
- A step-by-step guide to launching all components (Whisper listener, LLM planner, ROS 2 action servers).
- A full demonstration of an autonomous task: e.g., "Go to the table, find the blue cup, and pick it up."
4. Technical Considerations
4.1. Key Decisions
- LLM Abstraction: We will use a major LLM provider's API (e.g., OpenAI, Anthropic, or Google) to avoid the complexity of self-hosting. The focus is on the integration, not the model itself.
- ROS 2 Actions: The module will use the standard ROS 2 action interface for all robot tasks (e.g.,
NavigateToPose
,DetectObject
,PickAndPlace
). This ensures a modular and scalable design. - Focus on Integration: The code provided will be primarily "glue code" written in Python, connecting the various components of the VLA pipeline.
4.2. External Dependencies
- Software: Python 3, ROS 2, and the OpenAI Whisper library.
- APIs: An API key for an LLM provider will be required for the cognitive planning chapter.
- Hardware: A microphone for voice commands.
- Prerequisites: Completion of prior modules, including ROS 2 basics, simulation, and navigation.
--------------------------------------------------------------------------------
URL: https://Muham251.github.io/ai-hackathon-1/docs/module-4/#24-constraints
Content:
Spec: Module 4: Vision-Language-Action (VLA)
1. Business Understanding
1.1. Goal
The primary goal of this module is to teach students how to build a complete Vision-Language-Action (VLA) pipeline. This involves integrating Large Language Models (LLMs) with robotics, using Whisper for voice command input, employing LLMs for cognitive planning, and executing tasks through ROS 2 actions.
1.2. Justification
VLA is a transformative paradigm in robotics, enabling more natural human-robot interaction and a higher degree of autonomy. This module provides a crucial bridge between theoretical AI concepts (like LLMs) and practical, physical robotics, equipping students with highly sought-after skills in creating intelligent, responsive robots.
1.3. Target Audience
- Primary: Beginner to intermediate students in robotics and AI.
- Secondary: Developers and researchers interested in the practical application of LLMs in autonomous systems.
2. Functional Requirements
2.1. In Scope (Features)
- VLA Pipeline Overview: A clear, conceptual explanation of the entire Vision-Language-Action pipeline, from voice input to robot action.
- Voice Command Processing: A tutorial on using OpenAI Whisper to transcribe spoken language into text and extract actionable commands.
- LLM-based Planning: A guide on how to prompt an LLM to translate a high-level natural language task (e.g., "get me the apple") into a sequence of concrete ROS 2 action goals.
- Mini-Capstone Project: A complete, runnable example demonstrating the full VLA loop. A user will issue a voice command, and a simulated humanoid robot will plan and execute a simple multi-step task involving navigation, perception, and manipulation.
2.2. Out of Scope
- The development of a production-grade, highly reliable robotics stack.
- Training or fine-tuning custom Large Language Models.
- Complex, long-horizon tasks or navigation in dynamic, multi-room environments.
2.3. Success Criteria
- Understanding: Students can draw a diagram of the VLA pipeline and explain the role of each component (Voice, LLM, ROS 2).
- Application (Whisper): Students are able to run a demo that successfully converts a spoken command into a text string.
- Application (LLM Planning): Students can explain how a natural language goal is decomposed into a JSON or YAML sequence of ROS 2 actions by an LLM.
- Completion: The mini-capstone example runs end-to-end: a voice command triggers a sequence of autonomous actions in the simulated robot.
2.4. Constraints
- Word Count: 2000‚Äì3000 words.
- Format: Docusaurus Markdown, including code snippets for Python (for Whisper and LLM interaction) and configuration files (for ROS 2 actions).
- Sources: Content will be based on official OpenAI Whisper documentation, ROS 2 action design patterns, and foundational VLA research.
- Timeline: 1 week.
3. UX/UI (Reader Experience)
3.1. Content Structure
The module will be structured to guide the learner from individual components to a fully integrated system.
- Chapter 1: Voice-to-Action: Whisper and Command Extraction
- Introduction to Whisper for speech-to-text.
- Tutorial: Setting up Whisper and running a simple voice transcription demo.
- Guide: Extracting key intents and entities from the transcribed text (e.g., turning "Can you please find the red block" into
{action: "find", object: "red block"}
).
- Chapter 2: Cognitive Planning with LLMs
- The role of LLMs as a "reasoning engine" for robots.
- Tutorial: Prompt engineering techniques to make an LLM translate a task into a machine-readable plan (a sequence of ROS 2 actions).
- Example: Converting
{action: "find", object: "red block"}
into a plan like[{navigate: "table"}, {detect: "red_block"}]
.
- Chapter 3: Mini-Capstone: The Full VLA Loop
- An overview of the complete system architecture.
- A step-by-step guide to launching all components (Whisper listener, LLM planner, ROS 2 action servers).
- A full demonstration of an autonomous task: e.g., "Go to the table, find the blue cup, and pick it up."
4. Technical Considerations
4.1. Key Decisions
- LLM Abstraction: We will use a major LLM provider's API (e.g., OpenAI, Anthropic, or Google) to avoid the complexity of self-hosting. The focus is on the integration, not the model itself.
- ROS 2 Actions: The module will use the standard ROS 2 action interface for all robot tasks (e.g.,
NavigateToPose
,DetectObject
,PickAndPlace
). This ensures a modular and scalable design. - Focus on Integration: The code provided will be primarily "glue code" written in Python, connecting the various components of the VLA pipeline.
4.2. External Dependencies
- Software: Python 3, ROS 2, and the OpenAI Whisper library.
- APIs: An API key for an LLM provider will be required for the cognitive planning chapter.
- Hardware: A microphone for voice commands.
- Prerequisites: Completion of prior modules, including ROS 2 basics, simulation, and navigation.
--------------------------------------------------------------------------------
URL: https://Muham251.github.io/ai-hackathon-1/docs/module-4/#3-uxui-reader-experience
Content:
Spec: Module 4: Vision-Language-Action (VLA)
1. Business Understanding
1.1. Goal
The primary goal of this module is to teach students how to build a complete Vision-Language-Action (VLA) pipeline. This involves integrating Large Language Models (LLMs) with robotics, using Whisper for voice command input, employing LLMs for cognitive planning, and executing tasks through ROS 2 actions.
1.2. Justification
VLA is a transformative paradigm in robotics, enabling more natural human-robot interaction and a higher degree of autonomy. This module provides a crucial bridge between theoretical AI concepts (like LLMs) and practical, physical robotics, equipping students with highly sought-after skills in creating intelligent, responsive robots.
1.3. Target Audience
- Primary: Beginner to intermediate students in robotics and AI.
- Secondary: Developers and researchers interested in the practical application of LLMs in autonomous systems.
2. Functional Requirements
2.1. In Scope (Features)
- VLA Pipeline Overview: A clear, conceptual explanation of the entire Vision-Language-Action pipeline, from voice input to robot action.
- Voice Command Processing: A tutorial on using OpenAI Whisper to transcribe spoken language into text and extract actionable commands.
- LLM-based Planning: A guide on how to prompt an LLM to translate a high-level natural language task (e.g., "get me the apple") into a sequence of concrete ROS 2 action goals.
- Mini-Capstone Project: A complete, runnable example demonstrating the full VLA loop. A user will issue a voice command, and a simulated humanoid robot will plan and execute a simple multi-step task involving navigation, perception, and manipulation.
2.2. Out of Scope
- The development of a production-grade, highly reliable robotics stack.
- Training or fine-tuning custom Large Language Models.
- Complex, long-horizon tasks or navigation in dynamic, multi-room environments.
2.3. Success Criteria
- Understanding: Students can draw a diagram of the VLA pipeline and explain the role of each component (Voice, LLM, ROS 2).
- Application (Whisper): Students are able to run a demo that successfully converts a spoken command into a text string.
- Application (LLM Planning): Students can explain how a natural language goal is decomposed into a JSON or YAML sequence of ROS 2 actions by an LLM.
- Completion: The mini-capstone example runs end-to-end: a voice command triggers a sequence of autonomous actions in the simulated robot.
2.4. Constraints
- Word Count: 2000‚Äì3000 words.
- Format: Docusaurus Markdown, including code snippets for Python (for Whisper and LLM interaction) and configuration files (for ROS 2 actions).
- Sources: Content will be based on official OpenAI Whisper documentation, ROS 2 action design patterns, and foundational VLA research.
- Timeline: 1 week.
3. UX/UI (Reader Experience)
3.1. Content Structure
The module will be structured to guide the learner from individual components to a fully integrated system.
- Chapter 1: Voice-to-Action: Whisper and Command Extraction
- Introduction to Whisper for speech-to-text.
- Tutorial: Setting up Whisper and running a simple voice transcription demo.
- Guide: Extracting key intents and entities from the transcribed text (e.g., turning "Can you please find the red block" into
{action: "find", object: "red block"}
).
- Chapter 2: Cognitive Planning with LLMs
- The role of LLMs as a "reasoning engine" for robots.
- Tutorial: Prompt engineering techniques to make an LLM translate a task into a machine-readable plan (a sequence of ROS 2 actions).
- Example: Converting
{action: "find", object: "red block"}
into a plan like[{navigate: "table"}, {detect: "red_block"}]
.
- Chapter 3: Mini-Capstone: The Full VLA Loop
- An overview of the complete system architecture.
- A step-by-step guide to launching all components (Whisper listener, LLM planner, ROS 2 action servers).
- A full demonstration of an autonomous task: e.g., "Go to the table, find the blue cup, and pick it up."
4. Technical Considerations
4.1. Key Decisions
- LLM Abstraction: We will use a major LLM provider's API (e.g., OpenAI, Anthropic, or Google) to avoid the complexity of self-hosting. The focus is on the integration, not the model itself.
- ROS 2 Actions: The module will use the standard ROS 2 action interface for all robot tasks (e.g.,
NavigateToPose
,DetectObject
,PickAndPlace
). This ensures a modular and scalable design. - Focus on Integration: The code provided will be primarily "glue code" written in Python, connecting the various components of the VLA pipeline.
4.2. External Dependencies
- Software: Python 3, ROS 2, and the OpenAI Whisper library.
- APIs: An API key for an LLM provider will be required for the cognitive planning chapter.
- Hardware: A microphone for voice commands.
- Prerequisites: Completion of prior modules, including ROS 2 basics, simulation, and navigation.
--------------------------------------------------------------------------------
URL: https://Muham251.github.io/ai-hackathon-1/docs/module-4/#31-content-structure
Content:
Spec: Module 4: Vision-Language-Action (VLA)
1. Business Understanding
1.1. Goal
The primary goal of this module is to teach students how to build a complete Vision-Language-Action (VLA) pipeline. This involves integrating Large Language Models (LLMs) with robotics, using Whisper for voice command input, employing LLMs for cognitive planning, and executing tasks through ROS 2 actions.
1.2. Justification
VLA is a transformative paradigm in robotics, enabling more natural human-robot interaction and a higher degree of autonomy. This module provides a crucial bridge between theoretical AI concepts (like LLMs) and practical, physical robotics, equipping students with highly sought-after skills in creating intelligent, responsive robots.
1.3. Target Audience
- Primary: Beginner to intermediate students in robotics and AI.
- Secondary: Developers and researchers interested in the practical application of LLMs in autonomous systems.
2. Functional Requirements
2.1. In Scope (Features)
- VLA Pipeline Overview: A clear, conceptual explanation of the entire Vision-Language-Action pipeline, from voice input to robot action.
- Voice Command Processing: A tutorial on using OpenAI Whisper to transcribe spoken language into text and extract actionable commands.
- LLM-based Planning: A guide on how to prompt an LLM to translate a high-level natural language task (e.g., "get me the apple") into a sequence of concrete ROS 2 action goals.
- Mini-Capstone Project: A complete, runnable example demonstrating the full VLA loop. A user will issue a voice command, and a simulated humanoid robot will plan and execute a simple multi-step task involving navigation, perception, and manipulation.
2.2. Out of Scope
- The development of a production-grade, highly reliable robotics stack.
- Training or fine-tuning custom Large Language Models.
- Complex, long-horizon tasks or navigation in dynamic, multi-room environments.
2.3. Success Criteria
- Understanding: Students can draw a diagram of the VLA pipeline and explain the role of each component (Voice, LLM, ROS 2).
- Application (Whisper): Students are able to run a demo that successfully converts a spoken command into a text string.
- Application (LLM Planning): Students can explain how a natural language goal is decomposed into a JSON or YAML sequence of ROS 2 actions by an LLM.
- Completion: The mini-capstone example runs end-to-end: a voice command triggers a sequence of autonomous actions in the simulated robot.
2.4. Constraints
- Word Count: 2000‚Äì3000 words.
- Format: Docusaurus Markdown, including code snippets for Python (for Whisper and LLM interaction) and configuration files (for ROS 2 actions).
- Sources: Content will be based on official OpenAI Whisper documentation, ROS 2 action design patterns, and foundational VLA research.
- Timeline: 1 week.
3. UX/UI (Reader Experience)
3.1. Content Structure
The module will be structured to guide the learner from individual components to a fully integrated system.
- Chapter 1: Voice-to-Action: Whisper and Command Extraction
- Introduction to Whisper for speech-to-text.
- Tutorial: Setting up Whisper and running a simple voice transcription demo.
- Guide: Extracting key intents and entities from the transcribed text (e.g., turning "Can you please find the red block" into
{action: "find", object: "red block"}
).
- Chapter 2: Cognitive Planning with LLMs
- The role of LLMs as a "reasoning engine" for robots.
- Tutorial: Prompt engineering techniques to make an LLM translate a task into a machine-readable plan (a sequence of ROS 2 actions).
- Example: Converting
{action: "find", object: "red block"}
into a plan like[{navigate: "table"}, {detect: "red_block"}]
.
- Chapter 3: Mini-Capstone: The Full VLA Loop
- An overview of the complete system architecture.
- A step-by-step guide to launching all components (Whisper listener, LLM planner, ROS 2 action servers).
- A full demonstration of an autonomous task: e.g., "Go to the table, find the blue cup, and pick it up."
4. Technical Considerations
4.1. Key Decisions
- LLM Abstraction: We will use a major LLM provider's API (e.g., OpenAI, Anthropic, or Google) to avoid the complexity of self-hosting. The focus is on the integration, not the model itself.
- ROS 2 Actions: The module will use the standard ROS 2 action interface for all robot tasks (e.g.,
NavigateToPose
,DetectObject
,PickAndPlace
). This ensures a modular and scalable design. - Focus on Integration: The code provided will be primarily "glue code" written in Python, connecting the various components of the VLA pipeline.
4.2. External Dependencies
- Software: Python 3, ROS 2, and the OpenAI Whisper library.
- APIs: An API key for an LLM provider will be required for the cognitive planning chapter.
- Hardware: A microphone for voice commands.
- Prerequisites: Completion of prior modules, including ROS 2 basics, simulation, and navigation.
--------------------------------------------------------------------------------
URL: https://Muham251.github.io/ai-hackathon-1/docs/module-4/#4-technical-considerations
Content:
Spec: Module 4: Vision-Language-Action (VLA)
1. Business Understanding
1.1. Goal
The primary goal of this module is to teach students how to build a complete Vision-Language-Action (VLA) pipeline. This involves integrating Large Language Models (LLMs) with robotics, using Whisper for voice command input, employing LLMs for cognitive planning, and executing tasks through ROS 2 actions.
1.2. Justification
VLA is a transformative paradigm in robotics, enabling more natural human-robot interaction and a higher degree of autonomy. This module provides a crucial bridge between theoretical AI concepts (like LLMs) and practical, physical robotics, equipping students with highly sought-after skills in creating intelligent, responsive robots.
1.3. Target Audience
- Primary: Beginner to intermediate students in robotics and AI.
- Secondary: Developers and researchers interested in the practical application of LLMs in autonomous systems.
2. Functional Requirements
2.1. In Scope (Features)
- VLA Pipeline Overview: A clear, conceptual explanation of the entire Vision-Language-Action pipeline, from voice input to robot action.
- Voice Command Processing: A tutorial on using OpenAI Whisper to transcribe spoken language into text and extract actionable commands.
- LLM-based Planning: A guide on how to prompt an LLM to translate a high-level natural language task (e.g., "get me the apple") into a sequence of concrete ROS 2 action goals.
- Mini-Capstone Project: A complete, runnable example demonstrating the full VLA loop. A user will issue a voice command, and a simulated humanoid robot will plan and execute a simple multi-step task involving navigation, perception, and manipulation.
2.2. Out of Scope
- The development of a production-grade, highly reliable robotics stack.
- Training or fine-tuning custom Large Language Models.
- Complex, long-horizon tasks or navigation in dynamic, multi-room environments.
2.3. Success Criteria
- Understanding: Students can draw a diagram of the VLA pipeline and explain the role of each component (Voice, LLM, ROS 2).
- Application (Whisper): Students are able to run a demo that successfully converts a spoken command into a text string.
- Application (LLM Planning): Students can explain how a natural language goal is decomposed into a JSON or YAML sequence of ROS 2 actions by an LLM.
- Completion: The mini-capstone example runs end-to-end: a voice command triggers a sequence of autonomous actions in the simulated robot.
2.4. Constraints
- Word Count: 2000‚Äì3000 words.
- Format: Docusaurus Markdown, including code snippets for Python (for Whisper and LLM interaction) and configuration files (for ROS 2 actions).
- Sources: Content will be based on official OpenAI Whisper documentation, ROS 2 action design patterns, and foundational VLA research.
- Timeline: 1 week.
3. UX/UI (Reader Experience)
3.1. Content Structure
The module will be structured to guide the learner from individual components to a fully integrated system.
- Chapter 1: Voice-to-Action: Whisper and Command Extraction
- Introduction to Whisper for speech-to-text.
- Tutorial: Setting up Whisper and running a simple voice transcription demo.
- Guide: Extracting key intents and entities from the transcribed text (e.g., turning "Can you please find the red block" into
{action: "find", object: "red block"}
).
- Chapter 2: Cognitive Planning with LLMs
- The role of LLMs as a "reasoning engine" for robots.
- Tutorial: Prompt engineering techniques to make an LLM translate a task into a machine-readable plan (a sequence of ROS 2 actions).
- Example: Converting
{action: "find", object: "red block"}
into a plan like[{navigate: "table"}, {detect: "red_block"}]
.
- Chapter 3: Mini-Capstone: The Full VLA Loop
- An overview of the complete system architecture.
- A step-by-step guide to launching all components (Whisper listener, LLM planner, ROS 2 action servers).
- A full demonstration of an autonomous task: e.g., "Go to the table, find the blue cup, and pick it up."
4. Technical Considerations
4.1. Key Decisions
- LLM Abstraction: We will use a major LLM provider's API (e.g., OpenAI, Anthropic, or Google) to avoid the complexity of self-hosting. The focus is on the integration, not the model itself.
- ROS 2 Actions: The module will use the standard ROS 2 action interface for all robot tasks (e.g.,
NavigateToPose
,DetectObject
,PickAndPlace
). This ensures a modular and scalable design. - Focus on Integration: The code provided will be primarily "glue code" written in Python, connecting the various components of the VLA pipeline.
4.2. External Dependencies
- Software: Python 3, ROS 2, and the OpenAI Whisper library.
- APIs: An API key for an LLM provider will be required for the cognitive planning chapter.
- Hardware: A microphone for voice commands.
- Prerequisites: Completion of prior modules, including ROS 2 basics, simulation, and navigation.
--------------------------------------------------------------------------------
URL: https://Muham251.github.io/ai-hackathon-1/docs/module-4/#41-key-decisions
Content:
Spec: Module 4: Vision-Language-Action (VLA)
1. Business Understanding
1.1. Goal
The primary goal of this module is to teach students how to build a complete Vision-Language-Action (VLA) pipeline. This involves integrating Large Language Models (LLMs) with robotics, using Whisper for voice command input, employing LLMs for cognitive planning, and executing tasks through ROS 2 actions.
1.2. Justification
VLA is a transformative paradigm in robotics, enabling more natural human-robot interaction and a higher degree of autonomy. This module provides a crucial bridge between theoretical AI concepts (like LLMs) and practical, physical robotics, equipping students with highly sought-after skills in creating intelligent, responsive robots.
1.3. Target Audience
- Primary: Beginner to intermediate students in robotics and AI.
- Secondary: Developers and researchers interested in the practical application of LLMs in autonomous systems.
2. Functional Requirements
2.1. In Scope (Features)
- VLA Pipeline Overview: A clear, conceptual explanation of the entire Vision-Language-Action pipeline, from voice input to robot action.
- Voice Command Processing: A tutorial on using OpenAI Whisper to transcribe spoken language into text and extract actionable commands.
- LLM-based Planning: A guide on how to prompt an LLM to translate a high-level natural language task (e.g., "get me the apple") into a sequence of concrete ROS 2 action goals.
- Mini-Capstone Project: A complete, runnable example demonstrating the full VLA loop. A user will issue a voice command, and a simulated humanoid robot will plan and execute a simple multi-step task involving navigation, perception, and manipulation.
2.2. Out of Scope
- The development of a production-grade, highly reliable robotics stack.
- Training or fine-tuning custom Large Language Models.
- Complex, long-horizon tasks or navigation in dynamic, multi-room environments.
2.3. Success Criteria
- Understanding: Students can draw a diagram of the VLA pipeline and explain the role of each component (Voice, LLM, ROS 2).
- Application (Whisper): Students are able to run a demo that successfully converts a spoken command into a text string.
- Application (LLM Planning): Students can explain how a natural language goal is decomposed into a JSON or YAML sequence of ROS 2 actions by an LLM.
- Completion: The mini-capstone example runs end-to-end: a voice command triggers a sequence of autonomous actions in the simulated robot.
2.4. Constraints
- Word Count: 2000‚Äì3000 words.
- Format: Docusaurus Markdown, including code snippets for Python (for Whisper and LLM interaction) and configuration files (for ROS 2 actions).
- Sources: Content will be based on official OpenAI Whisper documentation, ROS 2 action design patterns, and foundational VLA research.
- Timeline: 1 week.
3. UX/UI (Reader Experience)
3.1. Content Structure
The module will be structured to guide the learner from individual components to a fully integrated system.
- Chapter 1: Voice-to-Action: Whisper and Command Extraction
- Introduction to Whisper for speech-to-text.
- Tutorial: Setting up Whisper and running a simple voice transcription demo.
- Guide: Extracting key intents and entities from the transcribed text (e.g., turning "Can you please find the red block" into
{action: "find", object: "red block"}
).
- Chapter 2: Cognitive Planning with LLMs
- The role of LLMs as a "reasoning engine" for robots.
- Tutorial: Prompt engineering techniques to make an LLM translate a task into a machine-readable plan (a sequence of ROS 2 actions).
- Example: Converting
{action: "find", object: "red block"}
into a plan like[{navigate: "table"}, {detect: "red_block"}]
.
- Chapter 3: Mini-Capstone: The Full VLA Loop
- An overview of the complete system architecture.
- A step-by-step guide to launching all components (Whisper listener, LLM planner, ROS 2 action servers).
- A full demonstration of an autonomous task: e.g., "Go to the table, find the blue cup, and pick it up."
4. Technical Considerations
4.1. Key Decisions
- LLM Abstraction: We will use a major LLM provider's API (e.g., OpenAI, Anthropic, or Google) to avoid the complexity of self-hosting. The focus is on the integration, not the model itself.
- ROS 2 Actions: The module will use the standard ROS 2 action interface for all robot tasks (e.g.,
NavigateToPose
,DetectObject
,PickAndPlace
). This ensures a modular and scalable design. - Focus on Integration: The code provided will be primarily "glue code" written in Python, connecting the various components of the VLA pipeline.
4.2. External Dependencies
- Software: Python 3, ROS 2, and the OpenAI Whisper library.
- APIs: An API key for an LLM provider will be required for the cognitive planning chapter.
- Hardware: A microphone for voice commands.
- Prerequisites: Completion of prior modules, including ROS 2 basics, simulation, and navigation.
--------------------------------------------------------------------------------
URL: https://Muham251.github.io/ai-hackathon-1/docs/module-4/#42-external-dependencies
Content:
Spec: Module 4: Vision-Language-Action (VLA)
1. Business Understanding
1.1. Goal
The primary goal of this module is to teach students how to build a complete Vision-Language-Action (VLA) pipeline. This involves integrating Large Language Models (LLMs) with robotics, using Whisper for voice command input, employing LLMs for cognitive planning, and executing tasks through ROS 2 actions.
1.2. Justification
VLA is a transformative paradigm in robotics, enabling more natural human-robot interaction and a higher degree of autonomy. This module provides a crucial bridge between theoretical AI concepts (like LLMs) and practical, physical robotics, equipping students with highly sought-after skills in creating intelligent, responsive robots.
1.3. Target Audience
- Primary: Beginner to intermediate students in robotics and AI.
- Secondary: Developers and researchers interested in the practical application of LLMs in autonomous systems.
2. Functional Requirements
2.1. In Scope (Features)
- VLA Pipeline Overview: A clear, conceptual explanation of the entire Vision-Language-Action pipeline, from voice input to robot action.
- Voice Command Processing: A tutorial on using OpenAI Whisper to transcribe spoken language into text and extract actionable commands.
- LLM-based Planning: A guide on how to prompt an LLM to translate a high-level natural language task (e.g., "get me the apple") into a sequence of concrete ROS 2 action goals.
- Mini-Capstone Project: A complete, runnable example demonstrating the full VLA loop. A user will issue a voice command, and a simulated humanoid robot will plan and execute a simple multi-step task involving navigation, perception, and manipulation.
2.2. Out of Scope
- The development of a production-grade, highly reliable robotics stack.
- Training or fine-tuning custom Large Language Models.
- Complex, long-horizon tasks or navigation in dynamic, multi-room environments.
2.3. Success Criteria
- Understanding: Students can draw a diagram of the VLA pipeline and explain the role of each component (Voice, LLM, ROS 2).
- Application (Whisper): Students are able to run a demo that successfully converts a spoken command into a text string.
- Application (LLM Planning): Students can explain how a natural language goal is decomposed into a JSON or YAML sequence of ROS 2 actions by an LLM.
- Completion: The mini-capstone example runs end-to-end: a voice command triggers a sequence of autonomous actions in the simulated robot.
2.4. Constraints
- Word Count: 2000‚Äì3000 words.
- Format: Docusaurus Markdown, including code snippets for Python (for Whisper and LLM interaction) and configuration files (for ROS 2 actions).
- Sources: Content will be based on official OpenAI Whisper documentation, ROS 2 action design patterns, and foundational VLA research.
- Timeline: 1 week.
3. UX/UI (Reader Experience)
3.1. Content Structure
The module will be structured to guide the learner from individual components to a fully integrated system.
- Chapter 1: Voice-to-Action: Whisper and Command Extraction
- Introduction to Whisper for speech-to-text.
- Tutorial: Setting up Whisper and running a simple voice transcription demo.
- Guide: Extracting key intents and entities from the transcribed text (e.g., turning "Can you please find the red block" into
{action: "find", object: "red block"}
).
- Chapter 2: Cognitive Planning with LLMs
- The role of LLMs as a "reasoning engine" for robots.
- Tutorial: Prompt engineering techniques to make an LLM translate a task into a machine-readable plan (a sequence of ROS 2 actions).
- Example: Converting
{action: "find", object: "red block"}
into a plan like[{navigate: "table"}, {detect: "red_block"}]
.
- Chapter 3: Mini-Capstone: The Full VLA Loop
- An overview of the complete system architecture.
- A step-by-step guide to launching all components (Whisper listener, LLM planner, ROS 2 action servers).
- A full demonstration of an autonomous task: e.g., "Go to the table, find the blue cup, and pick it up."
4. Technical Considerations
4.1. Key Decisions
- LLM Abstraction: We will use a major LLM provider's API (e.g., OpenAI, Anthropic, or Google) to avoid the complexity of self-hosting. The focus is on the integration, not the model itself.
- ROS 2 Actions: The module will use the standard ROS 2 action interface for all robot tasks (e.g.,
NavigateToPose
,DetectObject
,PickAndPlace
). This ensures a modular and scalable design. - Focus on Integration: The code provided will be primarily "glue code" written in Python, connecting the various components of the VLA pipeline.
4.2. External Dependencies
- Software: Python 3, ROS 2, and the OpenAI Whisper library.
- APIs: An API key for an LLM provider will be required for the cognitive planning chapter.
- Hardware: A microphone for voice commands.
- Prerequisites: Completion of prior modules, including ROS 2 basics, simulation, and navigation.
--------------------------------------------------------------------------------
URL: https://Muham251.github.io/ai-hackathon-1/docs/module-3/#1-business-understanding
Content:
Spec: Module 3: The AI-Robot Brain (NVIDIA Isaac‚Ñ¢)
1. Business Understanding
1.1. Goal
The objective of this module is to educate beginner to intermediate robotics students on leveraging the NVIDIA Isaac‚Ñ¢ ecosystem to build a robot's core intelligence. The curriculum will focus on Isaac Sim for photorealistic simulation, Isaac ROS for perception and navigation (specifically VSLAM), and Nav2 for autonomous path planning.
1.2. Justification
The NVIDIA Isaac‚Ñ¢ platform is at the forefront of AI-driven robotics, offering powerful tools for developing and testing intelligent robots in realistic virtual environments. This module provides learners with critical, industry-relevant skills in simulation, perception, and navigation, forming the foundation of a robot's "brain."
1.3. Target Audience
- Primary: Beginner to intermediate students in robotics and AI who have a foundational understanding of ROS and simulation concepts.
- Secondary: Developers and engineers looking to get started with the NVIDIA Isaac‚Ñ¢ toolchain for robotics projects.
2. Functional Requirements
2.1. In Scope (Features)
- Isaac Sim Fundamentals:
- An introduction to creating scenes in Isaac Sim.
- A guide on generating synthetic data (e.g., camera images, LiDAR data) for training and testing.
- Isaac ROS Workflow:
- A conceptual overview of the Visual SLAM (VSLAM) and navigation pipeline provided by Isaac ROS.
- A tutorial on setting up and running the Isaac ROS navigation stack on a simulated robot.
- Nav2 Path Planning:
- A practical example demonstrating how to use Nav2 for path planning with a humanoid robot in Isaac Sim.
- Runnable Demos:
- The module must include 2‚Äì3 complete, runnable demonstrations that users can execute.
- Format: The final content will be delivered as a Docusaurus-compatible Markdown file.
2.2. Out of Scope
- A complete, end-to-end humanoid control pipeline (e.g., manipulation, complex behaviors).
- The development or deep-dive analysis of custom SLAM or navigation algorithms.
- Advanced Isaac Sim features like custom Python scripting or detailed RTX rendering settings.
2.3. Success Criteria
- Understanding: Learners can explain the significance of photorealistic simulation and synthetic data generation for robotics.
- Explanation: Learners can describe the high-level workflow of the Isaac ROS VSLAM and navigation stack.
- Application: Learners can successfully launch and run a basic Nav2 path-planning demonstration within Isaac Sim.
- Completion: The module is published with 2‚Äì3 fully functional and validated demos.
2.4. Constraints
- Word Count: Approximately 2000‚Äì3000 words.
- Format: Docusaurus Markdown with appropriate code blocks for commands and configurations.
- Sources: All technical information must be derived from and cite official NVIDIA Isaac‚Ñ¢ documentation.
- Timeline: The module is to be completed within a 1-week timeframe.
3. UX/UI (Reader Experience)
3.1. Content Structure
The module will be organized into a clear, three-chapter structure to guide the learner from environment creation to autonomous navigation.
- Chapter 1: Isaac Sim Basics: Scenes & Synthetic Data
- Introduction to the Isaac Sim interface and its core concepts.
- Tutorial: Setting up a basic scene with a robot and obstacles.
- Guide: Generating and exporting synthetic sensor data from the simulation.
- Chapter 2: Isaac ROS: VSLAM & Navigation Workflow
- Overview of the Isaac ROS ecosystem and its advantages.
- Step-by-step guide to launching the Isaac ROS Docker container.
- Tutorial: Running the VSLAM and navigation stack on a robot within Isaac Sim.
- Chapter 3: Nav2: Path Planning for Humanoids
- Introduction to Nav2 and its role in autonomous navigation.
- Example: Sending a navigation goal to a humanoid robot and having it execute a path using Nav2.
4. Technical Considerations
4.1. Key Decisions
- Ecosystem Focus: This module will exclusively use the NVIDIA Isaac‚Ñ¢ toolchain to provide a cohesive learning experience.
- Runnable Demos: Demos will be provided as shell scripts or clear, step-by-step command sequences to ensure they are easy to run.
- Configuration over Code: The focus will be on configuring and running existing Isaac ROS packages rather than writing new code.
4.2. External Dependencies
- Hardware: A computer with a compatible NVIDIA GPU is required.
- Software: Users must have NVIDIA Isaac Sim, Docker, and NVIDIA Container Toolkit installed.
- Prerequisites: A foundational knowledge of ROS 2 concepts is assumed (as taught in Module 1).
--------------------------------------------------------------------------------
URL: https://Muham251.github.io/ai-hackathon-1/docs/module-3/#11-goal
Content:
Spec: Module 3: The AI-Robot Brain (NVIDIA Isaac‚Ñ¢)
1. Business Understanding
1.1. Goal
The objective of this module is to educate beginner to intermediate robotics students on leveraging the NVIDIA Isaac‚Ñ¢ ecosystem to build a robot's core intelligence. The curriculum will focus on Isaac Sim for photorealistic simulation, Isaac ROS for perception and navigation (specifically VSLAM), and Nav2 for autonomous path planning.
1.2. Justification
The NVIDIA Isaac‚Ñ¢ platform is at the forefront of AI-driven robotics, offering powerful tools for developing and testing intelligent robots in realistic virtual environments. This module provides learners with critical, industry-relevant skills in simulation, perception, and navigation, forming the foundation of a robot's "brain."
1.3. Target Audience
- Primary: Beginner to intermediate students in robotics and AI who have a foundational understanding of ROS and simulation concepts.
- Secondary: Developers and engineers looking to get started with the NVIDIA Isaac‚Ñ¢ toolchain for robotics projects.
2. Functional Requirements
2.1. In Scope (Features)
- Isaac Sim Fundamentals:
- An introduction to creating scenes in Isaac Sim.
- A guide on generating synthetic data (e.g., camera images, LiDAR data) for training and testing.
- Isaac ROS Workflow:
- A conceptual overview of the Visual SLAM (VSLAM) and navigation pipeline provided by Isaac ROS.
- A tutorial on setting up and running the Isaac ROS navigation stack on a simulated robot.
- Nav2 Path Planning:
- A practical example demonstrating how to use Nav2 for path planning with a humanoid robot in Isaac Sim.
- Runnable Demos:
- The module must include 2‚Äì3 complete, runnable demonstrations that users can execute.
- Format: The final content will be delivered as a Docusaurus-compatible Markdown file.
2.2. Out of Scope
- A complete, end-to-end humanoid control pipeline (e.g., manipulation, complex behaviors).
- The development or deep-dive analysis of custom SLAM or navigation algorithms.
- Advanced Isaac Sim features like custom Python scripting or detailed RTX rendering settings.
2.3. Success Criteria
- Understanding: Learners can explain the significance of photorealistic simulation and synthetic data generation for robotics.
- Explanation: Learners can describe the high-level workflow of the Isaac ROS VSLAM and navigation stack.
- Application: Learners can successfully launch and run a basic Nav2 path-planning demonstration within Isaac Sim.
- Completion: The module is published with 2‚Äì3 fully functional and validated demos.
2.4. Constraints
- Word Count: Approximately 2000‚Äì3000 words.
- Format: Docusaurus Markdown with appropriate code blocks for commands and configurations.
- Sources: All technical information must be derived from and cite official NVIDIA Isaac‚Ñ¢ documentation.
- Timeline: The module is to be completed within a 1-week timeframe.
3. UX/UI (Reader Experience)
3.1. Content Structure
The module will be organized into a clear, three-chapter structure to guide the learner from environment creation to autonomous navigation.
- Chapter 1: Isaac Sim Basics: Scenes & Synthetic Data
- Introduction to the Isaac Sim interface and its core concepts.
- Tutorial: Setting up a basic scene with a robot and obstacles.
- Guide: Generating and exporting synthetic sensor data from the simulation.
- Chapter 2: Isaac ROS: VSLAM & Navigation Workflow
- Overview of the Isaac ROS ecosystem and its advantages.
- Step-by-step guide to launching the Isaac ROS Docker container.
- Tutorial: Running the VSLAM and navigation stack on a robot within Isaac Sim.
- Chapter 3: Nav2: Path Planning for Humanoids
- Introduction to Nav2 and its role in autonomous navigation.
- Example: Sending a navigation goal to a humanoid robot and having it execute a path using Nav2.
4. Technical Considerations
4.1. Key Decisions
- Ecosystem Focus: This module will exclusively use the NVIDIA Isaac‚Ñ¢ toolchain to provide a cohesive learning experience.
- Runnable Demos: Demos will be provided as shell scripts or clear, step-by-step command sequences to ensure they are easy to run.
- Configuration over Code: The focus will be on configuring and running existing Isaac ROS packages rather than writing new code.
4.2. External Dependencies
- Hardware: A computer with a compatible NVIDIA GPU is required.
- Software: Users must have NVIDIA Isaac Sim, Docker, and NVIDIA Container Toolkit installed.
- Prerequisites: A foundational knowledge of ROS 2 concepts is assumed (as taught in Module 1).
--------------------------------------------------------------------------------
URL: https://Muham251.github.io/ai-hackathon-1/docs/module-3/#12-justification
Content:
Spec: Module 3: The AI-Robot Brain (NVIDIA Isaac‚Ñ¢)
1. Business Understanding
1.1. Goal
The objective of this module is to educate beginner to intermediate robotics students on leveraging the NVIDIA Isaac‚Ñ¢ ecosystem to build a robot's core intelligence. The curriculum will focus on Isaac Sim for photorealistic simulation, Isaac ROS for perception and navigation (specifically VSLAM), and Nav2 for autonomous path planning.
1.2. Justification
The NVIDIA Isaac‚Ñ¢ platform is at the forefront of AI-driven robotics, offering powerful tools for developing and testing intelligent robots in realistic virtual environments. This module provides learners with critical, industry-relevant skills in simulation, perception, and navigation, forming the foundation of a robot's "brain."
1.3. Target Audience
- Primary: Beginner to intermediate students in robotics and AI who have a foundational understanding of ROS and simulation concepts.
- Secondary: Developers and engineers looking to get started with the NVIDIA Isaac‚Ñ¢ toolchain for robotics projects.
2. Functional Requirements
2.1. In Scope (Features)
- Isaac Sim Fundamentals:
- An introduction to creating scenes in Isaac Sim.
- A guide on generating synthetic data (e.g., camera images, LiDAR data) for training and testing.
- Isaac ROS Workflow:
- A conceptual overview of the Visual SLAM (VSLAM) and navigation pipeline provided by Isaac ROS.
- A tutorial on setting up and running the Isaac ROS navigation stack on a simulated robot.
- Nav2 Path Planning:
- A practical example demonstrating how to use Nav2 for path planning with a humanoid robot in Isaac Sim.
- Runnable Demos:
- The module must include 2‚Äì3 complete, runnable demonstrations that users can execute.
- Format: The final content will be delivered as a Docusaurus-compatible Markdown file.
2.2. Out of Scope
- A complete, end-to-end humanoid control pipeline (e.g., manipulation, complex behaviors).
- The development or deep-dive analysis of custom SLAM or navigation algorithms.
- Advanced Isaac Sim features like custom Python scripting or detailed RTX rendering settings.
2.3. Success Criteria
- Understanding: Learners can explain the significance of photorealistic simulation and synthetic data generation for robotics.
- Explanation: Learners can describe the high-level workflow of the Isaac ROS VSLAM and navigation stack.
- Application: Learners can successfully launch and run a basic Nav2 path-planning demonstration within Isaac Sim.
- Completion: The module is published with 2‚Äì3 fully functional and validated demos.
2.4. Constraints
- Word Count: Approximately 2000‚Äì3000 words.
- Format: Docusaurus Markdown with appropriate code blocks for commands and configurations.
- Sources: All technical information must be derived from and cite official NVIDIA Isaac‚Ñ¢ documentation.
- Timeline: The module is to be completed within a 1-week timeframe.
3. UX/UI (Reader Experience)
3.1. Content Structure
The module will be organized into a clear, three-chapter structure to guide the learner from environment creation to autonomous navigation.
- Chapter 1: Isaac Sim Basics: Scenes & Synthetic Data
- Introduction to the Isaac Sim interface and its core concepts.
- Tutorial: Setting up a basic scene with a robot and obstacles.
- Guide: Generating and exporting synthetic sensor data from the simulation.
- Chapter 2: Isaac ROS: VSLAM & Navigation Workflow
- Overview of the Isaac ROS ecosystem and its advantages.
- Step-by-step guide to launching the Isaac ROS Docker container.
- Tutorial: Running the VSLAM and navigation stack on a robot within Isaac Sim.
- Chapter 3: Nav2: Path Planning for Humanoids
- Introduction to Nav2 and its role in autonomous navigation.
- Example: Sending a navigation goal to a humanoid robot and having it execute a path using Nav2.
4. Technical Considerations
4.1. Key Decisions
- Ecosystem Focus: This module will exclusively use the NVIDIA Isaac‚Ñ¢ toolchain to provide a cohesive learning experience.
- Runnable Demos: Demos will be provided as shell scripts or clear, step-by-step command sequences to ensure they are easy to run.
- Configuration over Code: The focus will be on configuring and running existing Isaac ROS packages rather than writing new code.
4.2. External Dependencies
- Hardware: A computer with a compatible NVIDIA GPU is required.
- Software: Users must have NVIDIA Isaac Sim, Docker, and NVIDIA Container Toolkit installed.
- Prerequisites: A foundational knowledge of ROS 2 concepts is assumed (as taught in Module 1).
--------------------------------------------------------------------------------
URL: https://Muham251.github.io/ai-hackathon-1/docs/module-3/#13-target-audience
Content:
Spec: Module 3: The AI-Robot Brain (NVIDIA Isaac‚Ñ¢)
1. Business Understanding
1.1. Goal
The objective of this module is to educate beginner to intermediate robotics students on leveraging the NVIDIA Isaac‚Ñ¢ ecosystem to build a robot's core intelligence. The curriculum will focus on Isaac Sim for photorealistic simulation, Isaac ROS for perception and navigation (specifically VSLAM), and Nav2 for autonomous path planning.
1.2. Justification
The NVIDIA Isaac‚Ñ¢ platform is at the forefront of AI-driven robotics, offering powerful tools for developing and testing intelligent robots in realistic virtual environments. This module provides learners with critical, industry-relevant skills in simulation, perception, and navigation, forming the foundation of a robot's "brain."
1.3. Target Audience
- Primary: Beginner to intermediate students in robotics and AI who have a foundational understanding of ROS and simulation concepts.
- Secondary: Developers and engineers looking to get started with the NVIDIA Isaac‚Ñ¢ toolchain for robotics projects.
2. Functional Requirements
2.1. In Scope (Features)
- Isaac Sim Fundamentals:
- An introduction to creating scenes in Isaac Sim.
- A guide on generating synthetic data (e.g., camera images, LiDAR data) for training and testing.
- Isaac ROS Workflow:
- A conceptual overview of the Visual SLAM (VSLAM) and navigation pipeline provided by Isaac ROS.
- A tutorial on setting up and running the Isaac ROS navigation stack on a simulated robot.
- Nav2 Path Planning:
- A practical example demonstrating how to use Nav2 for path planning with a humanoid robot in Isaac Sim.
- Runnable Demos:
- The module must include 2‚Äì3 complete, runnable demonstrations that users can execute.
- Format: The final content will be delivered as a Docusaurus-compatible Markdown file.
2.2. Out of Scope
- A complete, end-to-end humanoid control pipeline (e.g., manipulation, complex behaviors).
- The development or deep-dive analysis of custom SLAM or navigation algorithms.
- Advanced Isaac Sim features like custom Python scripting or detailed RTX rendering settings.
2.3. Success Criteria
- Understanding: Learners can explain the significance of photorealistic simulation and synthetic data generation for robotics.
- Explanation: Learners can describe the high-level workflow of the Isaac ROS VSLAM and navigation stack.
- Application: Learners can successfully launch and run a basic Nav2 path-planning demonstration within Isaac Sim.
- Completion: The module is published with 2‚Äì3 fully functional and validated demos.
2.4. Constraints
- Word Count: Approximately 2000‚Äì3000 words.
- Format: Docusaurus Markdown with appropriate code blocks for commands and configurations.
- Sources: All technical information must be derived from and cite official NVIDIA Isaac‚Ñ¢ documentation.
- Timeline: The module is to be completed within a 1-week timeframe.
3. UX/UI (Reader Experience)
3.1. Content Structure
The module will be organized into a clear, three-chapter structure to guide the learner from environment creation to autonomous navigation.
- Chapter 1: Isaac Sim Basics: Scenes & Synthetic Data
- Introduction to the Isaac Sim interface and its core concepts.
- Tutorial: Setting up a basic scene with a robot and obstacles.
- Guide: Generating and exporting synthetic sensor data from the simulation.
- Chapter 2: Isaac ROS: VSLAM & Navigation Workflow
- Overview of the Isaac ROS ecosystem and its advantages.
- Step-by-step guide to launching the Isaac ROS Docker container.
- Tutorial: Running the VSLAM and navigation stack on a robot within Isaac Sim.
- Chapter 3: Nav2: Path Planning for Humanoids
- Introduction to Nav2 and its role in autonomous navigation.
- Example: Sending a navigation goal to a humanoid robot and having it execute a path using Nav2.
4. Technical Considerations
4.1. Key Decisions
- Ecosystem Focus: This module will exclusively use the NVIDIA Isaac‚Ñ¢ toolchain to provide a cohesive learning experience.
- Runnable Demos: Demos will be provided as shell scripts or clear, step-by-step command sequences to ensure they are easy to run.
- Configuration over Code: The focus will be on configuring and running existing Isaac ROS packages rather than writing new code.
4.2. External Dependencies
- Hardware: A computer with a compatible NVIDIA GPU is required.
- Software: Users must have NVIDIA Isaac Sim, Docker, and NVIDIA Container Toolkit installed.
- Prerequisites: A foundational knowledge of ROS 2 concepts is assumed (as taught in Module 1).
--------------------------------------------------------------------------------
URL: https://Muham251.github.io/ai-hackathon-1/docs/module-3/#2-functional-requirements
Content:
Spec: Module 3: The AI-Robot Brain (NVIDIA Isaac‚Ñ¢)
1. Business Understanding
1.1. Goal
The objective of this module is to educate beginner to intermediate robotics students on leveraging the NVIDIA Isaac‚Ñ¢ ecosystem to build a robot's core intelligence. The curriculum will focus on Isaac Sim for photorealistic simulation, Isaac ROS for perception and navigation (specifically VSLAM), and Nav2 for autonomous path planning.
1.2. Justification
The NVIDIA Isaac‚Ñ¢ platform is at the forefront of AI-driven robotics, offering powerful tools for developing and testing intelligent robots in realistic virtual environments. This module provides learners with critical, industry-relevant skills in simulation, perception, and navigation, forming the foundation of a robot's "brain."
1.3. Target Audience
- Primary: Beginner to intermediate students in robotics and AI who have a foundational understanding of ROS and simulation concepts.
- Secondary: Developers and engineers looking to get started with the NVIDIA Isaac‚Ñ¢ toolchain for robotics projects.
2. Functional Requirements
2.1. In Scope (Features)
- Isaac Sim Fundamentals:
- An introduction to creating scenes in Isaac Sim.
- A guide on generating synthetic data (e.g., camera images, LiDAR data) for training and testing.
- Isaac ROS Workflow:
- A conceptual overview of the Visual SLAM (VSLAM) and navigation pipeline provided by Isaac ROS.
- A tutorial on setting up and running the Isaac ROS navigation stack on a simulated robot.
- Nav2 Path Planning:
- A practical example demonstrating how to use Nav2 for path planning with a humanoid robot in Isaac Sim.
- Runnable Demos:
- The module must include 2‚Äì3 complete, runnable demonstrations that users can execute.
- Format: The final content will be delivered as a Docusaurus-compatible Markdown file.
2.2. Out of Scope
- A complete, end-to-end humanoid control pipeline (e.g., manipulation, complex behaviors).
- The development or deep-dive analysis of custom SLAM or navigation algorithms.
- Advanced Isaac Sim features like custom Python scripting or detailed RTX rendering settings.
2.3. Success Criteria
- Understanding: Learners can explain the significance of photorealistic simulation and synthetic data generation for robotics.
- Explanation: Learners can describe the high-level workflow of the Isaac ROS VSLAM and navigation stack.
- Application: Learners can successfully launch and run a basic Nav2 path-planning demonstration within Isaac Sim.
- Completion: The module is published with 2‚Äì3 fully functional and validated demos.
2.4. Constraints
- Word Count: Approximately 2000‚Äì3000 words.
- Format: Docusaurus Markdown with appropriate code blocks for commands and configurations.
- Sources: All technical information must be derived from and cite official NVIDIA Isaac‚Ñ¢ documentation.
- Timeline: The module is to be completed within a 1-week timeframe.
3. UX/UI (Reader Experience)
3.1. Content Structure
The module will be organized into a clear, three-chapter structure to guide the learner from environment creation to autonomous navigation.
- Chapter 1: Isaac Sim Basics: Scenes & Synthetic Data
- Introduction to the Isaac Sim interface and its core concepts.
- Tutorial: Setting up a basic scene with a robot and obstacles.
- Guide: Generating and exporting synthetic sensor data from the simulation.
- Chapter 2: Isaac ROS: VSLAM & Navigation Workflow
- Overview of the Isaac ROS ecosystem and its advantages.
- Step-by-step guide to launching the Isaac ROS Docker container.
- Tutorial: Running the VSLAM and navigation stack on a robot within Isaac Sim.
- Chapter 3: Nav2: Path Planning for Humanoids
- Introduction to Nav2 and its role in autonomous navigation.
- Example: Sending a navigation goal to a humanoid robot and having it execute a path using Nav2.
4. Technical Considerations
4.1. Key Decisions
- Ecosystem Focus: This module will exclusively use the NVIDIA Isaac‚Ñ¢ toolchain to provide a cohesive learning experience.
- Runnable Demos: Demos will be provided as shell scripts or clear, step-by-step command sequences to ensure they are easy to run.
- Configuration over Code: The focus will be on configuring and running existing Isaac ROS packages rather than writing new code.
4.2. External Dependencies
- Hardware: A computer with a compatible NVIDIA GPU is required.
- Software: Users must have NVIDIA Isaac Sim, Docker, and NVIDIA Container Toolkit installed.
- Prerequisites: A foundational knowledge of ROS 2 concepts is assumed (as taught in Module 1).
--------------------------------------------------------------------------------
URL: https://Muham251.github.io/ai-hackathon-1/docs/module-3/#21-in-scope-features
Content:
Spec: Module 3: The AI-Robot Brain (NVIDIA Isaac‚Ñ¢)
1. Business Understanding
1.1. Goal
The objective of this module is to educate beginner to intermediate robotics students on leveraging the NVIDIA Isaac‚Ñ¢ ecosystem to build a robot's core intelligence. The curriculum will focus on Isaac Sim for photorealistic simulation, Isaac ROS for perception and navigation (specifically VSLAM), and Nav2 for autonomous path planning.
1.2. Justification
The NVIDIA Isaac‚Ñ¢ platform is at the forefront of AI-driven robotics, offering powerful tools for developing and testing intelligent robots in realistic virtual environments. This module provides learners with critical, industry-relevant skills in simulation, perception, and navigation, forming the foundation of a robot's "brain."
1.3. Target Audience
- Primary: Beginner to intermediate students in robotics and AI who have a foundational understanding of ROS and simulation concepts.
- Secondary: Developers and engineers looking to get started with the NVIDIA Isaac‚Ñ¢ toolchain for robotics projects.
2. Functional Requirements
2.1. In Scope (Features)
- Isaac Sim Fundamentals:
- An introduction to creating scenes in Isaac Sim.
- A guide on generating synthetic data (e.g., camera images, LiDAR data) for training and testing.
- Isaac ROS Workflow:
- A conceptual overview of the Visual SLAM (VSLAM) and navigation pipeline provided by Isaac ROS.
- A tutorial on setting up and running the Isaac ROS navigation stack on a simulated robot.
- Nav2 Path Planning:
- A practical example demonstrating how to use Nav2 for path planning with a humanoid robot in Isaac Sim.
- Runnable Demos:
- The module must include 2‚Äì3 complete, runnable demonstrations that users can execute.
- Format: The final content will be delivered as a Docusaurus-compatible Markdown file.
2.2. Out of Scope
- A complete, end-to-end humanoid control pipeline (e.g., manipulation, complex behaviors).
- The development or deep-dive analysis of custom SLAM or navigation algorithms.
- Advanced Isaac Sim features like custom Python scripting or detailed RTX rendering settings.
2.3. Success Criteria
- Understanding: Learners can explain the significance of photorealistic simulation and synthetic data generation for robotics.
- Explanation: Learners can describe the high-level workflow of the Isaac ROS VSLAM and navigation stack.
- Application: Learners can successfully launch and run a basic Nav2 path-planning demonstration within Isaac Sim.
- Completion: The module is published with 2‚Äì3 fully functional and validated demos.
2.4. Constraints
- Word Count: Approximately 2000‚Äì3000 words.
- Format: Docusaurus Markdown with appropriate code blocks for commands and configurations.
- Sources: All technical information must be derived from and cite official NVIDIA Isaac‚Ñ¢ documentation.
- Timeline: The module is to be completed within a 1-week timeframe.
3. UX/UI (Reader Experience)
3.1. Content Structure
The module will be organized into a clear, three-chapter structure to guide the learner from environment creation to autonomous navigation.
- Chapter 1: Isaac Sim Basics: Scenes & Synthetic Data
- Introduction to the Isaac Sim interface and its core concepts.
- Tutorial: Setting up a basic scene with a robot and obstacles.
- Guide: Generating and exporting synthetic sensor data from the simulation.
- Chapter 2: Isaac ROS: VSLAM & Navigation Workflow
- Overview of the Isaac ROS ecosystem and its advantages.
- Step-by-step guide to launching the Isaac ROS Docker container.
- Tutorial: Running the VSLAM and navigation stack on a robot within Isaac Sim.
- Chapter 3: Nav2: Path Planning for Humanoids
- Introduction to Nav2 and its role in autonomous navigation.
- Example: Sending a navigation goal to a humanoid robot and having it execute a path using Nav2.
4. Technical Considerations
4.1. Key Decisions
- Ecosystem Focus: This module will exclusively use the NVIDIA Isaac‚Ñ¢ toolchain to provide a cohesive learning experience.
- Runnable Demos: Demos will be provided as shell scripts or clear, step-by-step command sequences to ensure they are easy to run.
- Configuration over Code: The focus will be on configuring and running existing Isaac ROS packages rather than writing new code.
4.2. External Dependencies
- Hardware: A computer with a compatible NVIDIA GPU is required.
- Software: Users must have NVIDIA Isaac Sim, Docker, and NVIDIA Container Toolkit installed.
- Prerequisites: A foundational knowledge of ROS 2 concepts is assumed (as taught in Module 1).
--------------------------------------------------------------------------------
URL: https://Muham251.github.io/ai-hackathon-1/docs/module-3/#22-out-of-scope
Content:
Spec: Module 3: The AI-Robot Brain (NVIDIA Isaac‚Ñ¢)
1. Business Understanding
1.1. Goal
The objective of this module is to educate beginner to intermediate robotics students on leveraging the NVIDIA Isaac‚Ñ¢ ecosystem to build a robot's core intelligence. The curriculum will focus on Isaac Sim for photorealistic simulation, Isaac ROS for perception and navigation (specifically VSLAM), and Nav2 for autonomous path planning.
1.2. Justification
The NVIDIA Isaac‚Ñ¢ platform is at the forefront of AI-driven robotics, offering powerful tools for developing and testing intelligent robots in realistic virtual environments. This module provides learners with critical, industry-relevant skills in simulation, perception, and navigation, forming the foundation of a robot's "brain."
1.3. Target Audience
- Primary: Beginner to intermediate students in robotics and AI who have a foundational understanding of ROS and simulation concepts.
- Secondary: Developers and engineers looking to get started with the NVIDIA Isaac‚Ñ¢ toolchain for robotics projects.
2. Functional Requirements
2.1. In Scope (Features)
- Isaac Sim Fundamentals:
- An introduction to creating scenes in Isaac Sim.
- A guide on generating synthetic data (e.g., camera images, LiDAR data) for training and testing.
- Isaac ROS Workflow:
- A conceptual overview of the Visual SLAM (VSLAM) and navigation pipeline provided by Isaac ROS.
- A tutorial on setting up and running the Isaac ROS navigation stack on a simulated robot.
- Nav2 Path Planning:
- A practical example demonstrating how to use Nav2 for path planning with a humanoid robot in Isaac Sim.
- Runnable Demos:
- The module must include 2‚Äì3 complete, runnable demonstrations that users can execute.
- Format: The final content will be delivered as a Docusaurus-compatible Markdown file.
2.2. Out of Scope
- A complete, end-to-end humanoid control pipeline (e.g., manipulation, complex behaviors).
- The development or deep-dive analysis of custom SLAM or navigation algorithms.
- Advanced Isaac Sim features like custom Python scripting or detailed RTX rendering settings.
2.3. Success Criteria
- Understanding: Learners can explain the significance of photorealistic simulation and synthetic data generation for robotics.
- Explanation: Learners can describe the high-level workflow of the Isaac ROS VSLAM and navigation stack.
- Application: Learners can successfully launch and run a basic Nav2 path-planning demonstration within Isaac Sim.
- Completion: The module is published with 2‚Äì3 fully functional and validated demos.
2.4. Constraints
- Word Count: Approximately 2000‚Äì3000 words.
- Format: Docusaurus Markdown with appropriate code blocks for commands and configurations.
- Sources: All technical information must be derived from and cite official NVIDIA Isaac‚Ñ¢ documentation.
- Timeline: The module is to be completed within a 1-week timeframe.
3. UX/UI (Reader Experience)
3.1. Content Structure
The module will be organized into a clear, three-chapter structure to guide the learner from environment creation to autonomous navigation.
- Chapter 1: Isaac Sim Basics: Scenes & Synthetic Data
- Introduction to the Isaac Sim interface and its core concepts.
- Tutorial: Setting up a basic scene with a robot and obstacles.
- Guide: Generating and exporting synthetic sensor data from the simulation.
- Chapter 2: Isaac ROS: VSLAM & Navigation Workflow
- Overview of the Isaac ROS ecosystem and its advantages.
- Step-by-step guide to launching the Isaac ROS Docker container.
- Tutorial: Running the VSLAM and navigation stack on a robot within Isaac Sim.
- Chapter 3: Nav2: Path Planning for Humanoids
- Introduction to Nav2 and its role in autonomous navigation.
- Example: Sending a navigation goal to a humanoid robot and having it execute a path using Nav2.
4. Technical Considerations
4.1. Key Decisions
- Ecosystem Focus: This module will exclusively use the NVIDIA Isaac‚Ñ¢ toolchain to provide a cohesive learning experience.
- Runnable Demos: Demos will be provided as shell scripts or clear, step-by-step command sequences to ensure they are easy to run.
- Configuration over Code: The focus will be on configuring and running existing Isaac ROS packages rather than writing new code.
4.2. External Dependencies
- Hardware: A computer with a compatible NVIDIA GPU is required.
- Software: Users must have NVIDIA Isaac Sim, Docker, and NVIDIA Container Toolkit installed.
- Prerequisites: A foundational knowledge of ROS 2 concepts is assumed (as taught in Module 1).
--------------------------------------------------------------------------------
URL: https://Muham251.github.io/ai-hackathon-1/docs/module-3/#23-success-criteria
Content:
Spec: Module 3: The AI-Robot Brain (NVIDIA Isaac‚Ñ¢)
1. Business Understanding
1.1. Goal
The objective of this module is to educate beginner to intermediate robotics students on leveraging the NVIDIA Isaac‚Ñ¢ ecosystem to build a robot's core intelligence. The curriculum will focus on Isaac Sim for photorealistic simulation, Isaac ROS for perception and navigation (specifically VSLAM), and Nav2 for autonomous path planning.
1.2. Justification
The NVIDIA Isaac‚Ñ¢ platform is at the forefront of AI-driven robotics, offering powerful tools for developing and testing intelligent robots in realistic virtual environments. This module provides learners with critical, industry-relevant skills in simulation, perception, and navigation, forming the foundation of a robot's "brain."
1.3. Target Audience
- Primary: Beginner to intermediate students in robotics and AI who have a foundational understanding of ROS and simulation concepts.
- Secondary: Developers and engineers looking to get started with the NVIDIA Isaac‚Ñ¢ toolchain for robotics projects.
2. Functional Requirements
2.1. In Scope (Features)
- Isaac Sim Fundamentals:
- An introduction to creating scenes in Isaac Sim.
- A guide on generating synthetic data (e.g., camera images, LiDAR data) for training and testing.
- Isaac ROS Workflow:
- A conceptual overview of the Visual SLAM (VSLAM) and navigation pipeline provided by Isaac ROS.
- A tutorial on setting up and running the Isaac ROS navigation stack on a simulated robot.
- Nav2 Path Planning:
- A practical example demonstrating how to use Nav2 for path planning with a humanoid robot in Isaac Sim.
- Runnable Demos:
- The module must include 2‚Äì3 complete, runnable demonstrations that users can execute.
- Format: The final content will be delivered as a Docusaurus-compatible Markdown file.
2.2. Out of Scope
- A complete, end-to-end humanoid control pipeline (e.g., manipulation, complex behaviors).
- The development or deep-dive analysis of custom SLAM or navigation algorithms.
- Advanced Isaac Sim features like custom Python scripting or detailed RTX rendering settings.
2.3. Success Criteria
- Understanding: Learners can explain the significance of photorealistic simulation and synthetic data generation for robotics.
- Explanation: Learners can describe the high-level workflow of the Isaac ROS VSLAM and navigation stack.
- Application: Learners can successfully launch and run a basic Nav2 path-planning demonstration within Isaac Sim.
- Completion: The module is published with 2‚Äì3 fully functional and validated demos.
2.4. Constraints
- Word Count: Approximately 2000‚Äì3000 words.
- Format: Docusaurus Markdown with appropriate code blocks for commands and configurations.
- Sources: All technical information must be derived from and cite official NVIDIA Isaac‚Ñ¢ documentation.
- Timeline: The module is to be completed within a 1-week timeframe.
3. UX/UI (Reader Experience)
3.1. Content Structure
The module will be organized into a clear, three-chapter structure to guide the learner from environment creation to autonomous navigation.
- Chapter 1: Isaac Sim Basics: Scenes & Synthetic Data
- Introduction to the Isaac Sim interface and its core concepts.
- Tutorial: Setting up a basic scene with a robot and obstacles.
- Guide: Generating and exporting synthetic sensor data from the simulation.
- Chapter 2: Isaac ROS: VSLAM & Navigation Workflow
- Overview of the Isaac ROS ecosystem and its advantages.
- Step-by-step guide to launching the Isaac ROS Docker container.
- Tutorial: Running the VSLAM and navigation stack on a robot within Isaac Sim.
- Chapter 3: Nav2: Path Planning for Humanoids
- Introduction to Nav2 and its role in autonomous navigation.
- Example: Sending a navigation goal to a humanoid robot and having it execute a path using Nav2.
4. Technical Considerations
4.1. Key Decisions
- Ecosystem Focus: This module will exclusively use the NVIDIA Isaac‚Ñ¢ toolchain to provide a cohesive learning experience.
- Runnable Demos: Demos will be provided as shell scripts or clear, step-by-step command sequences to ensure they are easy to run.
- Configuration over Code: The focus will be on configuring and running existing Isaac ROS packages rather than writing new code.
4.2. External Dependencies
- Hardware: A computer with a compatible NVIDIA GPU is required.
- Software: Users must have NVIDIA Isaac Sim, Docker, and NVIDIA Container Toolkit installed.
- Prerequisites: A foundational knowledge of ROS 2 concepts is assumed (as taught in Module 1).
--------------------------------------------------------------------------------
URL: https://Muham251.github.io/ai-hackathon-1/docs/module-3/#24-constraints
Content:
Spec: Module 3: The AI-Robot Brain (NVIDIA Isaac‚Ñ¢)
1. Business Understanding
1.1. Goal
The objective of this module is to educate beginner to intermediate robotics students on leveraging the NVIDIA Isaac‚Ñ¢ ecosystem to build a robot's core intelligence. The curriculum will focus on Isaac Sim for photorealistic simulation, Isaac ROS for perception and navigation (specifically VSLAM), and Nav2 for autonomous path planning.
1.2. Justification
The NVIDIA Isaac‚Ñ¢ platform is at the forefront of AI-driven robotics, offering powerful tools for developing and testing intelligent robots in realistic virtual environments. This module provides learners with critical, industry-relevant skills in simulation, perception, and navigation, forming the foundation of a robot's "brain."
1.3. Target Audience
- Primary: Beginner to intermediate students in robotics and AI who have a foundational understanding of ROS and simulation concepts.
- Secondary: Developers and engineers looking to get started with the NVIDIA Isaac‚Ñ¢ toolchain for robotics projects.
2. Functional Requirements
2.1. In Scope (Features)
- Isaac Sim Fundamentals:
- An introduction to creating scenes in Isaac Sim.
- A guide on generating synthetic data (e.g., camera images, LiDAR data) for training and testing.
- Isaac ROS Workflow:
- A conceptual overview of the Visual SLAM (VSLAM) and navigation pipeline provided by Isaac ROS.
- A tutorial on setting up and running the Isaac ROS navigation stack on a simulated robot.
- Nav2 Path Planning:
- A practical example demonstrating how to use Nav2 for path planning with a humanoid robot in Isaac Sim.
- Runnable Demos:
- The module must include 2‚Äì3 complete, runnable demonstrations that users can execute.
- Format: The final content will be delivered as a Docusaurus-compatible Markdown file.
2.2. Out of Scope
- A complete, end-to-end humanoid control pipeline (e.g., manipulation, complex behaviors).
- The development or deep-dive analysis of custom SLAM or navigation algorithms.
- Advanced Isaac Sim features like custom Python scripting or detailed RTX rendering settings.
2.3. Success Criteria
- Understanding: Learners can explain the significance of photorealistic simulation and synthetic data generation for robotics.
- Explanation: Learners can describe the high-level workflow of the Isaac ROS VSLAM and navigation stack.
- Application: Learners can successfully launch and run a basic Nav2 path-planning demonstration within Isaac Sim.
- Completion: The module is published with 2‚Äì3 fully functional and validated demos.
2.4. Constraints
- Word Count: Approximately 2000‚Äì3000 words.
- Format: Docusaurus Markdown with appropriate code blocks for commands and configurations.
- Sources: All technical information must be derived from and cite official NVIDIA Isaac‚Ñ¢ documentation.
- Timeline: The module is to be completed within a 1-week timeframe.
3. UX/UI (Reader Experience)
3.1. Content Structure
The module will be organized into a clear, three-chapter structure to guide the learner from environment creation to autonomous navigation.
- Chapter 1: Isaac Sim Basics: Scenes & Synthetic Data
- Introduction to the Isaac Sim interface and its core concepts.
- Tutorial: Setting up a basic scene with a robot and obstacles.
- Guide: Generating and exporting synthetic sensor data from the simulation.
- Chapter 2: Isaac ROS: VSLAM & Navigation Workflow
- Overview of the Isaac ROS ecosystem and its advantages.
- Step-by-step guide to launching the Isaac ROS Docker container.
- Tutorial: Running the VSLAM and navigation stack on a robot within Isaac Sim.
- Chapter 3: Nav2: Path Planning for Humanoids
- Introduction to Nav2 and its role in autonomous navigation.
- Example: Sending a navigation goal to a humanoid robot and having it execute a path using Nav2.
4. Technical Considerations
4.1. Key Decisions
- Ecosystem Focus: This module will exclusively use the NVIDIA Isaac‚Ñ¢ toolchain to provide a cohesive learning experience.
- Runnable Demos: Demos will be provided as shell scripts or clear, step-by-step command sequences to ensure they are easy to run.
- Configuration over Code: The focus will be on configuring and running existing Isaac ROS packages rather than writing new code.
4.2. External Dependencies
- Hardware: A computer with a compatible NVIDIA GPU is required.
- Software: Users must have NVIDIA Isaac Sim, Docker, and NVIDIA Container Toolkit installed.
- Prerequisites: A foundational knowledge of ROS 2 concepts is assumed (as taught in Module 1).
--------------------------------------------------------------------------------
URL: https://Muham251.github.io/ai-hackathon-1/docs/module-3/#3-uxui-reader-experience
Content:
Spec: Module 3: The AI-Robot Brain (NVIDIA Isaac‚Ñ¢)
1. Business Understanding
1.1. Goal
The objective of this module is to educate beginner to intermediate robotics students on leveraging the NVIDIA Isaac‚Ñ¢ ecosystem to build a robot's core intelligence. The curriculum will focus on Isaac Sim for photorealistic simulation, Isaac ROS for perception and navigation (specifically VSLAM), and Nav2 for autonomous path planning.
1.2. Justification
The NVIDIA Isaac‚Ñ¢ platform is at the forefront of AI-driven robotics, offering powerful tools for developing and testing intelligent robots in realistic virtual environments. This module provides learners with critical, industry-relevant skills in simulation, perception, and navigation, forming the foundation of a robot's "brain."
1.3. Target Audience
- Primary: Beginner to intermediate students in robotics and AI who have a foundational understanding of ROS and simulation concepts.
- Secondary: Developers and engineers looking to get started with the NVIDIA Isaac‚Ñ¢ toolchain for robotics projects.
2. Functional Requirements
2.1. In Scope (Features)
- Isaac Sim Fundamentals:
- An introduction to creating scenes in Isaac Sim.
- A guide on generating synthetic data (e.g., camera images, LiDAR data) for training and testing.
- Isaac ROS Workflow:
- A conceptual overview of the Visual SLAM (VSLAM) and navigation pipeline provided by Isaac ROS.
- A tutorial on setting up and running the Isaac ROS navigation stack on a simulated robot.
- Nav2 Path Planning:
- A practical example demonstrating how to use Nav2 for path planning with a humanoid robot in Isaac Sim.
- Runnable Demos:
- The module must include 2‚Äì3 complete, runnable demonstrations that users can execute.
- Format: The final content will be delivered as a Docusaurus-compatible Markdown file.
2.2. Out of Scope
- A complete, end-to-end humanoid control pipeline (e.g., manipulation, complex behaviors).
- The development or deep-dive analysis of custom SLAM or navigation algorithms.
- Advanced Isaac Sim features like custom Python scripting or detailed RTX rendering settings.
2.3. Success Criteria
- Understanding: Learners can explain the significance of photorealistic simulation and synthetic data generation for robotics.
- Explanation: Learners can describe the high-level workflow of the Isaac ROS VSLAM and navigation stack.
- Application: Learners can successfully launch and run a basic Nav2 path-planning demonstration within Isaac Sim.
- Completion: The module is published with 2‚Äì3 fully functional and validated demos.
2.4. Constraints
- Word Count: Approximately 2000‚Äì3000 words.
- Format: Docusaurus Markdown with appropriate code blocks for commands and configurations.
- Sources: All technical information must be derived from and cite official NVIDIA Isaac‚Ñ¢ documentation.
- Timeline: The module is to be completed within a 1-week timeframe.
3. UX/UI (Reader Experience)
3.1. Content Structure
The module will be organized into a clear, three-chapter structure to guide the learner from environment creation to autonomous navigation.
- Chapter 1: Isaac Sim Basics: Scenes & Synthetic Data
- Introduction to the Isaac Sim interface and its core concepts.
- Tutorial: Setting up a basic scene with a robot and obstacles.
- Guide: Generating and exporting synthetic sensor data from the simulation.
- Chapter 2: Isaac ROS: VSLAM & Navigation Workflow
- Overview of the Isaac ROS ecosystem and its advantages.
- Step-by-step guide to launching the Isaac ROS Docker container.
- Tutorial: Running the VSLAM and navigation stack on a robot within Isaac Sim.
- Chapter 3: Nav2: Path Planning for Humanoids
- Introduction to Nav2 and its role in autonomous navigation.
- Example: Sending a navigation goal to a humanoid robot and having it execute a path using Nav2.
4. Technical Considerations
4.1. Key Decisions
- Ecosystem Focus: This module will exclusively use the NVIDIA Isaac‚Ñ¢ toolchain to provide a cohesive learning experience.
- Runnable Demos: Demos will be provided as shell scripts or clear, step-by-step command sequences to ensure they are easy to run.
- Configuration over Code: The focus will be on configuring and running existing Isaac ROS packages rather than writing new code.
4.2. External Dependencies
- Hardware: A computer with a compatible NVIDIA GPU is required.
- Software: Users must have NVIDIA Isaac Sim, Docker, and NVIDIA Container Toolkit installed.
- Prerequisites: A foundational knowledge of ROS 2 concepts is assumed (as taught in Module 1).
--------------------------------------------------------------------------------
URL: https://Muham251.github.io/ai-hackathon-1/docs/module-3/#31-content-structure
Content:
Spec: Module 3: The AI-Robot Brain (NVIDIA Isaac‚Ñ¢)
1. Business Understanding
1.1. Goal
The objective of this module is to educate beginner to intermediate robotics students on leveraging the NVIDIA Isaac‚Ñ¢ ecosystem to build a robot's core intelligence. The curriculum will focus on Isaac Sim for photorealistic simulation, Isaac ROS for perception and navigation (specifically VSLAM), and Nav2 for autonomous path planning.
1.2. Justification
The NVIDIA Isaac‚Ñ¢ platform is at the forefront of AI-driven robotics, offering powerful tools for developing and testing intelligent robots in realistic virtual environments. This module provides learners with critical, industry-relevant skills in simulation, perception, and navigation, forming the foundation of a robot's "brain."
1.3. Target Audience
- Primary: Beginner to intermediate students in robotics and AI who have a foundational understanding of ROS and simulation concepts.
- Secondary: Developers and engineers looking to get started with the NVIDIA Isaac‚Ñ¢ toolchain for robotics projects.
2. Functional Requirements
2.1. In Scope (Features)
- Isaac Sim Fundamentals:
- An introduction to creating scenes in Isaac Sim.
- A guide on generating synthetic data (e.g., camera images, LiDAR data) for training and testing.
- Isaac ROS Workflow:
- A conceptual overview of the Visual SLAM (VSLAM) and navigation pipeline provided by Isaac ROS.
- A tutorial on setting up and running the Isaac ROS navigation stack on a simulated robot.
- Nav2 Path Planning:
- A practical example demonstrating how to use Nav2 for path planning with a humanoid robot in Isaac Sim.
- Runnable Demos:
- The module must include 2‚Äì3 complete, runnable demonstrations that users can execute.
- Format: The final content will be delivered as a Docusaurus-compatible Markdown file.
2.2. Out of Scope
- A complete, end-to-end humanoid control pipeline (e.g., manipulation, complex behaviors).
- The development or deep-dive analysis of custom SLAM or navigation algorithms.
- Advanced Isaac Sim features like custom Python scripting or detailed RTX rendering settings.
2.3. Success Criteria
- Understanding: Learners can explain the significance of photorealistic simulation and synthetic data generation for robotics.
- Explanation: Learners can describe the high-level workflow of the Isaac ROS VSLAM and navigation stack.
- Application: Learners can successfully launch and run a basic Nav2 path-planning demonstration within Isaac Sim.
- Completion: The module is published with 2‚Äì3 fully functional and validated demos.
2.4. Constraints
- Word Count: Approximately 2000‚Äì3000 words.
- Format: Docusaurus Markdown with appropriate code blocks for commands and configurations.
- Sources: All technical information must be derived from and cite official NVIDIA Isaac‚Ñ¢ documentation.
- Timeline: The module is to be completed within a 1-week timeframe.
3. UX/UI (Reader Experience)
3.1. Content Structure
The module will be organized into a clear, three-chapter structure to guide the learner from environment creation to autonomous navigation.
- Chapter 1: Isaac Sim Basics: Scenes & Synthetic Data
- Introduction to the Isaac Sim interface and its core concepts.
- Tutorial: Setting up a basic scene with a robot and obstacles.
- Guide: Generating and exporting synthetic sensor data from the simulation.
- Chapter 2: Isaac ROS: VSLAM & Navigation Workflow
- Overview of the Isaac ROS ecosystem and its advantages.
- Step-by-step guide to launching the Isaac ROS Docker container.
- Tutorial: Running the VSLAM and navigation stack on a robot within Isaac Sim.
- Chapter 3: Nav2: Path Planning for Humanoids
- Introduction to Nav2 and its role in autonomous navigation.
- Example: Sending a navigation goal to a humanoid robot and having it execute a path using Nav2.
4. Technical Considerations
4.1. Key Decisions
- Ecosystem Focus: This module will exclusively use the NVIDIA Isaac‚Ñ¢ toolchain to provide a cohesive learning experience.
- Runnable Demos: Demos will be provided as shell scripts or clear, step-by-step command sequences to ensure they are easy to run.
- Configuration over Code: The focus will be on configuring and running existing Isaac ROS packages rather than writing new code.
4.2. External Dependencies
- Hardware: A computer with a compatible NVIDIA GPU is required.
- Software: Users must have NVIDIA Isaac Sim, Docker, and NVIDIA Container Toolkit installed.
- Prerequisites: A foundational knowledge of ROS 2 concepts is assumed (as taught in Module 1).
--------------------------------------------------------------------------------
URL: https://Muham251.github.io/ai-hackathon-1/docs/module-3/#4-technical-considerations
Content:
Spec: Module 3: The AI-Robot Brain (NVIDIA Isaac‚Ñ¢)
1. Business Understanding
1.1. Goal
The objective of this module is to educate beginner to intermediate robotics students on leveraging the NVIDIA Isaac‚Ñ¢ ecosystem to build a robot's core intelligence. The curriculum will focus on Isaac Sim for photorealistic simulation, Isaac ROS for perception and navigation (specifically VSLAM), and Nav2 for autonomous path planning.
1.2. Justification
The NVIDIA Isaac‚Ñ¢ platform is at the forefront of AI-driven robotics, offering powerful tools for developing and testing intelligent robots in realistic virtual environments. This module provides learners with critical, industry-relevant skills in simulation, perception, and navigation, forming the foundation of a robot's "brain."
1.3. Target Audience
- Primary: Beginner to intermediate students in robotics and AI who have a foundational understanding of ROS and simulation concepts.
- Secondary: Developers and engineers looking to get started with the NVIDIA Isaac‚Ñ¢ toolchain for robotics projects.
2. Functional Requirements
2.1. In Scope (Features)
- Isaac Sim Fundamentals:
- An introduction to creating scenes in Isaac Sim.
- A guide on generating synthetic data (e.g., camera images, LiDAR data) for training and testing.
- Isaac ROS Workflow:
- A conceptual overview of the Visual SLAM (VSLAM) and navigation pipeline provided by Isaac ROS.
- A tutorial on setting up and running the Isaac ROS navigation stack on a simulated robot.
- Nav2 Path Planning:
- A practical example demonstrating how to use Nav2 for path planning with a humanoid robot in Isaac Sim.
- Runnable Demos:
- The module must include 2‚Äì3 complete, runnable demonstrations that users can execute.
- Format: The final content will be delivered as a Docusaurus-compatible Markdown file.
2.2. Out of Scope
- A complete, end-to-end humanoid control pipeline (e.g., manipulation, complex behaviors).
- The development or deep-dive analysis of custom SLAM or navigation algorithms.
- Advanced Isaac Sim features like custom Python scripting or detailed RTX rendering settings.
2.3. Success Criteria
- Understanding: Learners can explain the significance of photorealistic simulation and synthetic data generation for robotics.
- Explanation: Learners can describe the high-level workflow of the Isaac ROS VSLAM and navigation stack.
- Application: Learners can successfully launch and run a basic Nav2 path-planning demonstration within Isaac Sim.
- Completion: The module is published with 2‚Äì3 fully functional and validated demos.
2.4. Constraints
- Word Count: Approximately 2000‚Äì3000 words.
- Format: Docusaurus Markdown with appropriate code blocks for commands and configurations.
- Sources: All technical information must be derived from and cite official NVIDIA Isaac‚Ñ¢ documentation.
- Timeline: The module is to be completed within a 1-week timeframe.
3. UX/UI (Reader Experience)
3.1. Content Structure
The module will be organized into a clear, three-chapter structure to guide the learner from environment creation to autonomous navigation.
- Chapter 1: Isaac Sim Basics: Scenes & Synthetic Data
- Introduction to the Isaac Sim interface and its core concepts.
- Tutorial: Setting up a basic scene with a robot and obstacles.
- Guide: Generating and exporting synthetic sensor data from the simulation.
- Chapter 2: Isaac ROS: VSLAM & Navigation Workflow
- Overview of the Isaac ROS ecosystem and its advantages.
- Step-by-step guide to launching the Isaac ROS Docker container.
- Tutorial: Running the VSLAM and navigation stack on a robot within Isaac Sim.
- Chapter 3: Nav2: Path Planning for Humanoids
- Introduction to Nav2 and its role in autonomous navigation.
- Example: Sending a navigation goal to a humanoid robot and having it execute a path using Nav2.
4. Technical Considerations
4.1. Key Decisions
- Ecosystem Focus: This module will exclusively use the NVIDIA Isaac‚Ñ¢ toolchain to provide a cohesive learning experience.
- Runnable Demos: Demos will be provided as shell scripts or clear, step-by-step command sequences to ensure they are easy to run.
- Configuration over Code: The focus will be on configuring and running existing Isaac ROS packages rather than writing new code.
4.2. External Dependencies
- Hardware: A computer with a compatible NVIDIA GPU is required.
- Software: Users must have NVIDIA Isaac Sim, Docker, and NVIDIA Container Toolkit installed.
- Prerequisites: A foundational knowledge of ROS 2 concepts is assumed (as taught in Module 1).
--------------------------------------------------------------------------------
URL: https://Muham251.github.io/ai-hackathon-1/docs/module-3/#41-key-decisions
Content:
Spec: Module 3: The AI-Robot Brain (NVIDIA Isaac‚Ñ¢)
1. Business Understanding
1.1. Goal
The objective of this module is to educate beginner to intermediate robotics students on leveraging the NVIDIA Isaac‚Ñ¢ ecosystem to build a robot's core intelligence. The curriculum will focus on Isaac Sim for photorealistic simulation, Isaac ROS for perception and navigation (specifically VSLAM), and Nav2 for autonomous path planning.
1.2. Justification
The NVIDIA Isaac‚Ñ¢ platform is at the forefront of AI-driven robotics, offering powerful tools for developing and testing intelligent robots in realistic virtual environments. This module provides learners with critical, industry-relevant skills in simulation, perception, and navigation, forming the foundation of a robot's "brain."
1.3. Target Audience
- Primary: Beginner to intermediate students in robotics and AI who have a foundational understanding of ROS and simulation concepts.
- Secondary: Developers and engineers looking to get started with the NVIDIA Isaac‚Ñ¢ toolchain for robotics projects.
2. Functional Requirements
2.1. In Scope (Features)
- Isaac Sim Fundamentals:
- An introduction to creating scenes in Isaac Sim.
- A guide on generating synthetic data (e.g., camera images, LiDAR data) for training and testing.
- Isaac ROS Workflow:
- A conceptual overview of the Visual SLAM (VSLAM) and navigation pipeline provided by Isaac ROS.
- A tutorial on setting up and running the Isaac ROS navigation stack on a simulated robot.
- Nav2 Path Planning:
- A practical example demonstrating how to use Nav2 for path planning with a humanoid robot in Isaac Sim.
- Runnable Demos:
- The module must include 2‚Äì3 complete, runnable demonstrations that users can execute.
- Format: The final content will be delivered as a Docusaurus-compatible Markdown file.
2.2. Out of Scope
- A complete, end-to-end humanoid control pipeline (e.g., manipulation, complex behaviors).
- The development or deep-dive analysis of custom SLAM or navigation algorithms.
- Advanced Isaac Sim features like custom Python scripting or detailed RTX rendering settings.
2.3. Success Criteria
- Understanding: Learners can explain the significance of photorealistic simulation and synthetic data generation for robotics.
- Explanation: Learners can describe the high-level workflow of the Isaac ROS VSLAM and navigation stack.
- Application: Learners can successfully launch and run a basic Nav2 path-planning demonstration within Isaac Sim.
- Completion: The module is published with 2‚Äì3 fully functional and validated demos.
2.4. Constraints
- Word Count: Approximately 2000‚Äì3000 words.
- Format: Docusaurus Markdown with appropriate code blocks for commands and configurations.
- Sources: All technical information must be derived from and cite official NVIDIA Isaac‚Ñ¢ documentation.
- Timeline: The module is to be completed within a 1-week timeframe.
3. UX/UI (Reader Experience)
3.1. Content Structure
The module will be organized into a clear, three-chapter structure to guide the learner from environment creation to autonomous navigation.
- Chapter 1: Isaac Sim Basics: Scenes & Synthetic Data
- Introduction to the Isaac Sim interface and its core concepts.
- Tutorial: Setting up a basic scene with a robot and obstacles.
- Guide: Generating and exporting synthetic sensor data from the simulation.
- Chapter 2: Isaac ROS: VSLAM & Navigation Workflow
- Overview of the Isaac ROS ecosystem and its advantages.
- Step-by-step guide to launching the Isaac ROS Docker container.
- Tutorial: Running the VSLAM and navigation stack on a robot within Isaac Sim.
- Chapter 3: Nav2: Path Planning for Humanoids
- Introduction to Nav2 and its role in autonomous navigation.
- Example: Sending a navigation goal to a humanoid robot and having it execute a path using Nav2.
4. Technical Considerations
4.1. Key Decisions
- Ecosystem Focus: This module will exclusively use the NVIDIA Isaac‚Ñ¢ toolchain to provide a cohesive learning experience.
- Runnable Demos: Demos will be provided as shell scripts or clear, step-by-step command sequences to ensure they are easy to run.
- Configuration over Code: The focus will be on configuring and running existing Isaac ROS packages rather than writing new code.
4.2. External Dependencies
- Hardware: A computer with a compatible NVIDIA GPU is required.
- Software: Users must have NVIDIA Isaac Sim, Docker, and NVIDIA Container Toolkit installed.
- Prerequisites: A foundational knowledge of ROS 2 concepts is assumed (as taught in Module 1).
--------------------------------------------------------------------------------
URL: https://Muham251.github.io/ai-hackathon-1/docs/module-3/#42-external-dependencies
Content:
Spec: Module 3: The AI-Robot Brain (NVIDIA Isaac‚Ñ¢)
1. Business Understanding
1.1. Goal
The objective of this module is to educate beginner to intermediate robotics students on leveraging the NVIDIA Isaac‚Ñ¢ ecosystem to build a robot's core intelligence. The curriculum will focus on Isaac Sim for photorealistic simulation, Isaac ROS for perception and navigation (specifically VSLAM), and Nav2 for autonomous path planning.
1.2. Justification
The NVIDIA Isaac‚Ñ¢ platform is at the forefront of AI-driven robotics, offering powerful tools for developing and testing intelligent robots in realistic virtual environments. This module provides learners with critical, industry-relevant skills in simulation, perception, and navigation, forming the foundation of a robot's "brain."
1.3. Target Audience
- Primary: Beginner to intermediate students in robotics and AI who have a foundational understanding of ROS and simulation concepts.
- Secondary: Developers and engineers looking to get started with the NVIDIA Isaac‚Ñ¢ toolchain for robotics projects.
2. Functional Requirements
2.1. In Scope (Features)
- Isaac Sim Fundamentals:
- An introduction to creating scenes in Isaac Sim.
- A guide on generating synthetic data (e.g., camera images, LiDAR data) for training and testing.
- Isaac ROS Workflow:
- A conceptual overview of the Visual SLAM (VSLAM) and navigation pipeline provided by Isaac ROS.
- A tutorial on setting up and running the Isaac ROS navigation stack on a simulated robot.
- Nav2 Path Planning:
- A practical example demonstrating how to use Nav2 for path planning with a humanoid robot in Isaac Sim.
- Runnable Demos:
- The module must include 2‚Äì3 complete, runnable demonstrations that users can execute.
- Format: The final content will be delivered as a Docusaurus-compatible Markdown file.
2.2. Out of Scope
- A complete, end-to-end humanoid control pipeline (e.g., manipulation, complex behaviors).
- The development or deep-dive analysis of custom SLAM or navigation algorithms.
- Advanced Isaac Sim features like custom Python scripting or detailed RTX rendering settings.
2.3. Success Criteria
- Understanding: Learners can explain the significance of photorealistic simulation and synthetic data generation for robotics.
- Explanation: Learners can describe the high-level workflow of the Isaac ROS VSLAM and navigation stack.
- Application: Learners can successfully launch and run a basic Nav2 path-planning demonstration within Isaac Sim.
- Completion: The module is published with 2‚Äì3 fully functional and validated demos.
2.4. Constraints
- Word Count: Approximately 2000‚Äì3000 words.
- Format: Docusaurus Markdown with appropriate code blocks for commands and configurations.
- Sources: All technical information must be derived from and cite official NVIDIA Isaac‚Ñ¢ documentation.
- Timeline: The module is to be completed within a 1-week timeframe.
3. UX/UI (Reader Experience)
3.1. Content Structure
The module will be organized into a clear, three-chapter structure to guide the learner from environment creation to autonomous navigation.
- Chapter 1: Isaac Sim Basics: Scenes & Synthetic Data
- Introduction to the Isaac Sim interface and its core concepts.
- Tutorial: Setting up a basic scene with a robot and obstacles.
- Guide: Generating and exporting synthetic sensor data from the simulation.
- Chapter 2: Isaac ROS: VSLAM & Navigation Workflow
- Overview of the Isaac ROS ecosystem and its advantages.
- Step-by-step guide to launching the Isaac ROS Docker container.
- Tutorial: Running the VSLAM and navigation stack on a robot within Isaac Sim.
- Chapter 3: Nav2: Path Planning for Humanoids
- Introduction to Nav2 and its role in autonomous navigation.
- Example: Sending a navigation goal to a humanoid robot and having it execute a path using Nav2.
4. Technical Considerations
4.1. Key Decisions
- Ecosystem Focus: This module will exclusively use the NVIDIA Isaac‚Ñ¢ toolchain to provide a cohesive learning experience.
- Runnable Demos: Demos will be provided as shell scripts or clear, step-by-step command sequences to ensure they are easy to run.
- Configuration over Code: The focus will be on configuring and running existing Isaac ROS packages rather than writing new code.
4.2. External Dependencies
- Hardware: A computer with a compatible NVIDIA GPU is required.
- Software: Users must have NVIDIA Isaac Sim, Docker, and NVIDIA Container Toolkit installed.
- Prerequisites: A foundational knowledge of ROS 2 concepts is assumed (as taught in Module 1).
--------------------------------------------------------------------------------
URL: https://Muham251.github.io/ai-hackathon-1/docs/tutorial-extras/translate-your-site#configure-i18n
Content:
Translate your site
Let's translate docs/intro.md
to French.
Configure i18n
Modify docusaurus.config.js
to add support for the fr
locale:
docusaurus.config.js
export default {
i18n: {
defaultLocale: 'en',
locales: ['en', 'fr'],
},
};
Translate a doc
Copy the docs/intro.md
file to the i18n/fr
folder:
mkdir -p i18n/fr/docusaurus-plugin-content-docs/current/
cp docs/intro.md i18n/fr/docusaurus-plugin-content-docs/current/intro.md
Translate i18n/fr/docusaurus-plugin-content-docs/current/intro.md
in French.
Start your localized site
Start your site on the French locale:
npm run start -- --locale fr
Your localized site is accessible at http://localhost:3000/fr/ and the Getting Started
page is translated.
caution
In development, you can only use one locale at a time.
Add a Locale Dropdown
To navigate seamlessly across languages, add a locale dropdown.
Modify the docusaurus.config.js
file:
docusaurus.config.js
export default {
themeConfig: {
navbar: {
items: [
{
type: 'localeDropdown',
},
],
},
},
};
The locale dropdown now appears in your navbar:
Build your localized site
Build your site for a specific locale:
npm run build -- --locale fr
Or build your site to include all the locales at once:
npm run build
--------------------------------------------------------------------------------
URL: https://Muham251.github.io/ai-hackathon-1/docs/tutorial-extras/translate-your-site#translate-a-doc
Content:
Translate your site
Let's translate docs/intro.md
to French.
Configure i18n
Modify docusaurus.config.js
to add support for the fr
locale:
docusaurus.config.js
export default {
i18n: {
defaultLocale: 'en',
locales: ['en', 'fr'],
},
};
Translate a doc
Copy the docs/intro.md
file to the i18n/fr
folder:
mkdir -p i18n/fr/docusaurus-plugin-content-docs/current/
cp docs/intro.md i18n/fr/docusaurus-plugin-content-docs/current/intro.md
Translate i18n/fr/docusaurus-plugin-content-docs/current/intro.md
in French.
Start your localized site
Start your site on the French locale:
npm run start -- --locale fr
Your localized site is accessible at http://localhost:3000/fr/ and the Getting Started
page is translated.
caution
In development, you can only use one locale at a time.
Add a Locale Dropdown
To navigate seamlessly across languages, add a locale dropdown.
Modify the docusaurus.config.js
file:
docusaurus.config.js
export default {
themeConfig: {
navbar: {
items: [
{
type: 'localeDropdown',
},
],
},
},
};
The locale dropdown now appears in your navbar:
Build your localized site
Build your site for a specific locale:
npm run build -- --locale fr
Or build your site to include all the locales at once:
npm run build
--------------------------------------------------------------------------------
URL: https://Muham251.github.io/ai-hackathon-1/docs/tutorial-extras/translate-your-site#start-your-localized-site
Content:
Translate your site
Let's translate docs/intro.md
to French.
Configure i18n
Modify docusaurus.config.js
to add support for the fr
locale:
docusaurus.config.js
export default {
i18n: {
defaultLocale: 'en',
locales: ['en', 'fr'],
},
};
Translate a doc
Copy the docs/intro.md
file to the i18n/fr
folder:
mkdir -p i18n/fr/docusaurus-plugin-content-docs/current/
cp docs/intro.md i18n/fr/docusaurus-plugin-content-docs/current/intro.md
Translate i18n/fr/docusaurus-plugin-content-docs/current/intro.md
in French.
Start your localized site
Start your site on the French locale:
npm run start -- --locale fr
Your localized site is accessible at http://localhost:3000/fr/ and the Getting Started
page is translated.
caution
In development, you can only use one locale at a time.
Add a Locale Dropdown
To navigate seamlessly across languages, add a locale dropdown.
Modify the docusaurus.config.js
file:
docusaurus.config.js
export default {
themeConfig: {
navbar: {
items: [
{
type: 'localeDropdown',
},
],
},
},
};
The locale dropdown now appears in your navbar:
Build your localized site
Build your site for a specific locale:
npm run build -- --locale fr
Or build your site to include all the locales at once:
npm run build
--------------------------------------------------------------------------------
URL: https://Muham251.github.io/ai-hackathon-1/docs/tutorial-extras/translate-your-site#add-a-locale-dropdown
Content:
Translate your site
Let's translate docs/intro.md
to French.
Configure i18n
Modify docusaurus.config.js
to add support for the fr
locale:
docusaurus.config.js
export default {
i18n: {
defaultLocale: 'en',
locales: ['en', 'fr'],
},
};
Translate a doc
Copy the docs/intro.md
file to the i18n/fr
folder:
mkdir -p i18n/fr/docusaurus-plugin-content-docs/current/
cp docs/intro.md i18n/fr/docusaurus-plugin-content-docs/current/intro.md
Translate i18n/fr/docusaurus-plugin-content-docs/current/intro.md
in French.
Start your localized site
Start your site on the French locale:
npm run start -- --locale fr
Your localized site is accessible at http://localhost:3000/fr/ and the Getting Started
page is translated.
caution
In development, you can only use one locale at a time.
Add a Locale Dropdown
To navigate seamlessly across languages, add a locale dropdown.
Modify the docusaurus.config.js
file:
docusaurus.config.js
export default {
themeConfig: {
navbar: {
items: [
{
type: 'localeDropdown',
},
],
},
},
};
The locale dropdown now appears in your navbar:
Build your localized site
Build your site for a specific locale:
npm run build -- --locale fr
Or build your site to include all the locales at once:
npm run build
--------------------------------------------------------------------------------
URL: https://Muham251.github.io/ai-hackathon-1/docs/tutorial-extras/translate-your-site#build-your-localized-site
Content:
Translate your site
Let's translate docs/intro.md
to French.
Configure i18n
Modify docusaurus.config.js
to add support for the fr
locale:
docusaurus.config.js
export default {
i18n: {
defaultLocale: 'en',
locales: ['en', 'fr'],
},
};
Translate a doc
Copy the docs/intro.md
file to the i18n/fr
folder:
mkdir -p i18n/fr/docusaurus-plugin-content-docs/current/
cp docs/intro.md i18n/fr/docusaurus-plugin-content-docs/current/intro.md
Translate i18n/fr/docusaurus-plugin-content-docs/current/intro.md
in French.
Start your localized site
Start your site on the French locale:
npm run start -- --locale fr
Your localized site is accessible at http://localhost:3000/fr/ and the Getting Started
page is translated.
caution
In development, you can only use one locale at a time.
Add a Locale Dropdown
To navigate seamlessly across languages, add a locale dropdown.
Modify the docusaurus.config.js
file:
docusaurus.config.js
export default {
themeConfig: {
navbar: {
items: [
{
type: 'localeDropdown',
},
],
},
},
};
The locale dropdown now appears in your navbar:
Build your localized site
Build your site for a specific locale:
npm run build -- --locale fr
Or build your site to include all the locales at once:
npm run build
--------------------------------------------------------------------------------
URL: https://Muham251.github.io/ai-hackathon-1/docs/tutorial-extras/manage-docs-versions#create-a-docs-version
Content:
Manage Docs Versions
Docusaurus can manage multiple versions of your docs.
Create a docs version
Release a version 1.0 of your project:
npm run docusaurus docs:version 1.0
The docs
folder is copied into versioned_docs/version-1.0
and versions.json
is created.
Your docs now have 2 versions:
1.0
athttp://localhost:3000/docs/
for the version 1.0 docscurrent
athttp://localhost:3000/docs/next/
for the upcoming, unreleased docs
Add a Version Dropdown
To navigate seamlessly across versions, add a version dropdown.
Modify the docusaurus.config.js
file:
docusaurus.config.js
export default {
themeConfig: {
navbar: {
items: [
{
type: 'docsVersionDropdown',
},
],
},
},
};
The docs version dropdown appears in your navbar:
Update an existing version
It is possible to edit versioned docs in their respective folder:
versioned_docs/version-1.0/hello.md
updateshttp://localhost:3000/docs/hello
docs/hello.md
updateshttp://localhost:3000/docs/next/hello
--------------------------------------------------------------------------------
URL: https://Muham251.github.io/ai-hackathon-1/docs/tutorial-extras/manage-docs-versions#add-a-version-dropdown
Content:
Manage Docs Versions
Docusaurus can manage multiple versions of your docs.
Create a docs version
Release a version 1.0 of your project:
npm run docusaurus docs:version 1.0
The docs
folder is copied into versioned_docs/version-1.0
and versions.json
is created.
Your docs now have 2 versions:
1.0
athttp://localhost:3000/docs/
for the version 1.0 docscurrent
athttp://localhost:3000/docs/next/
for the upcoming, unreleased docs
Add a Version Dropdown
To navigate seamlessly across versions, add a version dropdown.
Modify the docusaurus.config.js
file:
docusaurus.config.js
export default {
themeConfig: {
navbar: {
items: [
{
type: 'docsVersionDropdown',
},
],
},
},
};
The docs version dropdown appears in your navbar:
Update an existing version
It is possible to edit versioned docs in their respective folder:
versioned_docs/version-1.0/hello.md
updateshttp://localhost:3000/docs/hello
docs/hello.md
updateshttp://localhost:3000/docs/next/hello
--------------------------------------------------------------------------------
URL: https://Muham251.github.io/ai-hackathon-1/docs/tutorial-extras/manage-docs-versions#update-an-existing-version
Content:
Manage Docs Versions
Docusaurus can manage multiple versions of your docs.
Create a docs version
Release a version 1.0 of your project:
npm run docusaurus docs:version 1.0
The docs
folder is copied into versioned_docs/version-1.0
and versions.json
is created.
Your docs now have 2 versions:
1.0
athttp://localhost:3000/docs/
for the version 1.0 docscurrent
athttp://localhost:3000/docs/next/
for the upcoming, unreleased docs
Add a Version Dropdown
To navigate seamlessly across versions, add a version dropdown.
Modify the docusaurus.config.js
file:
docusaurus.config.js
export default {
themeConfig: {
navbar: {
items: [
{
type: 'docsVersionDropdown',
},
],
},
},
};
The docs version dropdown appears in your navbar:
Update an existing version
It is possible to edit versioned docs in their respective folder:
versioned_docs/version-1.0/hello.md
updateshttp://localhost:3000/docs/hello
docs/hello.md
updateshttp://localhost:3000/docs/next/hello
--------------------------------------------------------------------------------
URL: https://Muham251.github.io/ai-hackathon-1/docs/module-2/#1-business-understanding
Content:
Spec: Module 2: The Digital Twin (Gazebo & Unity)
1. Business Understanding
1.1. Goal
The goal of this module is to educate students on creating and utilizing digital twins for humanoid robotics. The focus is on mastering physics simulation, environment construction, and sensor simulation within Gazebo and leveraging Unity for high-fidelity rendering.
1.2. Justification
Digital twins are fundamental to modern robotics development, enabling safe, cost-effective, and efficient testing of robot behaviors before deployment. This module provides the essential skills for simulating robots and their interactions with the environment, which is a cornerstone of Physical AI.
1.3. Target Audience
- Primary: Beginner to intermediate students in Physical AI and Humanoid Robotics.
- Secondary: Hobbyists and developers looking to expand their skills into robotics simulation.
2. Functional Requirements
2.1. In Scope (Features)
- Gazebo Physics Simulation:
- Detailed explanation of core physics concepts: gravity, friction, and collision models.
- A tutorial on building a simple environment (e.g., a room with obstacles) using SDF.
- A demonstration of a humanoid robot interacting with the environment.
- Unity for High-Fidelity Rendering:
- An overview of Unity's rendering pipeline and its application in robotics.
- A guide to setting up a simple Unity scene for robotic visualization.
- Sensor Simulation:
- A guide to simulating common robotic sensors: LiDAR, Depth Cameras, and IMUs in both Gazebo and Unity.
- Configuration examples for each sensor.
- Runnable Examples:
- The module must include 2‚Äì3 complete, runnable code examples demonstrating the concepts.
- Format: The final output must be a Docusaurus-compatible Markdown file.
2.2. Out of Scope
- Full-scale game development or advanced Unity features.
- Complex humanoid control algorithms (this is reserved for Module 3).
- Networked or multi-agent simulations.
2.3. Success Criteria
- Knowledge: Students can clearly explain the role of physics engines (Gazebo) and rendering engines (Unity) in creating a digital twin.
- Practical Skill (Gazebo): Students can successfully build a simple Gazebo world and simulate a humanoid robot within it.
- Practical Skill (Unity): Students can set up a Unity scene to render a robot model.
- Practical Skill (Sensors): Students are able to add and configure virtual LiDAR, Depth Camera, and IMU sensors to a simulated robot.
- Completion: The module contains 2-3 fully functional examples that run without errors.
2.4. Constraints
- Word Count: 2000‚Äì3000 words.
- Format: Docusaurus Markdown, including formatted code blocks for SDF, C#, and Python/C++.
- Sources: Content must be based on official documentation from Gazebo, Unity, and relevant sensor simulation libraries.
- Timeline: 1 week for completion.
3. UX/UI (Reader Experience)
3.1. Content Structure
The module will be structured into clear, logical chapters to guide the reader progressively.
- Chapter 1: Gazebo Physics & Environments
- Introduction to Gazebo as a physics simulator.
- Core concepts: gravity, collisions, and inertia.
- Tutorial: Creating a
.world
file with ground plane and basic shapes. - Example: Spawning a humanoid model and observing basic physical interactions.
- Chapter 2: High-Fidelity Rendering with Unity
- The role of Unity in robotics for high-quality visualization.
- Setting up a Unity project for robotics (e.g., using ROS-Unity connectors).
- Example: Importing a URDF model and creating a simple visualization scene.
- Chapter 3: Simulating the Senses
- Introduction to sensor simulation.
- Simulating LiDAR and visualizing point clouds.
- Simulating Depth Cameras and interpreting depth images.
- Simulating an IMU to get orientation and acceleration data.
- Example: A single robot model equipped with all three sensors, with configuration snippets for each.
4. Technical Considerations
4.1. Key Decisions
- Simulation Tools: Gazebo will be used for robust physics simulation, while Unity will be highlighted for its superior rendering capabilities. The text will explain the trade-offs.
- Code Examples: Examples will be provided as self-contained code blocks that can be easily copied and run.
- Structure: The content will follow the chapter structure defined in section 3.1.
4.2. External Dependencies
- The module assumes readers have access to and a basic understanding of a Linux environment (for Gazebo/ROS).
- Readers will need to have Gazebo and Unity Hub (with a recent Unity version) installed.
- Code examples may rely on ROS 2,
rclpy
, orroscpp
for integration, building on concepts from Module 1.
--------------------------------------------------------------------------------
URL: https://Muham251.github.io/ai-hackathon-1/docs/module-2/#11-goal
Content:
Spec: Module 2: The Digital Twin (Gazebo & Unity)
1. Business Understanding
1.1. Goal
The goal of this module is to educate students on creating and utilizing digital twins for humanoid robotics. The focus is on mastering physics simulation, environment construction, and sensor simulation within Gazebo and leveraging Unity for high-fidelity rendering.
1.2. Justification
Digital twins are fundamental to modern robotics development, enabling safe, cost-effective, and efficient testing of robot behaviors before deployment. This module provides the essential skills for simulating robots and their interactions with the environment, which is a cornerstone of Physical AI.
1.3. Target Audience
- Primary: Beginner to intermediate students in Physical AI and Humanoid Robotics.
- Secondary: Hobbyists and developers looking to expand their skills into robotics simulation.
2. Functional Requirements
2.1. In Scope (Features)
- Gazebo Physics Simulation:
- Detailed explanation of core physics concepts: gravity, friction, and collision models.
- A tutorial on building a simple environment (e.g., a room with obstacles) using SDF.
- A demonstration of a humanoid robot interacting with the environment.
- Unity for High-Fidelity Rendering:
- An overview of Unity's rendering pipeline and its application in robotics.
- A guide to setting up a simple Unity scene for robotic visualization.
- Sensor Simulation:
- A guide to simulating common robotic sensors: LiDAR, Depth Cameras, and IMUs in both Gazebo and Unity.
- Configuration examples for each sensor.
- Runnable Examples:
- The module must include 2‚Äì3 complete, runnable code examples demonstrating the concepts.
- Format: The final output must be a Docusaurus-compatible Markdown file.
2.2. Out of Scope
- Full-scale game development or advanced Unity features.
- Complex humanoid control algorithms (this is reserved for Module 3).
- Networked or multi-agent simulations.
2.3. Success Criteria
- Knowledge: Students can clearly explain the role of physics engines (Gazebo) and rendering engines (Unity) in creating a digital twin.
- Practical Skill (Gazebo): Students can successfully build a simple Gazebo world and simulate a humanoid robot within it.
- Practical Skill (Unity): Students can set up a Unity scene to render a robot model.
- Practical Skill (Sensors): Students are able to add and configure virtual LiDAR, Depth Camera, and IMU sensors to a simulated robot.
- Completion: The module contains 2-3 fully functional examples that run without errors.
2.4. Constraints
- Word Count: 2000‚Äì3000 words.
- Format: Docusaurus Markdown, including formatted code blocks for SDF, C#, and Python/C++.
- Sources: Content must be based on official documentation from Gazebo, Unity, and relevant sensor simulation libraries.
- Timeline: 1 week for completion.
3. UX/UI (Reader Experience)
3.1. Content Structure
The module will be structured into clear, logical chapters to guide the reader progressively.
- Chapter 1: Gazebo Physics & Environments
- Introduction to Gazebo as a physics simulator.
- Core concepts: gravity, collisions, and inertia.
- Tutorial: Creating a
.world
file with ground plane and basic shapes. - Example: Spawning a humanoid model and observing basic physical interactions.
- Chapter 2: High-Fidelity Rendering with Unity
- The role of Unity in robotics for high-quality visualization.
- Setting up a Unity project for robotics (e.g., using ROS-Unity connectors).
- Example: Importing a URDF model and creating a simple visualization scene.
- Chapter 3: Simulating the Senses
- Introduction to sensor simulation.
- Simulating LiDAR and visualizing point clouds.
- Simulating Depth Cameras and interpreting depth images.
- Simulating an IMU to get orientation and acceleration data.
- Example: A single robot model equipped with all three sensors, with configuration snippets for each.
4. Technical Considerations
4.1. Key Decisions
- Simulation Tools: Gazebo will be used for robust physics simulation, while Unity will be highlighted for its superior rendering capabilities. The text will explain the trade-offs.
- Code Examples: Examples will be provided as self-contained code blocks that can be easily copied and run.
- Structure: The content will follow the chapter structure defined in section 3.1.
4.2. External Dependencies
- The module assumes readers have access to and a basic understanding of a Linux environment (for Gazebo/ROS).
- Readers will need to have Gazebo and Unity Hub (with a recent Unity version) installed.
- Code examples may rely on ROS 2,
rclpy
, orroscpp
for integration, building on concepts from Module 1.
--------------------------------------------------------------------------------
URL: https://Muham251.github.io/ai-hackathon-1/docs/module-2/#12-justification
Content:
Spec: Module 2: The Digital Twin (Gazebo & Unity)
1. Business Understanding
1.1. Goal
The goal of this module is to educate students on creating and utilizing digital twins for humanoid robotics. The focus is on mastering physics simulation, environment construction, and sensor simulation within Gazebo and leveraging Unity for high-fidelity rendering.
1.2. Justification
Digital twins are fundamental to modern robotics development, enabling safe, cost-effective, and efficient testing of robot behaviors before deployment. This module provides the essential skills for simulating robots and their interactions with the environment, which is a cornerstone of Physical AI.
1.3. Target Audience
- Primary: Beginner to intermediate students in Physical AI and Humanoid Robotics.
- Secondary: Hobbyists and developers looking to expand their skills into robotics simulation.
2. Functional Requirements
2.1. In Scope (Features)
- Gazebo Physics Simulation:
- Detailed explanation of core physics concepts: gravity, friction, and collision models.
- A tutorial on building a simple environment (e.g., a room with obstacles) using SDF.
- A demonstration of a humanoid robot interacting with the environment.
- Unity for High-Fidelity Rendering:
- An overview of Unity's rendering pipeline and its application in robotics.
- A guide to setting up a simple Unity scene for robotic visualization.
- Sensor Simulation:
- A guide to simulating common robotic sensors: LiDAR, Depth Cameras, and IMUs in both Gazebo and Unity.
- Configuration examples for each sensor.
- Runnable Examples:
- The module must include 2‚Äì3 complete, runnable code examples demonstrating the concepts.
- Format: The final output must be a Docusaurus-compatible Markdown file.
2.2. Out of Scope
- Full-scale game development or advanced Unity features.
- Complex humanoid control algorithms (this is reserved for Module 3).
- Networked or multi-agent simulations.
2.3. Success Criteria
- Knowledge: Students can clearly explain the role of physics engines (Gazebo) and rendering engines (Unity) in creating a digital twin.
- Practical Skill (Gazebo): Students can successfully build a simple Gazebo world and simulate a humanoid robot within it.
- Practical Skill (Unity): Students can set up a Unity scene to render a robot model.
- Practical Skill (Sensors): Students are able to add and configure virtual LiDAR, Depth Camera, and IMU sensors to a simulated robot.
- Completion: The module contains 2-3 fully functional examples that run without errors.
2.4. Constraints
- Word Count: 2000‚Äì3000 words.
- Format: Docusaurus Markdown, including formatted code blocks for SDF, C#, and Python/C++.
- Sources: Content must be based on official documentation from Gazebo, Unity, and relevant sensor simulation libraries.
- Timeline: 1 week for completion.
3. UX/UI (Reader Experience)
3.1. Content Structure
The module will be structured into clear, logical chapters to guide the reader progressively.
- Chapter 1: Gazebo Physics & Environments
- Introduction to Gazebo as a physics simulator.
- Core concepts: gravity, collisions, and inertia.
- Tutorial: Creating a
.world
file with ground plane and basic shapes. - Example: Spawning a humanoid model and observing basic physical interactions.
- Chapter 2: High-Fidelity Rendering with Unity
- The role of Unity in robotics for high-quality visualization.
- Setting up a Unity project for robotics (e.g., using ROS-Unity connectors).
- Example: Importing a URDF model and creating a simple visualization scene.
- Chapter 3: Simulating the Senses
- Introduction to sensor simulation.
- Simulating LiDAR and visualizing point clouds.
- Simulating Depth Cameras and interpreting depth images.
- Simulating an IMU to get orientation and acceleration data.
- Example: A single robot model equipped with all three sensors, with configuration snippets for each.
4. Technical Considerations
4.1. Key Decisions
- Simulation Tools: Gazebo will be used for robust physics simulation, while Unity will be highlighted for its superior rendering capabilities. The text will explain the trade-offs.
- Code Examples: Examples will be provided as self-contained code blocks that can be easily copied and run.
- Structure: The content will follow the chapter structure defined in section 3.1.
4.2. External Dependencies
- The module assumes readers have access to and a basic understanding of a Linux environment (for Gazebo/ROS).
- Readers will need to have Gazebo and Unity Hub (with a recent Unity version) installed.
- Code examples may rely on ROS 2,
rclpy
, orroscpp
for integration, building on concepts from Module 1.
--------------------------------------------------------------------------------
URL: https://Muham251.github.io/ai-hackathon-1/docs/module-2/#13-target-audience
Content:
Spec: Module 2: The Digital Twin (Gazebo & Unity)
1. Business Understanding
1.1. Goal
The goal of this module is to educate students on creating and utilizing digital twins for humanoid robotics. The focus is on mastering physics simulation, environment construction, and sensor simulation within Gazebo and leveraging Unity for high-fidelity rendering.
1.2. Justification
Digital twins are fundamental to modern robotics development, enabling safe, cost-effective, and efficient testing of robot behaviors before deployment. This module provides the essential skills for simulating robots and their interactions with the environment, which is a cornerstone of Physical AI.
1.3. Target Audience
- Primary: Beginner to intermediate students in Physical AI and Humanoid Robotics.
- Secondary: Hobbyists and developers looking to expand their skills into robotics simulation.
2. Functional Requirements
2.1. In Scope (Features)
- Gazebo Physics Simulation:
- Detailed explanation of core physics concepts: gravity, friction, and collision models.
- A tutorial on building a simple environment (e.g., a room with obstacles) using SDF.
- A demonstration of a humanoid robot interacting with the environment.
- Unity for High-Fidelity Rendering:
- An overview of Unity's rendering pipeline and its application in robotics.
- A guide to setting up a simple Unity scene for robotic visualization.
- Sensor Simulation:
- A guide to simulating common robotic sensors: LiDAR, Depth Cameras, and IMUs in both Gazebo and Unity.
- Configuration examples for each sensor.
- Runnable Examples:
- The module must include 2‚Äì3 complete, runnable code examples demonstrating the concepts.
- Format: The final output must be a Docusaurus-compatible Markdown file.
2.2. Out of Scope
- Full-scale game development or advanced Unity features.
- Complex humanoid control algorithms (this is reserved for Module 3).
- Networked or multi-agent simulations.
2.3. Success Criteria
- Knowledge: Students can clearly explain the role of physics engines (Gazebo) and rendering engines (Unity) in creating a digital twin.
- Practical Skill (Gazebo): Students can successfully build a simple Gazebo world and simulate a humanoid robot within it.
- Practical Skill (Unity): Students can set up a Unity scene to render a robot model.
- Practical Skill (Sensors): Students are able to add and configure virtual LiDAR, Depth Camera, and IMU sensors to a simulated robot.
- Completion: The module contains 2-3 fully functional examples that run without errors.
2.4. Constraints
- Word Count: 2000‚Äì3000 words.
- Format: Docusaurus Markdown, including formatted code blocks for SDF, C#, and Python/C++.
- Sources: Content must be based on official documentation from Gazebo, Unity, and relevant sensor simulation libraries.
- Timeline: 1 week for completion.
3. UX/UI (Reader Experience)
3.1. Content Structure
The module will be structured into clear, logical chapters to guide the reader progressively.
- Chapter 1: Gazebo Physics & Environments
- Introduction to Gazebo as a physics simulator.
- Core concepts: gravity, collisions, and inertia.
- Tutorial: Creating a
.world
file with ground plane and basic shapes. - Example: Spawning a humanoid model and observing basic physical interactions.
- Chapter 2: High-Fidelity Rendering with Unity
- The role of Unity in robotics for high-quality visualization.
- Setting up a Unity project for robotics (e.g., using ROS-Unity connectors).
- Example: Importing a URDF model and creating a simple visualization scene.
- Chapter 3: Simulating the Senses
- Introduction to sensor simulation.
- Simulating LiDAR and visualizing point clouds.
- Simulating Depth Cameras and interpreting depth images.
- Simulating an IMU to get orientation and acceleration data.
- Example: A single robot model equipped with all three sensors, with configuration snippets for each.
4. Technical Considerations
4.1. Key Decisions
- Simulation Tools: Gazebo will be used for robust physics simulation, while Unity will be highlighted for its superior rendering capabilities. The text will explain the trade-offs.
- Code Examples: Examples will be provided as self-contained code blocks that can be easily copied and run.
- Structure: The content will follow the chapter structure defined in section 3.1.
4.2. External Dependencies
- The module assumes readers have access to and a basic understanding of a Linux environment (for Gazebo/ROS).
- Readers will need to have Gazebo and Unity Hub (with a recent Unity version) installed.
- Code examples may rely on ROS 2,
rclpy
, orroscpp
for integration, building on concepts from Module 1.
--------------------------------------------------------------------------------
URL: https://Muham251.github.io/ai-hackathon-1/docs/module-2/#2-functional-requirements
Content:
Spec: Module 2: The Digital Twin (Gazebo & Unity)
1. Business Understanding
1.1. Goal
The goal of this module is to educate students on creating and utilizing digital twins for humanoid robotics. The focus is on mastering physics simulation, environment construction, and sensor simulation within Gazebo and leveraging Unity for high-fidelity rendering.
1.2. Justification
Digital twins are fundamental to modern robotics development, enabling safe, cost-effective, and efficient testing of robot behaviors before deployment. This module provides the essential skills for simulating robots and their interactions with the environment, which is a cornerstone of Physical AI.
1.3. Target Audience
- Primary: Beginner to intermediate students in Physical AI and Humanoid Robotics.
- Secondary: Hobbyists and developers looking to expand their skills into robotics simulation.
2. Functional Requirements
2.1. In Scope (Features)
- Gazebo Physics Simulation:
- Detailed explanation of core physics concepts: gravity, friction, and collision models.
- A tutorial on building a simple environment (e.g., a room with obstacles) using SDF.
- A demonstration of a humanoid robot interacting with the environment.
- Unity for High-Fidelity Rendering:
- An overview of Unity's rendering pipeline and its application in robotics.
- A guide to setting up a simple Unity scene for robotic visualization.
- Sensor Simulation:
- A guide to simulating common robotic sensors: LiDAR, Depth Cameras, and IMUs in both Gazebo and Unity.
- Configuration examples for each sensor.
- Runnable Examples:
- The module must include 2‚Äì3 complete, runnable code examples demonstrating the concepts.
- Format: The final output must be a Docusaurus-compatible Markdown file.
2.2. Out of Scope
- Full-scale game development or advanced Unity features.
- Complex humanoid control algorithms (this is reserved for Module 3).
- Networked or multi-agent simulations.
2.3. Success Criteria
- Knowledge: Students can clearly explain the role of physics engines (Gazebo) and rendering engines (Unity) in creating a digital twin.
- Practical Skill (Gazebo): Students can successfully build a simple Gazebo world and simulate a humanoid robot within it.
- Practical Skill (Unity): Students can set up a Unity scene to render a robot model.
- Practical Skill (Sensors): Students are able to add and configure virtual LiDAR, Depth Camera, and IMU sensors to a simulated robot.
- Completion: The module contains 2-3 fully functional examples that run without errors.
2.4. Constraints
- Word Count: 2000‚Äì3000 words.
- Format: Docusaurus Markdown, including formatted code blocks for SDF, C#, and Python/C++.
- Sources: Content must be based on official documentation from Gazebo, Unity, and relevant sensor simulation libraries.
- Timeline: 1 week for completion.
3. UX/UI (Reader Experience)
3.1. Content Structure
The module will be structured into clear, logical chapters to guide the reader progressively.
- Chapter 1: Gazebo Physics & Environments
- Introduction to Gazebo as a physics simulator.
- Core concepts: gravity, collisions, and inertia.
- Tutorial: Creating a
.world
file with ground plane and basic shapes. - Example: Spawning a humanoid model and observing basic physical interactions.
- Chapter 2: High-Fidelity Rendering with Unity
- The role of Unity in robotics for high-quality visualization.
- Setting up a Unity project for robotics (e.g., using ROS-Unity connectors).
- Example: Importing a URDF model and creating a simple visualization scene.
- Chapter 3: Simulating the Senses
- Introduction to sensor simulation.
- Simulating LiDAR and visualizing point clouds.
- Simulating Depth Cameras and interpreting depth images.
- Simulating an IMU to get orientation and acceleration data.
- Example: A single robot model equipped with all three sensors, with configuration snippets for each.
4. Technical Considerations
4.1. Key Decisions
- Simulation Tools: Gazebo will be used for robust physics simulation, while Unity will be highlighted for its superior rendering capabilities. The text will explain the trade-offs.
- Code Examples: Examples will be provided as self-contained code blocks that can be easily copied and run.
- Structure: The content will follow the chapter structure defined in section 3.1.
4.2. External Dependencies
- The module assumes readers have access to and a basic understanding of a Linux environment (for Gazebo/ROS).
- Readers will need to have Gazebo and Unity Hub (with a recent Unity version) installed.
- Code examples may rely on ROS 2,
rclpy
, orroscpp
for integration, building on concepts from Module 1.
--------------------------------------------------------------------------------
URL: https://Muham251.github.io/ai-hackathon-1/docs/module-2/#21-in-scope-features
Content:
Spec: Module 2: The Digital Twin (Gazebo & Unity)
1. Business Understanding
1.1. Goal
The goal of this module is to educate students on creating and utilizing digital twins for humanoid robotics. The focus is on mastering physics simulation, environment construction, and sensor simulation within Gazebo and leveraging Unity for high-fidelity rendering.
1.2. Justification
Digital twins are fundamental to modern robotics development, enabling safe, cost-effective, and efficient testing of robot behaviors before deployment. This module provides the essential skills for simulating robots and their interactions with the environment, which is a cornerstone of Physical AI.
1.3. Target Audience
- Primary: Beginner to intermediate students in Physical AI and Humanoid Robotics.
- Secondary: Hobbyists and developers looking to expand their skills into robotics simulation.
2. Functional Requirements
2.1. In Scope (Features)
- Gazebo Physics Simulation:
- Detailed explanation of core physics concepts: gravity, friction, and collision models.
- A tutorial on building a simple environment (e.g., a room with obstacles) using SDF.
- A demonstration of a humanoid robot interacting with the environment.
- Unity for High-Fidelity Rendering:
- An overview of Unity's rendering pipeline and its application in robotics.
- A guide to setting up a simple Unity scene for robotic visualization.
- Sensor Simulation:
- A guide to simulating common robotic sensors: LiDAR, Depth Cameras, and IMUs in both Gazebo and Unity.
- Configuration examples for each sensor.
- Runnable Examples:
- The module must include 2‚Äì3 complete, runnable code examples demonstrating the concepts.
- Format: The final output must be a Docusaurus-compatible Markdown file.
2.2. Out of Scope
- Full-scale game development or advanced Unity features.
- Complex humanoid control algorithms (this is reserved for Module 3).
- Networked or multi-agent simulations.
2.3. Success Criteria
- Knowledge: Students can clearly explain the role of physics engines (Gazebo) and rendering engines (Unity) in creating a digital twin.
- Practical Skill (Gazebo): Students can successfully build a simple Gazebo world and simulate a humanoid robot within it.
- Practical Skill (Unity): Students can set up a Unity scene to render a robot model.
- Practical Skill (Sensors): Students are able to add and configure virtual LiDAR, Depth Camera, and IMU sensors to a simulated robot.
- Completion: The module contains 2-3 fully functional examples that run without errors.
2.4. Constraints
- Word Count: 2000‚Äì3000 words.
- Format: Docusaurus Markdown, including formatted code blocks for SDF, C#, and Python/C++.
- Sources: Content must be based on official documentation from Gazebo, Unity, and relevant sensor simulation libraries.
- Timeline: 1 week for completion.
3. UX/UI (Reader Experience)
3.1. Content Structure
The module will be structured into clear, logical chapters to guide the reader progressively.
- Chapter 1: Gazebo Physics & Environments
- Introduction to Gazebo as a physics simulator.
- Core concepts: gravity, collisions, and inertia.
- Tutorial: Creating a
.world
file with ground plane and basic shapes. - Example: Spawning a humanoid model and observing basic physical interactions.
- Chapter 2: High-Fidelity Rendering with Unity
- The role of Unity in robotics for high-quality visualization.
- Setting up a Unity project for robotics (e.g., using ROS-Unity connectors).
- Example: Importing a URDF model and creating a simple visualization scene.
- Chapter 3: Simulating the Senses
- Introduction to sensor simulation.
- Simulating LiDAR and visualizing point clouds.
- Simulating Depth Cameras and interpreting depth images.
- Simulating an IMU to get orientation and acceleration data.
- Example: A single robot model equipped with all three sensors, with configuration snippets for each.
4. Technical Considerations
4.1. Key Decisions
- Simulation Tools: Gazebo will be used for robust physics simulation, while Unity will be highlighted for its superior rendering capabilities. The text will explain the trade-offs.
- Code Examples: Examples will be provided as self-contained code blocks that can be easily copied and run.
- Structure: The content will follow the chapter structure defined in section 3.1.
4.2. External Dependencies
- The module assumes readers have access to and a basic understanding of a Linux environment (for Gazebo/ROS).
- Readers will need to have Gazebo and Unity Hub (with a recent Unity version) installed.
- Code examples may rely on ROS 2,
rclpy
, orroscpp
for integration, building on concepts from Module 1.
--------------------------------------------------------------------------------
URL: https://Muham251.github.io/ai-hackathon-1/docs/module-2/#22-out-of-scope
Content:
Spec: Module 2: The Digital Twin (Gazebo & Unity)
1. Business Understanding
1.1. Goal
The goal of this module is to educate students on creating and utilizing digital twins for humanoid robotics. The focus is on mastering physics simulation, environment construction, and sensor simulation within Gazebo and leveraging Unity for high-fidelity rendering.
1.2. Justification
Digital twins are fundamental to modern robotics development, enabling safe, cost-effective, and efficient testing of robot behaviors before deployment. This module provides the essential skills for simulating robots and their interactions with the environment, which is a cornerstone of Physical AI.
1.3. Target Audience
- Primary: Beginner to intermediate students in Physical AI and Humanoid Robotics.
- Secondary: Hobbyists and developers looking to expand their skills into robotics simulation.
2. Functional Requirements
2.1. In Scope (Features)
- Gazebo Physics Simulation:
- Detailed explanation of core physics concepts: gravity, friction, and collision models.
- A tutorial on building a simple environment (e.g., a room with obstacles) using SDF.
- A demonstration of a humanoid robot interacting with the environment.
- Unity for High-Fidelity Rendering:
- An overview of Unity's rendering pipeline and its application in robotics.
- A guide to setting up a simple Unity scene for robotic visualization.
- Sensor Simulation:
- A guide to simulating common robotic sensors: LiDAR, Depth Cameras, and IMUs in both Gazebo and Unity.
- Configuration examples for each sensor.
- Runnable Examples:
- The module must include 2‚Äì3 complete, runnable code examples demonstrating the concepts.
- Format: The final output must be a Docusaurus-compatible Markdown file.
2.2. Out of Scope
- Full-scale game development or advanced Unity features.
- Complex humanoid control algorithms (this is reserved for Module 3).
- Networked or multi-agent simulations.
2.3. Success Criteria
- Knowledge: Students can clearly explain the role of physics engines (Gazebo) and rendering engines (Unity) in creating a digital twin.
- Practical Skill (Gazebo): Students can successfully build a simple Gazebo world and simulate a humanoid robot within it.
- Practical Skill (Unity): Students can set up a Unity scene to render a robot model.
- Practical Skill (Sensors): Students are able to add and configure virtual LiDAR, Depth Camera, and IMU sensors to a simulated robot.
- Completion: The module contains 2-3 fully functional examples that run without errors.
2.4. Constraints
- Word Count: 2000‚Äì3000 words.
- Format: Docusaurus Markdown, including formatted code blocks for SDF, C#, and Python/C++.
- Sources: Content must be based on official documentation from Gazebo, Unity, and relevant sensor simulation libraries.
- Timeline: 1 week for completion.
3. UX/UI (Reader Experience)
3.1. Content Structure
The module will be structured into clear, logical chapters to guide the reader progressively.
- Chapter 1: Gazebo Physics & Environments
- Introduction to Gazebo as a physics simulator.
- Core concepts: gravity, collisions, and inertia.
- Tutorial: Creating a
.world
file with ground plane and basic shapes. - Example: Spawning a humanoid model and observing basic physical interactions.
- Chapter 2: High-Fidelity Rendering with Unity
- The role of Unity in robotics for high-quality visualization.
- Setting up a Unity project for robotics (e.g., using ROS-Unity connectors).
- Example: Importing a URDF model and creating a simple visualization scene.
- Chapter 3: Simulating the Senses
- Introduction to sensor simulation.
- Simulating LiDAR and visualizing point clouds.
- Simulating Depth Cameras and interpreting depth images.
- Simulating an IMU to get orientation and acceleration data.
- Example: A single robot model equipped with all three sensors, with configuration snippets for each.
4. Technical Considerations
4.1. Key Decisions
- Simulation Tools: Gazebo will be used for robust physics simulation, while Unity will be highlighted for its superior rendering capabilities. The text will explain the trade-offs.
- Code Examples: Examples will be provided as self-contained code blocks that can be easily copied and run.
- Structure: The content will follow the chapter structure defined in section 3.1.
4.2. External Dependencies
- The module assumes readers have access to and a basic understanding of a Linux environment (for Gazebo/ROS).
- Readers will need to have Gazebo and Unity Hub (with a recent Unity version) installed.
- Code examples may rely on ROS 2,
rclpy
, orroscpp
for integration, building on concepts from Module 1.
--------------------------------------------------------------------------------
URL: https://Muham251.github.io/ai-hackathon-1/docs/module-2/#23-success-criteria
Content:
Spec: Module 2: The Digital Twin (Gazebo & Unity)
1. Business Understanding
1.1. Goal
The goal of this module is to educate students on creating and utilizing digital twins for humanoid robotics. The focus is on mastering physics simulation, environment construction, and sensor simulation within Gazebo and leveraging Unity for high-fidelity rendering.
1.2. Justification
Digital twins are fundamental to modern robotics development, enabling safe, cost-effective, and efficient testing of robot behaviors before deployment. This module provides the essential skills for simulating robots and their interactions with the environment, which is a cornerstone of Physical AI.
1.3. Target Audience
- Primary: Beginner to intermediate students in Physical AI and Humanoid Robotics.
- Secondary: Hobbyists and developers looking to expand their skills into robotics simulation.
2. Functional Requirements
2.1. In Scope (Features)
- Gazebo Physics Simulation:
- Detailed explanation of core physics concepts: gravity, friction, and collision models.
- A tutorial on building a simple environment (e.g., a room with obstacles) using SDF.
- A demonstration of a humanoid robot interacting with the environment.
- Unity for High-Fidelity Rendering:
- An overview of Unity's rendering pipeline and its application in robotics.
- A guide to setting up a simple Unity scene for robotic visualization.
- Sensor Simulation:
- A guide to simulating common robotic sensors: LiDAR, Depth Cameras, and IMUs in both Gazebo and Unity.
- Configuration examples for each sensor.
- Runnable Examples:
- The module must include 2‚Äì3 complete, runnable code examples demonstrating the concepts.
- Format: The final output must be a Docusaurus-compatible Markdown file.
2.2. Out of Scope
- Full-scale game development or advanced Unity features.
- Complex humanoid control algorithms (this is reserved for Module 3).
- Networked or multi-agent simulations.
2.3. Success Criteria
- Knowledge: Students can clearly explain the role of physics engines (Gazebo) and rendering engines (Unity) in creating a digital twin.
- Practical Skill (Gazebo): Students can successfully build a simple Gazebo world and simulate a humanoid robot within it.
- Practical Skill (Unity): Students can set up a Unity scene to render a robot model.
- Practical Skill (Sensors): Students are able to add and configure virtual LiDAR, Depth Camera, and IMU sensors to a simulated robot.
- Completion: The module contains 2-3 fully functional examples that run without errors.
2.4. Constraints
- Word Count: 2000‚Äì3000 words.
- Format: Docusaurus Markdown, including formatted code blocks for SDF, C#, and Python/C++.
- Sources: Content must be based on official documentation from Gazebo, Unity, and relevant sensor simulation libraries.
- Timeline: 1 week for completion.
3. UX/UI (Reader Experience)
3.1. Content Structure
The module will be structured into clear, logical chapters to guide the reader progressively.
- Chapter 1: Gazebo Physics & Environments
- Introduction to Gazebo as a physics simulator.
- Core concepts: gravity, collisions, and inertia.
- Tutorial: Creating a
.world
file with ground plane and basic shapes. - Example: Spawning a humanoid model and observing basic physical interactions.
- Chapter 2: High-Fidelity Rendering with Unity
- The role of Unity in robotics for high-quality visualization.
- Setting up a Unity project for robotics (e.g., using ROS-Unity connectors).
- Example: Importing a URDF model and creating a simple visualization scene.
- Chapter 3: Simulating the Senses
- Introduction to sensor simulation.
- Simulating LiDAR and visualizing point clouds.
- Simulating Depth Cameras and interpreting depth images.
- Simulating an IMU to get orientation and acceleration data.
- Example: A single robot model equipped with all three sensors, with configuration snippets for each.
4. Technical Considerations
4.1. Key Decisions
- Simulation Tools: Gazebo will be used for robust physics simulation, while Unity will be highlighted for its superior rendering capabilities. The text will explain the trade-offs.
- Code Examples: Examples will be provided as self-contained code blocks that can be easily copied and run.
- Structure: The content will follow the chapter structure defined in section 3.1.
4.2. External Dependencies
- The module assumes readers have access to and a basic understanding of a Linux environment (for Gazebo/ROS).
- Readers will need to have Gazebo and Unity Hub (with a recent Unity version) installed.
- Code examples may rely on ROS 2,
rclpy
, orroscpp
for integration, building on concepts from Module 1.
--------------------------------------------------------------------------------
URL: https://Muham251.github.io/ai-hackathon-1/docs/module-2/#24-constraints
Content:
Spec: Module 2: The Digital Twin (Gazebo & Unity)
1. Business Understanding
1.1. Goal
The goal of this module is to educate students on creating and utilizing digital twins for humanoid robotics. The focus is on mastering physics simulation, environment construction, and sensor simulation within Gazebo and leveraging Unity for high-fidelity rendering.
1.2. Justification
Digital twins are fundamental to modern robotics development, enabling safe, cost-effective, and efficient testing of robot behaviors before deployment. This module provides the essential skills for simulating robots and their interactions with the environment, which is a cornerstone of Physical AI.
1.3. Target Audience
- Primary: Beginner to intermediate students in Physical AI and Humanoid Robotics.
- Secondary: Hobbyists and developers looking to expand their skills into robotics simulation.
2. Functional Requirements
2.1. In Scope (Features)
- Gazebo Physics Simulation:
- Detailed explanation of core physics concepts: gravity, friction, and collision models.
- A tutorial on building a simple environment (e.g., a room with obstacles) using SDF.
- A demonstration of a humanoid robot interacting with the environment.
- Unity for High-Fidelity Rendering:
- An overview of Unity's rendering pipeline and its application in robotics.
- A guide to setting up a simple Unity scene for robotic visualization.
- Sensor Simulation:
- A guide to simulating common robotic sensors: LiDAR, Depth Cameras, and IMUs in both Gazebo and Unity.
- Configuration examples for each sensor.
- Runnable Examples:
- The module must include 2‚Äì3 complete, runnable code examples demonstrating the concepts.
- Format: The final output must be a Docusaurus-compatible Markdown file.
2.2. Out of Scope
- Full-scale game development or advanced Unity features.
- Complex humanoid control algorithms (this is reserved for Module 3).
- Networked or multi-agent simulations.
2.3. Success Criteria
- Knowledge: Students can clearly explain the role of physics engines (Gazebo) and rendering engines (Unity) in creating a digital twin.
- Practical Skill (Gazebo): Students can successfully build a simple Gazebo world and simulate a humanoid robot within it.
- Practical Skill (Unity): Students can set up a Unity scene to render a robot model.
- Practical Skill (Sensors): Students are able to add and configure virtual LiDAR, Depth Camera, and IMU sensors to a simulated robot.
- Completion: The module contains 2-3 fully functional examples that run without errors.
2.4. Constraints
- Word Count: 2000‚Äì3000 words.
- Format: Docusaurus Markdown, including formatted code blocks for SDF, C#, and Python/C++.
- Sources: Content must be based on official documentation from Gazebo, Unity, and relevant sensor simulation libraries.
- Timeline: 1 week for completion.
3. UX/UI (Reader Experience)
3.1. Content Structure
The module will be structured into clear, logical chapters to guide the reader progressively.
- Chapter 1: Gazebo Physics & Environments
- Introduction to Gazebo as a physics simulator.
- Core concepts: gravity, collisions, and inertia.
- Tutorial: Creating a
.world
file with ground plane and basic shapes. - Example: Spawning a humanoid model and observing basic physical interactions.
- Chapter 2: High-Fidelity Rendering with Unity
- The role of Unity in robotics for high-quality visualization.
- Setting up a Unity project for robotics (e.g., using ROS-Unity connectors).
- Example: Importing a URDF model and creating a simple visualization scene.
- Chapter 3: Simulating the Senses
- Introduction to sensor simulation.
- Simulating LiDAR and visualizing point clouds.
- Simulating Depth Cameras and interpreting depth images.
- Simulating an IMU to get orientation and acceleration data.
- Example: A single robot model equipped with all three sensors, with configuration snippets for each.
4. Technical Considerations
4.1. Key Decisions
- Simulation Tools: Gazebo will be used for robust physics simulation, while Unity will be highlighted for its superior rendering capabilities. The text will explain the trade-offs.
- Code Examples: Examples will be provided as self-contained code blocks that can be easily copied and run.
- Structure: The content will follow the chapter structure defined in section 3.1.
4.2. External Dependencies
- The module assumes readers have access to and a basic understanding of a Linux environment (for Gazebo/ROS).
- Readers will need to have Gazebo and Unity Hub (with a recent Unity version) installed.
- Code examples may rely on ROS 2,
rclpy
, orroscpp
for integration, building on concepts from Module 1.
--------------------------------------------------------------------------------
URL: https://Muham251.github.io/ai-hackathon-1/docs/module-2/#3-uxui-reader-experience
Content:
Spec: Module 2: The Digital Twin (Gazebo & Unity)
1. Business Understanding
1.1. Goal
The goal of this module is to educate students on creating and utilizing digital twins for humanoid robotics. The focus is on mastering physics simulation, environment construction, and sensor simulation within Gazebo and leveraging Unity for high-fidelity rendering.
1.2. Justification
Digital twins are fundamental to modern robotics development, enabling safe, cost-effective, and efficient testing of robot behaviors before deployment. This module provides the essential skills for simulating robots and their interactions with the environment, which is a cornerstone of Physical AI.
1.3. Target Audience
- Primary: Beginner to intermediate students in Physical AI and Humanoid Robotics.
- Secondary: Hobbyists and developers looking to expand their skills into robotics simulation.
2. Functional Requirements
2.1. In Scope (Features)
- Gazebo Physics Simulation:
- Detailed explanation of core physics concepts: gravity, friction, and collision models.
- A tutorial on building a simple environment (e.g., a room with obstacles) using SDF.
- A demonstration of a humanoid robot interacting with the environment.
- Unity for High-Fidelity Rendering:
- An overview of Unity's rendering pipeline and its application in robotics.
- A guide to setting up a simple Unity scene for robotic visualization.
- Sensor Simulation:
- A guide to simulating common robotic sensors: LiDAR, Depth Cameras, and IMUs in both Gazebo and Unity.
- Configuration examples for each sensor.
- Runnable Examples:
- The module must include 2‚Äì3 complete, runnable code examples demonstrating the concepts.
- Format: The final output must be a Docusaurus-compatible Markdown file.
2.2. Out of Scope
- Full-scale game development or advanced Unity features.
- Complex humanoid control algorithms (this is reserved for Module 3).
- Networked or multi-agent simulations.
2.3. Success Criteria
- Knowledge: Students can clearly explain the role of physics engines (Gazebo) and rendering engines (Unity) in creating a digital twin.
- Practical Skill (Gazebo): Students can successfully build a simple Gazebo world and simulate a humanoid robot within it.
- Practical Skill (Unity): Students can set up a Unity scene to render a robot model.
- Practical Skill (Sensors): Students are able to add and configure virtual LiDAR, Depth Camera, and IMU sensors to a simulated robot.
- Completion: The module contains 2-3 fully functional examples that run without errors.
2.4. Constraints
- Word Count: 2000‚Äì3000 words.
- Format: Docusaurus Markdown, including formatted code blocks for SDF, C#, and Python/C++.
- Sources: Content must be based on official documentation from Gazebo, Unity, and relevant sensor simulation libraries.
- Timeline: 1 week for completion.
3. UX/UI (Reader Experience)
3.1. Content Structure
The module will be structured into clear, logical chapters to guide the reader progressively.
- Chapter 1: Gazebo Physics & Environments
- Introduction to Gazebo as a physics simulator.
- Core concepts: gravity, collisions, and inertia.
- Tutorial: Creating a
.world
file with ground plane and basic shapes. - Example: Spawning a humanoid model and observing basic physical interactions.
- Chapter 2: High-Fidelity Rendering with Unity
- The role of Unity in robotics for high-quality visualization.
- Setting up a Unity project for robotics (e.g., using ROS-Unity connectors).
- Example: Importing a URDF model and creating a simple visualization scene.
- Chapter 3: Simulating the Senses
- Introduction to sensor simulation.
- Simulating LiDAR and visualizing point clouds.
- Simulating Depth Cameras and interpreting depth images.
- Simulating an IMU to get orientation and acceleration data.
- Example: A single robot model equipped with all three sensors, with configuration snippets for each.
4. Technical Considerations
4.1. Key Decisions
- Simulation Tools: Gazebo will be used for robust physics simulation, while Unity will be highlighted for its superior rendering capabilities. The text will explain the trade-offs.
- Code Examples: Examples will be provided as self-contained code blocks that can be easily copied and run.
- Structure: The content will follow the chapter structure defined in section 3.1.
4.2. External Dependencies
- The module assumes readers have access to and a basic understanding of a Linux environment (for Gazebo/ROS).
- Readers will need to have Gazebo and Unity Hub (with a recent Unity version) installed.
- Code examples may rely on ROS 2,
rclpy
, orroscpp
for integration, building on concepts from Module 1.
--------------------------------------------------------------------------------
URL: https://Muham251.github.io/ai-hackathon-1/docs/module-2/#31-content-structure
Content:
Spec: Module 2: The Digital Twin (Gazebo & Unity)
1. Business Understanding
1.1. Goal
The goal of this module is to educate students on creating and utilizing digital twins for humanoid robotics. The focus is on mastering physics simulation, environment construction, and sensor simulation within Gazebo and leveraging Unity for high-fidelity rendering.
1.2. Justification
Digital twins are fundamental to modern robotics development, enabling safe, cost-effective, and efficient testing of robot behaviors before deployment. This module provides the essential skills for simulating robots and their interactions with the environment, which is a cornerstone of Physical AI.
1.3. Target Audience
- Primary: Beginner to intermediate students in Physical AI and Humanoid Robotics.
- Secondary: Hobbyists and developers looking to expand their skills into robotics simulation.
2. Functional Requirements
2.1. In Scope (Features)
- Gazebo Physics Simulation:
- Detailed explanation of core physics concepts: gravity, friction, and collision models.
- A tutorial on building a simple environment (e.g., a room with obstacles) using SDF.
- A demonstration of a humanoid robot interacting with the environment.
- Unity for High-Fidelity Rendering:
- An overview of Unity's rendering pipeline and its application in robotics.
- A guide to setting up a simple Unity scene for robotic visualization.
- Sensor Simulation:
- A guide to simulating common robotic sensors: LiDAR, Depth Cameras, and IMUs in both Gazebo and Unity.
- Configuration examples for each sensor.
- Runnable Examples:
- The module must include 2‚Äì3 complete, runnable code examples demonstrating the concepts.
- Format: The final output must be a Docusaurus-compatible Markdown file.
2.2. Out of Scope
- Full-scale game development or advanced Unity features.
- Complex humanoid control algorithms (this is reserved for Module 3).
- Networked or multi-agent simulations.
2.3. Success Criteria
- Knowledge: Students can clearly explain the role of physics engines (Gazebo) and rendering engines (Unity) in creating a digital twin.
- Practical Skill (Gazebo): Students can successfully build a simple Gazebo world and simulate a humanoid robot within it.
- Practical Skill (Unity): Students can set up a Unity scene to render a robot model.
- Practical Skill (Sensors): Students are able to add and configure virtual LiDAR, Depth Camera, and IMU sensors to a simulated robot.
- Completion: The module contains 2-3 fully functional examples that run without errors.
2.4. Constraints
- Word Count: 2000‚Äì3000 words.
- Format: Docusaurus Markdown, including formatted code blocks for SDF, C#, and Python/C++.
- Sources: Content must be based on official documentation from Gazebo, Unity, and relevant sensor simulation libraries.
- Timeline: 1 week for completion.
3. UX/UI (Reader Experience)
3.1. Content Structure
The module will be structured into clear, logical chapters to guide the reader progressively.
- Chapter 1: Gazebo Physics & Environments
- Introduction to Gazebo as a physics simulator.
- Core concepts: gravity, collisions, and inertia.
- Tutorial: Creating a
.world
file with ground plane and basic shapes. - Example: Spawning a humanoid model and observing basic physical interactions.
- Chapter 2: High-Fidelity Rendering with Unity
- The role of Unity in robotics for high-quality visualization.
- Setting up a Unity project for robotics (e.g., using ROS-Unity connectors).
- Example: Importing a URDF model and creating a simple visualization scene.
- Chapter 3: Simulating the Senses
- Introduction to sensor simulation.
- Simulating LiDAR and visualizing point clouds.
- Simulating Depth Cameras and interpreting depth images.
- Simulating an IMU to get orientation and acceleration data.
- Example: A single robot model equipped with all three sensors, with configuration snippets for each.
4. Technical Considerations
4.1. Key Decisions
- Simulation Tools: Gazebo will be used for robust physics simulation, while Unity will be highlighted for its superior rendering capabilities. The text will explain the trade-offs.
- Code Examples: Examples will be provided as self-contained code blocks that can be easily copied and run.
- Structure: The content will follow the chapter structure defined in section 3.1.
4.2. External Dependencies
- The module assumes readers have access to and a basic understanding of a Linux environment (for Gazebo/ROS).
- Readers will need to have Gazebo and Unity Hub (with a recent Unity version) installed.
- Code examples may rely on ROS 2,
rclpy
, orroscpp
for integration, building on concepts from Module 1.
--------------------------------------------------------------------------------
URL: https://Muham251.github.io/ai-hackathon-1/docs/module-2/#4-technical-considerations
Content:
Spec: Module 2: The Digital Twin (Gazebo & Unity)
1. Business Understanding
1.1. Goal
The goal of this module is to educate students on creating and utilizing digital twins for humanoid robotics. The focus is on mastering physics simulation, environment construction, and sensor simulation within Gazebo and leveraging Unity for high-fidelity rendering.
1.2. Justification
Digital twins are fundamental to modern robotics development, enabling safe, cost-effective, and efficient testing of robot behaviors before deployment. This module provides the essential skills for simulating robots and their interactions with the environment, which is a cornerstone of Physical AI.
1.3. Target Audience
- Primary: Beginner to intermediate students in Physical AI and Humanoid Robotics.
- Secondary: Hobbyists and developers looking to expand their skills into robotics simulation.
2. Functional Requirements
2.1. In Scope (Features)
- Gazebo Physics Simulation:
- Detailed explanation of core physics concepts: gravity, friction, and collision models.
- A tutorial on building a simple environment (e.g., a room with obstacles) using SDF.
- A demonstration of a humanoid robot interacting with the environment.
- Unity for High-Fidelity Rendering:
- An overview of Unity's rendering pipeline and its application in robotics.
- A guide to setting up a simple Unity scene for robotic visualization.
- Sensor Simulation:
- A guide to simulating common robotic sensors: LiDAR, Depth Cameras, and IMUs in both Gazebo and Unity.
- Configuration examples for each sensor.
- Runnable Examples:
- The module must include 2‚Äì3 complete, runnable code examples demonstrating the concepts.
- Format: The final output must be a Docusaurus-compatible Markdown file.
2.2. Out of Scope
- Full-scale game development or advanced Unity features.
- Complex humanoid control algorithms (this is reserved for Module 3).
- Networked or multi-agent simulations.
2.3. Success Criteria
- Knowledge: Students can clearly explain the role of physics engines (Gazebo) and rendering engines (Unity) in creating a digital twin.
- Practical Skill (Gazebo): Students can successfully build a simple Gazebo world and simulate a humanoid robot within it.
- Practical Skill (Unity): Students can set up a Unity scene to render a robot model.
- Practical Skill (Sensors): Students are able to add and configure virtual LiDAR, Depth Camera, and IMU sensors to a simulated robot.
- Completion: The module contains 2-3 fully functional examples that run without errors.
2.4. Constraints
- Word Count: 2000‚Äì3000 words.
- Format: Docusaurus Markdown, including formatted code blocks for SDF, C#, and Python/C++.
- Sources: Content must be based on official documentation from Gazebo, Unity, and relevant sensor simulation libraries.
- Timeline: 1 week for completion.
3. UX/UI (Reader Experience)
3.1. Content Structure
The module will be structured into clear, logical chapters to guide the reader progressively.
- Chapter 1: Gazebo Physics & Environments
- Introduction to Gazebo as a physics simulator.
- Core concepts: gravity, collisions, and inertia.
- Tutorial: Creating a
.world
file with ground plane and basic shapes. - Example: Spawning a humanoid model and observing basic physical interactions.
- Chapter 2: High-Fidelity Rendering with Unity
- The role of Unity in robotics for high-quality visualization.
- Setting up a Unity project for robotics (e.g., using ROS-Unity connectors).
- Example: Importing a URDF model and creating a simple visualization scene.
- Chapter 3: Simulating the Senses
- Introduction to sensor simulation.
- Simulating LiDAR and visualizing point clouds.
- Simulating Depth Cameras and interpreting depth images.
- Simulating an IMU to get orientation and acceleration data.
- Example: A single robot model equipped with all three sensors, with configuration snippets for each.
4. Technical Considerations
4.1. Key Decisions
- Simulation Tools: Gazebo will be used for robust physics simulation, while Unity will be highlighted for its superior rendering capabilities. The text will explain the trade-offs.
- Code Examples: Examples will be provided as self-contained code blocks that can be easily copied and run.
- Structure: The content will follow the chapter structure defined in section 3.1.
4.2. External Dependencies
- The module assumes readers have access to and a basic understanding of a Linux environment (for Gazebo/ROS).
- Readers will need to have Gazebo and Unity Hub (with a recent Unity version) installed.
- Code examples may rely on ROS 2,
rclpy
, orroscpp
for integration, building on concepts from Module 1.
--------------------------------------------------------------------------------
URL: https://Muham251.github.io/ai-hackathon-1/docs/module-2/#41-key-decisions
Content:
Spec: Module 2: The Digital Twin (Gazebo & Unity)
1. Business Understanding
1.1. Goal
The goal of this module is to educate students on creating and utilizing digital twins for humanoid robotics. The focus is on mastering physics simulation, environment construction, and sensor simulation within Gazebo and leveraging Unity for high-fidelity rendering.
1.2. Justification
Digital twins are fundamental to modern robotics development, enabling safe, cost-effective, and efficient testing of robot behaviors before deployment. This module provides the essential skills for simulating robots and their interactions with the environment, which is a cornerstone of Physical AI.
1.3. Target Audience
- Primary: Beginner to intermediate students in Physical AI and Humanoid Robotics.
- Secondary: Hobbyists and developers looking to expand their skills into robotics simulation.
2. Functional Requirements
2.1. In Scope (Features)
- Gazebo Physics Simulation:
- Detailed explanation of core physics concepts: gravity, friction, and collision models.
- A tutorial on building a simple environment (e.g., a room with obstacles) using SDF.
- A demonstration of a humanoid robot interacting with the environment.
- Unity for High-Fidelity Rendering:
- An overview of Unity's rendering pipeline and its application in robotics.
- A guide to setting up a simple Unity scene for robotic visualization.
- Sensor Simulation:
- A guide to simulating common robotic sensors: LiDAR, Depth Cameras, and IMUs in both Gazebo and Unity.
- Configuration examples for each sensor.
- Runnable Examples:
- The module must include 2‚Äì3 complete, runnable code examples demonstrating the concepts.
- Format: The final output must be a Docusaurus-compatible Markdown file.
2.2. Out of Scope
- Full-scale game development or advanced Unity features.
- Complex humanoid control algorithms (this is reserved for Module 3).
- Networked or multi-agent simulations.
2.3. Success Criteria
- Knowledge: Students can clearly explain the role of physics engines (Gazebo) and rendering engines (Unity) in creating a digital twin.
- Practical Skill (Gazebo): Students can successfully build a simple Gazebo world and simulate a humanoid robot within it.
- Practical Skill (Unity): Students can set up a Unity scene to render a robot model.
- Practical Skill (Sensors): Students are able to add and configure virtual LiDAR, Depth Camera, and IMU sensors to a simulated robot.
- Completion: The module contains 2-3 fully functional examples that run without errors.
2.4. Constraints
- Word Count: 2000‚Äì3000 words.
- Format: Docusaurus Markdown, including formatted code blocks for SDF, C#, and Python/C++.
- Sources: Content must be based on official documentation from Gazebo, Unity, and relevant sensor simulation libraries.
- Timeline: 1 week for completion.
3. UX/UI (Reader Experience)
3.1. Content Structure
The module will be structured into clear, logical chapters to guide the reader progressively.
- Chapter 1: Gazebo Physics & Environments
- Introduction to Gazebo as a physics simulator.
- Core concepts: gravity, collisions, and inertia.
- Tutorial: Creating a
.world
file with ground plane and basic shapes. - Example: Spawning a humanoid model and observing basic physical interactions.
- Chapter 2: High-Fidelity Rendering with Unity
- The role of Unity in robotics for high-quality visualization.
- Setting up a Unity project for robotics (e.g., using ROS-Unity connectors).
- Example: Importing a URDF model and creating a simple visualization scene.
- Chapter 3: Simulating the Senses
- Introduction to sensor simulation.
- Simulating LiDAR and visualizing point clouds.
- Simulating Depth Cameras and interpreting depth images.
- Simulating an IMU to get orientation and acceleration data.
- Example: A single robot model equipped with all three sensors, with configuration snippets for each.
4. Technical Considerations
4.1. Key Decisions
- Simulation Tools: Gazebo will be used for robust physics simulation, while Unity will be highlighted for its superior rendering capabilities. The text will explain the trade-offs.
- Code Examples: Examples will be provided as self-contained code blocks that can be easily copied and run.
- Structure: The content will follow the chapter structure defined in section 3.1.
4.2. External Dependencies
- The module assumes readers have access to and a basic understanding of a Linux environment (for Gazebo/ROS).
- Readers will need to have Gazebo and Unity Hub (with a recent Unity version) installed.
- Code examples may rely on ROS 2,
rclpy
, orroscpp
for integration, building on concepts from Module 1.
--------------------------------------------------------------------------------
URL: https://Muham251.github.io/ai-hackathon-1/docs/module-2/#42-external-dependencies
Content:
Spec: Module 2: The Digital Twin (Gazebo & Unity)
1. Business Understanding
1.1. Goal
The goal of this module is to educate students on creating and utilizing digital twins for humanoid robotics. The focus is on mastering physics simulation, environment construction, and sensor simulation within Gazebo and leveraging Unity for high-fidelity rendering.
1.2. Justification
Digital twins are fundamental to modern robotics development, enabling safe, cost-effective, and efficient testing of robot behaviors before deployment. This module provides the essential skills for simulating robots and their interactions with the environment, which is a cornerstone of Physical AI.
1.3. Target Audience
- Primary: Beginner to intermediate students in Physical AI and Humanoid Robotics.
- Secondary: Hobbyists and developers looking to expand their skills into robotics simulation.
2. Functional Requirements
2.1. In Scope (Features)
- Gazebo Physics Simulation:
- Detailed explanation of core physics concepts: gravity, friction, and collision models.
- A tutorial on building a simple environment (e.g., a room with obstacles) using SDF.
- A demonstration of a humanoid robot interacting with the environment.
- Unity for High-Fidelity Rendering:
- An overview of Unity's rendering pipeline and its application in robotics.
- A guide to setting up a simple Unity scene for robotic visualization.
- Sensor Simulation:
- A guide to simulating common robotic sensors: LiDAR, Depth Cameras, and IMUs in both Gazebo and Unity.
- Configuration examples for each sensor.
- Runnable Examples:
- The module must include 2‚Äì3 complete, runnable code examples demonstrating the concepts.
- Format: The final output must be a Docusaurus-compatible Markdown file.
2.2. Out of Scope
- Full-scale game development or advanced Unity features.
- Complex humanoid control algorithms (this is reserved for Module 3).
- Networked or multi-agent simulations.
2.3. Success Criteria
- Knowledge: Students can clearly explain the role of physics engines (Gazebo) and rendering engines (Unity) in creating a digital twin.
- Practical Skill (Gazebo): Students can successfully build a simple Gazebo world and simulate a humanoid robot within it.
- Practical Skill (Unity): Students can set up a Unity scene to render a robot model.
- Practical Skill (Sensors): Students are able to add and configure virtual LiDAR, Depth Camera, and IMU sensors to a simulated robot.
- Completion: The module contains 2-3 fully functional examples that run without errors.
2.4. Constraints
- Word Count: 2000‚Äì3000 words.
- Format: Docusaurus Markdown, including formatted code blocks for SDF, C#, and Python/C++.
- Sources: Content must be based on official documentation from Gazebo, Unity, and relevant sensor simulation libraries.
- Timeline: 1 week for completion.
3. UX/UI (Reader Experience)
3.1. Content Structure
The module will be structured into clear, logical chapters to guide the reader progressively.
- Chapter 1: Gazebo Physics & Environments
- Introduction to Gazebo as a physics simulator.
- Core concepts: gravity, collisions, and inertia.
- Tutorial: Creating a
.world
file with ground plane and basic shapes. - Example: Spawning a humanoid model and observing basic physical interactions.
- Chapter 2: High-Fidelity Rendering with Unity
- The role of Unity in robotics for high-quality visualization.
- Setting up a Unity project for robotics (e.g., using ROS-Unity connectors).
- Example: Importing a URDF model and creating a simple visualization scene.
- Chapter 3: Simulating the Senses
- Introduction to sensor simulation.
- Simulating LiDAR and visualizing point clouds.
- Simulating Depth Cameras and interpreting depth images.
- Simulating an IMU to get orientation and acceleration data.
- Example: A single robot model equipped with all three sensors, with configuration snippets for each.
4. Technical Considerations
4.1. Key Decisions
- Simulation Tools: Gazebo will be used for robust physics simulation, while Unity will be highlighted for its superior rendering capabilities. The text will explain the trade-offs.
- Code Examples: Examples will be provided as self-contained code blocks that can be easily copied and run.
- Structure: The content will follow the chapter structure defined in section 3.1.
4.2. External Dependencies
- The module assumes readers have access to and a basic understanding of a Linux environment (for Gazebo/ROS).
- Readers will need to have Gazebo and Unity Hub (with a recent Unity version) installed.
- Code examples may rely on ROS 2,
rclpy
, orroscpp
for integration, building on concepts from Module 1.
--------------------------------------------------------------------------------
URL: https://Muham251.github.io/ai-hackathon-1/docs/tutorial-basics/congratulations#whats-next
Content:
Congratulations!
You have just learned the basics of Docusaurus and made some changes to the initial template.
Docusaurus has much more to offer!
Have 5 more minutes? Take a look at versioning and i18n.
Anything unclear or buggy in this tutorial? Please report it!
What's next?
- Read the official documentation
- Modify your site configuration with
docusaurus.config.js
- Add navbar and footer items with
themeConfig
- Add a custom Design and Layout
- Add a search bar
- Find inspirations in the Docusaurus showcase
- Get involved in the Docusaurus Community
--------------------------------------------------------------------------------
URL: https://Muham251.github.io/ai-hackathon-1/docs/tutorial-basics/deploy-your-site#build-your-site
Content:
Deploy your site
Docusaurus is a static-site-generator (also called Jamstack).
It builds your site as simple static HTML, JavaScript and CSS files.
Build your site
Build your site for production:
npm run build
The static files are generated in the build
folder.
Deploy your site
Test your production build locally:
npm run serve
The build
folder is now served at http://localhost:3000/.
You can now deploy the build
folder almost anywhere easily, for free or very small cost (read the Deployment Guide).
--------------------------------------------------------------------------------
URL: https://Muham251.github.io/ai-hackathon-1/docs/tutorial-basics/deploy-your-site#deploy-your-site-1
Content:
Deploy your site
Docusaurus is a static-site-generator (also called Jamstack).
It builds your site as simple static HTML, JavaScript and CSS files.
Build your site
Build your site for production:
npm run build
The static files are generated in the build
folder.
Deploy your site
Test your production build locally:
npm run serve
The build
folder is now served at http://localhost:3000/.
You can now deploy the build
folder almost anywhere easily, for free or very small cost (read the Deployment Guide).
--------------------------------------------------------------------------------
URL: https://Muham251.github.io/ai-hackathon-1/docs/tutorial-basics/markdown-features#front-matter
Content:
Markdown Features
Docusaurus supports Markdown and a few additional features.
Front Matter
Markdown documents have metadata at the top called Front Matter:
---
id: my-doc-id
title: My document title
description: My document description
slug: /my-custom-url
---
## Markdown heading
Markdown text with [links](./hello.md)
Links
Regular Markdown links are supported, using url paths or relative file paths.
Let's see how to [Create a page](/create-a-page).
Let's see how to [Create a page](./create-a-page.md).
Result: Let's see how to Create a page.
Images
Regular Markdown images are supported.
You can use absolute paths to reference images in the static directory (static/img/docusaurus.png
):
![Docusaurus logo](/img/docusaurus.png)
You can reference images relative to the current file as well. This is particularly useful to colocate images close to the Markdown files using them:
![Docusaurus logo](./img/docusaurus.png)
Code Blocks
Markdown code blocks are supported with Syntax highlighting.
```jsx title="src/components/HelloDocusaurus.js"
function HelloDocusaurus() {
return <h1>Hello, Docusaurus!</h1>;
}
```
function HelloDocusaurus() {
return <h1>Hello, Docusaurus!</h1>;
}
Admonitions
Docusaurus has a special syntax to create admonitions and callouts:
:::tip[My tip]
Use this awesome feature option
:::
:::danger[Take care]
This action is dangerous
:::
Use this awesome feature option
This action is dangerous
MDX and React Components
MDX can make your documentation more interactive and allows using any React components inside Markdown:
export const Highlight = ({children, color}) => (
<span
style={{
backgroundColor: color,
borderRadius: '20px',
color: '#fff',
padding: '10px',
cursor: 'pointer',
}}
onClick={() => {
alert(`You clicked the color ${color} with label ${children}`)
}}>
{children}
</span>
);
This is <Highlight color="#25c2a0">Docusaurus green</Highlight> !
This is <Highlight color="#1877F2">Facebook blue</Highlight> !
This is Docusaurus green !
This is Facebook blue !
--------------------------------------------------------------------------------
URL: https://Muham251.github.io/ai-hackathon-1/docs/tutorial-basics/markdown-features#links
Content:
Markdown Features
Docusaurus supports Markdown and a few additional features.
Front Matter
Markdown documents have metadata at the top called Front Matter:
---
id: my-doc-id
title: My document title
description: My document description
slug: /my-custom-url
---
## Markdown heading
Markdown text with [links](./hello.md)
Links
Regular Markdown links are supported, using url paths or relative file paths.
Let's see how to [Create a page](/create-a-page).
Let's see how to [Create a page](./create-a-page.md).
Result: Let's see how to Create a page.
Images
Regular Markdown images are supported.
You can use absolute paths to reference images in the static directory (static/img/docusaurus.png
):
![Docusaurus logo](/img/docusaurus.png)
You can reference images relative to the current file as well. This is particularly useful to colocate images close to the Markdown files using them:
![Docusaurus logo](./img/docusaurus.png)
Code Blocks
Markdown code blocks are supported with Syntax highlighting.
```jsx title="src/components/HelloDocusaurus.js"
function HelloDocusaurus() {
return <h1>Hello, Docusaurus!</h1>;
}
```
function HelloDocusaurus() {
return <h1>Hello, Docusaurus!</h1>;
}
Admonitions
Docusaurus has a special syntax to create admonitions and callouts:
:::tip[My tip]
Use this awesome feature option
:::
:::danger[Take care]
This action is dangerous
:::
Use this awesome feature option
This action is dangerous
MDX and React Components
MDX can make your documentation more interactive and allows using any React components inside Markdown:
export const Highlight = ({children, color}) => (
<span
style={{
backgroundColor: color,
borderRadius: '20px',
color: '#fff',
padding: '10px',
cursor: 'pointer',
}}
onClick={() => {
alert(`You clicked the color ${color} with label ${children}`)
}}>
{children}
</span>
);
This is <Highlight color="#25c2a0">Docusaurus green</Highlight> !
This is <Highlight color="#1877F2">Facebook blue</Highlight> !
This is Docusaurus green !
This is Facebook blue !
--------------------------------------------------------------------------------
URL: https://Muham251.github.io/ai-hackathon-1/docs/tutorial-basics/markdown-features#images
Content:
Markdown Features
Docusaurus supports Markdown and a few additional features.
Front Matter
Markdown documents have metadata at the top called Front Matter:
---
id: my-doc-id
title: My document title
description: My document description
slug: /my-custom-url
---
## Markdown heading
Markdown text with [links](./hello.md)
Links
Regular Markdown links are supported, using url paths or relative file paths.
Let's see how to [Create a page](/create-a-page).
Let's see how to [Create a page](./create-a-page.md).
Result: Let's see how to Create a page.
Images
Regular Markdown images are supported.
You can use absolute paths to reference images in the static directory (static/img/docusaurus.png
):
![Docusaurus logo](/img/docusaurus.png)
You can reference images relative to the current file as well. This is particularly useful to colocate images close to the Markdown files using them:
![Docusaurus logo](./img/docusaurus.png)
Code Blocks
Markdown code blocks are supported with Syntax highlighting.
```jsx title="src/components/HelloDocusaurus.js"
function HelloDocusaurus() {
return <h1>Hello, Docusaurus!</h1>;
}
```
function HelloDocusaurus() {
return <h1>Hello, Docusaurus!</h1>;
}
Admonitions
Docusaurus has a special syntax to create admonitions and callouts:
:::tip[My tip]
Use this awesome feature option
:::
:::danger[Take care]
This action is dangerous
:::
Use this awesome feature option
This action is dangerous
MDX and React Components
MDX can make your documentation more interactive and allows using any React components inside Markdown:
export const Highlight = ({children, color}) => (
<span
style={{
backgroundColor: color,
borderRadius: '20px',
color: '#fff',
padding: '10px',
cursor: 'pointer',
}}
onClick={() => {
alert(`You clicked the color ${color} with label ${children}`)
}}>
{children}
</span>
);
This is <Highlight color="#25c2a0">Docusaurus green</Highlight> !
This is <Highlight color="#1877F2">Facebook blue</Highlight> !
This is Docusaurus green !
This is Facebook blue !
--------------------------------------------------------------------------------
URL: https://Muham251.github.io/ai-hackathon-1/docs/tutorial-basics/markdown-features#code-blocks
Content:
Markdown Features
Docusaurus supports Markdown and a few additional features.
Front Matter
Markdown documents have metadata at the top called Front Matter:
---
id: my-doc-id
title: My document title
description: My document description
slug: /my-custom-url
---
## Markdown heading
Markdown text with [links](./hello.md)
Links
Regular Markdown links are supported, using url paths or relative file paths.
Let's see how to [Create a page](/create-a-page).
Let's see how to [Create a page](./create-a-page.md).
Result: Let's see how to Create a page.
Images
Regular Markdown images are supported.
You can use absolute paths to reference images in the static directory (static/img/docusaurus.png
):
![Docusaurus logo](/img/docusaurus.png)
You can reference images relative to the current file as well. This is particularly useful to colocate images close to the Markdown files using them:
![Docusaurus logo](./img/docusaurus.png)
Code Blocks
Markdown code blocks are supported with Syntax highlighting.
```jsx title="src/components/HelloDocusaurus.js"
function HelloDocusaurus() {
return <h1>Hello, Docusaurus!</h1>;
}
```
function HelloDocusaurus() {
return <h1>Hello, Docusaurus!</h1>;
}
Admonitions
Docusaurus has a special syntax to create admonitions and callouts:
:::tip[My tip]
Use this awesome feature option
:::
:::danger[Take care]
This action is dangerous
:::
Use this awesome feature option
This action is dangerous
MDX and React Components
MDX can make your documentation more interactive and allows using any React components inside Markdown:
export const Highlight = ({children, color}) => (
<span
style={{
backgroundColor: color,
borderRadius: '20px',
color: '#fff',
padding: '10px',
cursor: 'pointer',
}}
onClick={() => {
alert(`You clicked the color ${color} with label ${children}`)
}}>
{children}
</span>
);
This is <Highlight color="#25c2a0">Docusaurus green</Highlight> !
This is <Highlight color="#1877F2">Facebook blue</Highlight> !
This is Docusaurus green !
This is Facebook blue !
--------------------------------------------------------------------------------
URL: https://Muham251.github.io/ai-hackathon-1/docs/tutorial-basics/markdown-features#admonitions
Content:
Markdown Features
Docusaurus supports Markdown and a few additional features.
Front Matter
Markdown documents have metadata at the top called Front Matter:
---
id: my-doc-id
title: My document title
description: My document description
slug: /my-custom-url
---
## Markdown heading
Markdown text with [links](./hello.md)
Links
Regular Markdown links are supported, using url paths or relative file paths.
Let's see how to [Create a page](/create-a-page).
Let's see how to [Create a page](./create-a-page.md).
Result: Let's see how to Create a page.
Images
Regular Markdown images are supported.
You can use absolute paths to reference images in the static directory (static/img/docusaurus.png
):
![Docusaurus logo](/img/docusaurus.png)
You can reference images relative to the current file as well. This is particularly useful to colocate images close to the Markdown files using them:
![Docusaurus logo](./img/docusaurus.png)
Code Blocks
Markdown code blocks are supported with Syntax highlighting.
```jsx title="src/components/HelloDocusaurus.js"
function HelloDocusaurus() {
return <h1>Hello, Docusaurus!</h1>;
}
```
function HelloDocusaurus() {
return <h1>Hello, Docusaurus!</h1>;
}
Admonitions
Docusaurus has a special syntax to create admonitions and callouts:
:::tip[My tip]
Use this awesome feature option
:::
:::danger[Take care]
This action is dangerous
:::
Use this awesome feature option
This action is dangerous
MDX and React Components
MDX can make your documentation more interactive and allows using any React components inside Markdown:
export const Highlight = ({children, color}) => (
<span
style={{
backgroundColor: color,
borderRadius: '20px',
color: '#fff',
padding: '10px',
cursor: 'pointer',
}}
onClick={() => {
alert(`You clicked the color ${color} with label ${children}`)
}}>
{children}
</span>
);
This is <Highlight color="#25c2a0">Docusaurus green</Highlight> !
This is <Highlight color="#1877F2">Facebook blue</Highlight> !
This is Docusaurus green !
This is Facebook blue !
--------------------------------------------------------------------------------
URL: https://Muham251.github.io/ai-hackathon-1/docs/tutorial-basics/markdown-features#mdx-and-react-components
Content:
Markdown Features
Docusaurus supports Markdown and a few additional features.
Front Matter
Markdown documents have metadata at the top called Front Matter:
---
id: my-doc-id
title: My document title
description: My document description
slug: /my-custom-url
---
## Markdown heading
Markdown text with [links](./hello.md)
Links
Regular Markdown links are supported, using url paths or relative file paths.
Let's see how to [Create a page](/create-a-page).
Let's see how to [Create a page](./create-a-page.md).
Result: Let's see how to Create a page.
Images
Regular Markdown images are supported.
You can use absolute paths to reference images in the static directory (static/img/docusaurus.png
):
![Docusaurus logo](/img/docusaurus.png)
You can reference images relative to the current file as well. This is particularly useful to colocate images close to the Markdown files using them:
![Docusaurus logo](./img/docusaurus.png)
Code Blocks
Markdown code blocks are supported with Syntax highlighting.
```jsx title="src/components/HelloDocusaurus.js"
function HelloDocusaurus() {
return <h1>Hello, Docusaurus!</h1>;
}
```
function HelloDocusaurus() {
return <h1>Hello, Docusaurus!</h1>;
}
Admonitions
Docusaurus has a special syntax to create admonitions and callouts:
:::tip[My tip]
Use this awesome feature option
:::
:::danger[Take care]
This action is dangerous
:::
Use this awesome feature option
This action is dangerous
MDX and React Components
MDX can make your documentation more interactive and allows using any React components inside Markdown:
export const Highlight = ({children, color}) => (
<span
style={{
backgroundColor: color,
borderRadius: '20px',
color: '#fff',
padding: '10px',
cursor: 'pointer',
}}
onClick={() => {
alert(`You clicked the color ${color} with label ${children}`)
}}>
{children}
</span>
);
This is <Highlight color="#25c2a0">Docusaurus green</Highlight> !
This is <Highlight color="#1877F2">Facebook blue</Highlight> !
This is Docusaurus green !
This is Facebook blue !
--------------------------------------------------------------------------------
URL: https://Muham251.github.io/ai-hackathon-1/docs/tutorial-basics/create-a-blog-post#create-your-first-post
Content:
Create a Blog Post
Docusaurus creates a page for each blog post, but also a blog index page, a tag system, an RSS feed...
Create your first Post
Create a file at blog/2021-02-28-greetings.md
:
blog/2021-02-28-greetings.md
---
slug: greetings
title: Greetings!
authors:
- name: Joel Marcey
title: Co-creator of Docusaurus 1
url: https://github.com/JoelMarcey
image_url: https://github.com/JoelMarcey.png
- name: S√©bastien Lorber
title: Docusaurus maintainer
url: https://sebastienlorber.com
image_url: https://github.com/slorber.png
tags: [greetings]
---
Congratulations, you have made your first post!
Feel free to play around and edit this post as much as you like.
A new blog post is now available at http://localhost:3000/blog/greetings.
--------------------------------------------------------------------------------
URL: https://Muham251.github.io/ai-hackathon-1/docs/tutorial-basics/create-a-document#create-your-first-doc
Content:
Create a Document
Documents are groups of pages connected through:
- a sidebar
- previous/next navigation
- versioning
Create your first Doc
Create a Markdown file at docs/hello.md
:
docs/hello.md
# Hello
This is my **first Docusaurus document**!
A new document is now available at http://localhost:3000/docs/hello.
Configure the Sidebar
Docusaurus automatically creates a sidebar from the docs
folder.
Add metadata to customize the sidebar label and position:
docs/hello.md
---
sidebar_label: 'Hi!'
sidebar_position: 3
---
# Hello
This is my **first Docusaurus document**!
It is also possible to create your sidebar explicitly in sidebars.js
:
sidebars.js
export default {
tutorialSidebar: [
'intro',
'hello',
{
type: 'category',
label: 'Tutorial',
items: ['tutorial-basics/create-a-document'],
},
],
};
--------------------------------------------------------------------------------
URL: https://Muham251.github.io/ai-hackathon-1/docs/tutorial-basics/create-a-document#configure-the-sidebar
Content:
Create a Document
Documents are groups of pages connected through:
- a sidebar
- previous/next navigation
- versioning
Create your first Doc
Create a Markdown file at docs/hello.md
:
docs/hello.md
# Hello
This is my **first Docusaurus document**!
A new document is now available at http://localhost:3000/docs/hello.
Configure the Sidebar
Docusaurus automatically creates a sidebar from the docs
folder.
Add metadata to customize the sidebar label and position:
docs/hello.md
---
sidebar_label: 'Hi!'
sidebar_position: 3
---
# Hello
This is my **first Docusaurus document**!
It is also possible to create your sidebar explicitly in sidebars.js
:
sidebars.js
export default {
tutorialSidebar: [
'intro',
'hello',
{
type: 'category',
label: 'Tutorial',
items: ['tutorial-basics/create-a-document'],
},
],
};
--------------------------------------------------------------------------------
URL: https://Muham251.github.io/ai-hackathon-1/docs/tutorial-basics/create-a-page#create-your-first-react-page
Content:
Create a Page
Add Markdown or React files to src/pages
to create a standalone page:
src/pages/index.js
‚Üílocalhost:3000/
src/pages/foo.md
‚Üílocalhost:3000/foo
src/pages/foo/bar.js
‚Üílocalhost:3000/foo/bar
Create your first React Page
Create a file at src/pages/my-react-page.js
:
src/pages/my-react-page.js
import React from 'react';
import Layout from '@theme/Layout';
export default function MyReactPage() {
return (
<Layout>
<h1>My React page</h1>
<p>This is a React page</p>
</Layout>
);
}
A new page is now available at http://localhost:3000/my-react-page.
Create your first Markdown Page
Create a file at src/pages/my-markdown-page.md
:
src/pages/my-markdown-page.md
# My Markdown page
This is a Markdown page
A new page is now available at http://localhost:3000/my-markdown-page.
--------------------------------------------------------------------------------
URL: https://Muham251.github.io/ai-hackathon-1/docs/tutorial-basics/create-a-page#create-your-first-markdown-page
Content:
Create a Page
Add Markdown or React files to src/pages
to create a standalone page:
src/pages/index.js
‚Üílocalhost:3000/
src/pages/foo.md
‚Üílocalhost:3000/foo
src/pages/foo/bar.js
‚Üílocalhost:3000/foo/bar
Create your first React Page
Create a file at src/pages/my-react-page.js
:
src/pages/my-react-page.js
import React from 'react';
import Layout from '@theme/Layout';
export default function MyReactPage() {
return (
<Layout>
<h1>My React page</h1>
<p>This is a React page</p>
</Layout>
);
}
A new page is now available at http://localhost:3000/my-react-page.
Create your first Markdown Page
Create a file at src/pages/my-markdown-page.md
:
src/pages/my-markdown-page.md
# My Markdown page
This is a Markdown page
A new page is now available at http://localhost:3000/my-markdown-page.
--------------------------------------------------------------------------------
URL: https://Muham251.github.io/ai-hackathon-1/docs/module-1/urdf-essentials#key-urdf-tags
Content:
URDF Basics for Humanoid Models
The Unified Robot Description Format (URDF) is an XML format used in ROS to describe all the physical elements of a robot model. For a humanoid, the URDF file is its digital blueprint. It defines the robot's body parts, how they are connected, and their physical properties.
A URDF file describes the robot as a tree of links connected by joints.
<link>
: A physical part of the robot (e.g., a forearm, a foot, a torso).<joint>
: The connection between two links. It defines how one link moves relative to another (e.g., rotating like an elbow, sliding, or being fixed).
Key URDF Tags
The <robot>
Tag
Every URDF file must start and end with the <robot>
tag. It's the root element, and you give your robot a name here.
<robot name="simple_humanoid">
... all your links and joints go here ...
</robot>
The <link>
Tag
Each link has a name and can contain three important sub-tags:
<visual>
: Defines the appearance of the link (shape, size, color, texture). This is what you see in simulation tools like RViz.<collision>
: Defines the collision geometry of the link. This is what the physics engine uses to calculate collisions with other objects. It's often a simpler shape than the visual one for performance reasons.<inertial>
: Defines the dynamic properties of the link: its mass, center of mass, and inertia matrix. This is crucial for realistic physics simulation.
<link name="torso">
<visual>
<geometry>
<box size="0.2 0.3 0.5"/>
</geometry>
<material name="grey">
<color rgba="0.5 0.5 0.5 1"/>
</material>
</visual>
<collision>
<geometry>
<box size="0.2 0.3 0.5"/>
</geometry>
</collision>
<inertial>
<mass value="10"/>
<inertia ixx="0.4" ixy="0.0" ixz="0.0" iyy="0.4" iyz="0.0" izz="0.2"/>
</inertial>
</link>
The <joint>
Tag
Each joint connects two links, called the parent and the child. It also defines the motion allowed between them.
name
: The name of the joint (e.g., "left_shoulder_joint").type
: The type of motion. Common types are:revolute
: Rotates around an axis (like an elbow). Requires<limit>
tags for upper and lower angles.continuous
: Rotates around an axis with no angle limits.prismatic
: Slides along an axis (like a piston).fixed
: No motion is allowed between the two links.
<parent link="..." />
: The name of the parent link.<child link="..." />
: The name of the child link.<origin xyz="..." rpy="..." />
: The transform (position and orientation) from the parent link's origin to the joint's origin.<axis xyz="..." />
: The axis of rotation or translation forrevolute
andprismatic
joints.
<joint name="neck_joint" type="revolute">
<parent link="torso"/>
<child link="head"/>
<origin xyz="0 0 0.25" rpy="0 0 0"/>
<axis xyz="0 0 1"/>
<limit lower="-0.5" upper="0.5" effort="10" velocity="1.0"/>
</joint>
Simple Humanoid URDF Example
This example describes a very simple humanoid robot. It has a torso, a head, and a single arm with a shoulder and elbow joint. This demonstrates the parent-child tree structure. The "torso" is the root link of our tree.
<!-- simple_humanoid.urdf -->
<robot name="simple_humanoid">
<!-- A. Materials (for color) -->
<material name="blue">
<color rgba="0.0 0.0 0.8 1.0"/>
</material>
<material name="white">
<color rgba="1.0 1.0 1.0 1.0"/>
</material>
<!-- B. Base Link (Torso) -->
<link name="torso">
<visual>
<geometry><box size="0.2 0.3 0.5"/></geometry>
<origin xyz="0 0 0" rpy="0 0 0"/>
<material name="blue"/>
</visual>
<collision><geometry><box size="0.2 0.3 0.5"/></geometry></collision>
<inertial>
<mass value="10"/>
<inertia ixx="1.0" ixy="0" ixz="0" iyy="1.0" iyz="0" izz="1.0"/>
</inertial>
</link>
<!-- C. Head Link -->
<link name="head">
<visual>
<geometry><sphere radius="0.1"/></geometry>
<origin xyz="0 0 0" rpy="0 0 0"/>
<material name="white"/>
</visual>
<collision><geometry><sphere radius="0.1"/></geometry></collision>
<inertial>
<mass value="1"/>
<inertia ixx="0.1" ixy="0" ixz="0" iyy="0.1" iyz="0" izz="0.1"/>
</inertial>
</link>
<!-- D. Neck Joint (Connects Torso to Head) -->
<joint name="neck_joint" type="revolute">
<parent link="torso"/>
<child link="head"/>
<origin xyz="0 0 0.30" rpy="0 0 0"/> <!-- Position of head relative to torso -->
<axis xyz="0 0 1"/>
<limit lower="-0.7" upper="0.7" effort="5" velocity="0.5"/>
</joint>
<!-- E. Right Arm -->
<link name="right_upper_arm">
<visual>
<geometry><cylinder length="0.3" radius="0.05"/></geometry>
<origin xyz="0 0 -0.15" rpy="0 0 0"/>
<material name="white"/>
</visual>
<collision><geometry><cylinder length="0.3" radius="0.05"/></geometry></collision>
<inertial>
<mass value="2"/>
<inertia ixx="0.1" ixy="0" ixz="0" iyy="0.1" iyz="0" izz="0.1"/>
</inertial>
</link>
<!-- F. Right Shoulder Joint -->
<joint name="right_shoulder_joint" type="revolute">
<parent link="torso"/>
<child link="right_upper_arm"/>
<origin xyz="0 -0.20 0.15" rpy="1.5707 0 0"/> <!-- Position of arm relative to torso -->
<axis xyz="0 0 1"/>
<limit lower="-1.57" upper="1.57" effort="10" velocity="1.0"/>
</joint>
</robot>
How to Use This URDF
- Save the code above as
simple_humanoid.urdf
. - You can check its validity using the
check_urdf
command:check_urdf simple_humanoid.urdf
. - To visualize it, you can use ROS tools like RViz. This typically requires a small launch file to publish the robot's state, which is a topic for a later module.
This simple example provides the foundation. A full humanoid model would extend this tree with more links and joints for the legs, a more complex arm, and hands.
--------------------------------------------------------------------------------
URL: https://Muham251.github.io/ai-hackathon-1/docs/module-1/urdf-essentials#the-robot-tag
Content:
URDF Basics for Humanoid Models
The Unified Robot Description Format (URDF) is an XML format used in ROS to describe all the physical elements of a robot model. For a humanoid, the URDF file is its digital blueprint. It defines the robot's body parts, how they are connected, and their physical properties.
A URDF file describes the robot as a tree of links connected by joints.
<link>
: A physical part of the robot (e.g., a forearm, a foot, a torso).<joint>
: The connection between two links. It defines how one link moves relative to another (e.g., rotating like an elbow, sliding, or being fixed).
Key URDF Tags
The <robot>
Tag
Every URDF file must start and end with the <robot>
tag. It's the root element, and you give your robot a name here.
<robot name="simple_humanoid">
... all your links and joints go here ...
</robot>
The <link>
Tag
Each link has a name and can contain three important sub-tags:
<visual>
: Defines the appearance of the link (shape, size, color, texture). This is what you see in simulation tools like RViz.<collision>
: Defines the collision geometry of the link. This is what the physics engine uses to calculate collisions with other objects. It's often a simpler shape than the visual one for performance reasons.<inertial>
: Defines the dynamic properties of the link: its mass, center of mass, and inertia matrix. This is crucial for realistic physics simulation.
<link name="torso">
<visual>
<geometry>
<box size="0.2 0.3 0.5"/>
</geometry>
<material name="grey">
<color rgba="0.5 0.5 0.5 1"/>
</material>
</visual>
<collision>
<geometry>
<box size="0.2 0.3 0.5"/>
</geometry>
</collision>
<inertial>
<mass value="10"/>
<inertia ixx="0.4" ixy="0.0" ixz="0.0" iyy="0.4" iyz="0.0" izz="0.2"/>
</inertial>
</link>
The <joint>
Tag
Each joint connects two links, called the parent and the child. It also defines the motion allowed between them.
name
: The name of the joint (e.g., "left_shoulder_joint").type
: The type of motion. Common types are:revolute
: Rotates around an axis (like an elbow). Requires<limit>
tags for upper and lower angles.continuous
: Rotates around an axis with no angle limits.prismatic
: Slides along an axis (like a piston).fixed
: No motion is allowed between the two links.
<parent link="..." />
: The name of the parent link.<child link="..." />
: The name of the child link.<origin xyz="..." rpy="..." />
: The transform (position and orientation) from the parent link's origin to the joint's origin.<axis xyz="..." />
: The axis of rotation or translation forrevolute
andprismatic
joints.
<joint name="neck_joint" type="revolute">
<parent link="torso"/>
<child link="head"/>
<origin xyz="0 0 0.25" rpy="0 0 0"/>
<axis xyz="0 0 1"/>
<limit lower="-0.5" upper="0.5" effort="10" velocity="1.0"/>
</joint>
Simple Humanoid URDF Example
This example describes a very simple humanoid robot. It has a torso, a head, and a single arm with a shoulder and elbow joint. This demonstrates the parent-child tree structure. The "torso" is the root link of our tree.
<!-- simple_humanoid.urdf -->
<robot name="simple_humanoid">
<!-- A. Materials (for color) -->
<material name="blue">
<color rgba="0.0 0.0 0.8 1.0"/>
</material>
<material name="white">
<color rgba="1.0 1.0 1.0 1.0"/>
</material>
<!-- B. Base Link (Torso) -->
<link name="torso">
<visual>
<geometry><box size="0.2 0.3 0.5"/></geometry>
<origin xyz="0 0 0" rpy="0 0 0"/>
<material name="blue"/>
</visual>
<collision><geometry><box size="0.2 0.3 0.5"/></geometry></collision>
<inertial>
<mass value="10"/>
<inertia ixx="1.0" ixy="0" ixz="0" iyy="1.0" iyz="0" izz="1.0"/>
</inertial>
</link>
<!-- C. Head Link -->
<link name="head">
<visual>
<geometry><sphere radius="0.1"/></geometry>
<origin xyz="0 0 0" rpy="0 0 0"/>
<material name="white"/>
</visual>
<collision><geometry><sphere radius="0.1"/></geometry></collision>
<inertial>
<mass value="1"/>
<inertia ixx="0.1" ixy="0" ixz="0" iyy="0.1" iyz="0" izz="0.1"/>
</inertial>
</link>
<!-- D. Neck Joint (Connects Torso to Head) -->
<joint name="neck_joint" type="revolute">
<parent link="torso"/>
<child link="head"/>
<origin xyz="0 0 0.30" rpy="0 0 0"/> <!-- Position of head relative to torso -->
<axis xyz="0 0 1"/>
<limit lower="-0.7" upper="0.7" effort="5" velocity="0.5"/>
</joint>
<!-- E. Right Arm -->
<link name="right_upper_arm">
<visual>
<geometry><cylinder length="0.3" radius="0.05"/></geometry>
<origin xyz="0 0 -0.15" rpy="0 0 0"/>
<material name="white"/>
</visual>
<collision><geometry><cylinder length="0.3" radius="0.05"/></geometry></collision>
<inertial>
<mass value="2"/>
<inertia ixx="0.1" ixy="0" ixz="0" iyy="0.1" iyz="0" izz="0.1"/>
</inertial>
</link>
<!-- F. Right Shoulder Joint -->
<joint name="right_shoulder_joint" type="revolute">
<parent link="torso"/>
<child link="right_upper_arm"/>
<origin xyz="0 -0.20 0.15" rpy="1.5707 0 0"/> <!-- Position of arm relative to torso -->
<axis xyz="0 0 1"/>
<limit lower="-1.57" upper="1.57" effort="10" velocity="1.0"/>
</joint>
</robot>
How to Use This URDF
- Save the code above as
simple_humanoid.urdf
. - You can check its validity using the
check_urdf
command:check_urdf simple_humanoid.urdf
. - To visualize it, you can use ROS tools like RViz. This typically requires a small launch file to publish the robot's state, which is a topic for a later module.
This simple example provides the foundation. A full humanoid model would extend this tree with more links and joints for the legs, a more complex arm, and hands.
--------------------------------------------------------------------------------
URL: https://Muham251.github.io/ai-hackathon-1/docs/module-1/urdf-essentials#the-link-tag
Content:
URDF Basics for Humanoid Models
The Unified Robot Description Format (URDF) is an XML format used in ROS to describe all the physical elements of a robot model. For a humanoid, the URDF file is its digital blueprint. It defines the robot's body parts, how they are connected, and their physical properties.
A URDF file describes the robot as a tree of links connected by joints.
<link>
: A physical part of the robot (e.g., a forearm, a foot, a torso).<joint>
: The connection between two links. It defines how one link moves relative to another (e.g., rotating like an elbow, sliding, or being fixed).
Key URDF Tags
The <robot>
Tag
Every URDF file must start and end with the <robot>
tag. It's the root element, and you give your robot a name here.
<robot name="simple_humanoid">
... all your links and joints go here ...
</robot>
The <link>
Tag
Each link has a name and can contain three important sub-tags:
<visual>
: Defines the appearance of the link (shape, size, color, texture). This is what you see in simulation tools like RViz.<collision>
: Defines the collision geometry of the link. This is what the physics engine uses to calculate collisions with other objects. It's often a simpler shape than the visual one for performance reasons.<inertial>
: Defines the dynamic properties of the link: its mass, center of mass, and inertia matrix. This is crucial for realistic physics simulation.
<link name="torso">
<visual>
<geometry>
<box size="0.2 0.3 0.5"/>
</geometry>
<material name="grey">
<color rgba="0.5 0.5 0.5 1"/>
</material>
</visual>
<collision>
<geometry>
<box size="0.2 0.3 0.5"/>
</geometry>
</collision>
<inertial>
<mass value="10"/>
<inertia ixx="0.4" ixy="0.0" ixz="0.0" iyy="0.4" iyz="0.0" izz="0.2"/>
</inertial>
</link>
The <joint>
Tag
Each joint connects two links, called the parent and the child. It also defines the motion allowed between them.
name
: The name of the joint (e.g., "left_shoulder_joint").type
: The type of motion. Common types are:revolute
: Rotates around an axis (like an elbow). Requires<limit>
tags for upper and lower angles.continuous
: Rotates around an axis with no angle limits.prismatic
: Slides along an axis (like a piston).fixed
: No motion is allowed between the two links.
<parent link="..." />
: The name of the parent link.<child link="..." />
: The name of the child link.<origin xyz="..." rpy="..." />
: The transform (position and orientation) from the parent link's origin to the joint's origin.<axis xyz="..." />
: The axis of rotation or translation forrevolute
andprismatic
joints.
<joint name="neck_joint" type="revolute">
<parent link="torso"/>
<child link="head"/>
<origin xyz="0 0 0.25" rpy="0 0 0"/>
<axis xyz="0 0 1"/>
<limit lower="-0.5" upper="0.5" effort="10" velocity="1.0"/>
</joint>
Simple Humanoid URDF Example
This example describes a very simple humanoid robot. It has a torso, a head, and a single arm with a shoulder and elbow joint. This demonstrates the parent-child tree structure. The "torso" is the root link of our tree.
<!-- simple_humanoid.urdf -->
<robot name="simple_humanoid">
<!-- A. Materials (for color) -->
<material name="blue">
<color rgba="0.0 0.0 0.8 1.0"/>
</material>
<material name="white">
<color rgba="1.0 1.0 1.0 1.0"/>
</material>
<!-- B. Base Link (Torso) -->
<link name="torso">
<visual>
<geometry><box size="0.2 0.3 0.5"/></geometry>
<origin xyz="0 0 0" rpy="0 0 0"/>
<material name="blue"/>
</visual>
<collision><geometry><box size="0.2 0.3 0.5"/></geometry></collision>
<inertial>
<mass value="10"/>
<inertia ixx="1.0" ixy="0" ixz="0" iyy="1.0" iyz="0" izz="1.0"/>
</inertial>
</link>
<!-- C. Head Link -->
<link name="head">
<visual>
<geometry><sphere radius="0.1"/></geometry>
<origin xyz="0 0 0" rpy="0 0 0"/>
<material name="white"/>
</visual>
<collision><geometry><sphere radius="0.1"/></geometry></collision>
<inertial>
<mass value="1"/>
<inertia ixx="0.1" ixy="0" ixz="0" iyy="0.1" iyz="0" izz="0.1"/>
</inertial>
</link>
<!-- D. Neck Joint (Connects Torso to Head) -->
<joint name="neck_joint" type="revolute">
<parent link="torso"/>
<child link="head"/>
<origin xyz="0 0 0.30" rpy="0 0 0"/> <!-- Position of head relative to torso -->
<axis xyz="0 0 1"/>
<limit lower="-0.7" upper="0.7" effort="5" velocity="0.5"/>
</joint>
<!-- E. Right Arm -->
<link name="right_upper_arm">
<visual>
<geometry><cylinder length="0.3" radius="0.05"/></geometry>
<origin xyz="0 0 -0.15" rpy="0 0 0"/>
<material name="white"/>
</visual>
<collision><geometry><cylinder length="0.3" radius="0.05"/></geometry></collision>
<inertial>
<mass value="2"/>
<inertia ixx="0.1" ixy="0" ixz="0" iyy="0.1" iyz="0" izz="0.1"/>
</inertial>
</link>
<!-- F. Right Shoulder Joint -->
<joint name="right_shoulder_joint" type="revolute">
<parent link="torso"/>
<child link="right_upper_arm"/>
<origin xyz="0 -0.20 0.15" rpy="1.5707 0 0"/> <!-- Position of arm relative to torso -->
<axis xyz="0 0 1"/>
<limit lower="-1.57" upper="1.57" effort="10" velocity="1.0"/>
</joint>
</robot>
How to Use This URDF
- Save the code above as
simple_humanoid.urdf
. - You can check its validity using the
check_urdf
command:check_urdf simple_humanoid.urdf
. - To visualize it, you can use ROS tools like RViz. This typically requires a small launch file to publish the robot's state, which is a topic for a later module.
This simple example provides the foundation. A full humanoid model would extend this tree with more links and joints for the legs, a more complex arm, and hands.
--------------------------------------------------------------------------------
URL: https://Muham251.github.io/ai-hackathon-1/docs/module-1/urdf-essentials#the-joint-tag
Content:
URDF Basics for Humanoid Models
The Unified Robot Description Format (URDF) is an XML format used in ROS to describe all the physical elements of a robot model. For a humanoid, the URDF file is its digital blueprint. It defines the robot's body parts, how they are connected, and their physical properties.
A URDF file describes the robot as a tree of links connected by joints.
<link>
: A physical part of the robot (e.g., a forearm, a foot, a torso).<joint>
: The connection between two links. It defines how one link moves relative to another (e.g., rotating like an elbow, sliding, or being fixed).
Key URDF Tags
The <robot>
Tag
Every URDF file must start and end with the <robot>
tag. It's the root element, and you give your robot a name here.
<robot name="simple_humanoid">
... all your links and joints go here ...
</robot>
The <link>
Tag
Each link has a name and can contain three important sub-tags:
<visual>
: Defines the appearance of the link (shape, size, color, texture). This is what you see in simulation tools like RViz.<collision>
: Defines the collision geometry of the link. This is what the physics engine uses to calculate collisions with other objects. It's often a simpler shape than the visual one for performance reasons.<inertial>
: Defines the dynamic properties of the link: its mass, center of mass, and inertia matrix. This is crucial for realistic physics simulation.
<link name="torso">
<visual>
<geometry>
<box size="0.2 0.3 0.5"/>
</geometry>
<material name="grey">
<color rgba="0.5 0.5 0.5 1"/>
</material>
</visual>
<collision>
<geometry>
<box size="0.2 0.3 0.5"/>
</geometry>
</collision>
<inertial>
<mass value="10"/>
<inertia ixx="0.4" ixy="0.0" ixz="0.0" iyy="0.4" iyz="0.0" izz="0.2"/>
</inertial>
</link>
The <joint>
Tag
Each joint connects two links, called the parent and the child. It also defines the motion allowed between them.
name
: The name of the joint (e.g., "left_shoulder_joint").type
: The type of motion. Common types are:revolute
: Rotates around an axis (like an elbow). Requires<limit>
tags for upper and lower angles.continuous
: Rotates around an axis with no angle limits.prismatic
: Slides along an axis (like a piston).fixed
: No motion is allowed between the two links.
<parent link="..." />
: The name of the parent link.<child link="..." />
: The name of the child link.<origin xyz="..." rpy="..." />
: The transform (position and orientation) from the parent link's origin to the joint's origin.<axis xyz="..." />
: The axis of rotation or translation forrevolute
andprismatic
joints.
<joint name="neck_joint" type="revolute">
<parent link="torso"/>
<child link="head"/>
<origin xyz="0 0 0.25" rpy="0 0 0"/>
<axis xyz="0 0 1"/>
<limit lower="-0.5" upper="0.5" effort="10" velocity="1.0"/>
</joint>
Simple Humanoid URDF Example
This example describes a very simple humanoid robot. It has a torso, a head, and a single arm with a shoulder and elbow joint. This demonstrates the parent-child tree structure. The "torso" is the root link of our tree.
<!-- simple_humanoid.urdf -->
<robot name="simple_humanoid">
<!-- A. Materials (for color) -->
<material name="blue">
<color rgba="0.0 0.0 0.8 1.0"/>
</material>
<material name="white">
<color rgba="1.0 1.0 1.0 1.0"/>
</material>
<!-- B. Base Link (Torso) -->
<link name="torso">
<visual>
<geometry><box size="0.2 0.3 0.5"/></geometry>
<origin xyz="0 0 0" rpy="0 0 0"/>
<material name="blue"/>
</visual>
<collision><geometry><box size="0.2 0.3 0.5"/></geometry></collision>
<inertial>
<mass value="10"/>
<inertia ixx="1.0" ixy="0" ixz="0" iyy="1.0" iyz="0" izz="1.0"/>
</inertial>
</link>
<!-- C. Head Link -->
<link name="head">
<visual>
<geometry><sphere radius="0.1"/></geometry>
<origin xyz="0 0 0" rpy="0 0 0"/>
<material name="white"/>
</visual>
<collision><geometry><sphere radius="0.1"/></geometry></collision>
<inertial>
<mass value="1"/>
<inertia ixx="0.1" ixy="0" ixz="0" iyy="0.1" iyz="0" izz="0.1"/>
</inertial>
</link>
<!-- D. Neck Joint (Connects Torso to Head) -->
<joint name="neck_joint" type="revolute">
<parent link="torso"/>
<child link="head"/>
<origin xyz="0 0 0.30" rpy="0 0 0"/> <!-- Position of head relative to torso -->
<axis xyz="0 0 1"/>
<limit lower="-0.7" upper="0.7" effort="5" velocity="0.5"/>
</joint>
<!-- E. Right Arm -->
<link name="right_upper_arm">
<visual>
<geometry><cylinder length="0.3" radius="0.05"/></geometry>
<origin xyz="0 0 -0.15" rpy="0 0 0"/>
<material name="white"/>
</visual>
<collision><geometry><cylinder length="0.3" radius="0.05"/></geometry></collision>
<inertial>
<mass value="2"/>
<inertia ixx="0.1" ixy="0" ixz="0" iyy="0.1" iyz="0" izz="0.1"/>
</inertial>
</link>
<!-- F. Right Shoulder Joint -->
<joint name="right_shoulder_joint" type="revolute">
<parent link="torso"/>
<child link="right_upper_arm"/>
<origin xyz="0 -0.20 0.15" rpy="1.5707 0 0"/> <!-- Position of arm relative to torso -->
<axis xyz="0 0 1"/>
<limit lower="-1.57" upper="1.57" effort="10" velocity="1.0"/>
</joint>
</robot>
How to Use This URDF
- Save the code above as
simple_humanoid.urdf
. - You can check its validity using the
check_urdf
command:check_urdf simple_humanoid.urdf
. - To visualize it, you can use ROS tools like RViz. This typically requires a small launch file to publish the robot's state, which is a topic for a later module.
This simple example provides the foundation. A full humanoid model would extend this tree with more links and joints for the legs, a more complex arm, and hands.
--------------------------------------------------------------------------------
URL: https://Muham251.github.io/ai-hackathon-1/docs/module-1/urdf-essentials#simple-humanoid-urdf-example
Content:
URDF Basics for Humanoid Models
The Unified Robot Description Format (URDF) is an XML format used in ROS to describe all the physical elements of a robot model. For a humanoid, the URDF file is its digital blueprint. It defines the robot's body parts, how they are connected, and their physical properties.
A URDF file describes the robot as a tree of links connected by joints.
<link>
: A physical part of the robot (e.g., a forearm, a foot, a torso).<joint>
: The connection between two links. It defines how one link moves relative to another (e.g., rotating like an elbow, sliding, or being fixed).
Key URDF Tags
The <robot>
Tag
Every URDF file must start and end with the <robot>
tag. It's the root element, and you give your robot a name here.
<robot name="simple_humanoid">
... all your links and joints go here ...
</robot>
The <link>
Tag
Each link has a name and can contain three important sub-tags:
<visual>
: Defines the appearance of the link (shape, size, color, texture). This is what you see in simulation tools like RViz.<collision>
: Defines the collision geometry of the link. This is what the physics engine uses to calculate collisions with other objects. It's often a simpler shape than the visual one for performance reasons.<inertial>
: Defines the dynamic properties of the link: its mass, center of mass, and inertia matrix. This is crucial for realistic physics simulation.
<link name="torso">
<visual>
<geometry>
<box size="0.2 0.3 0.5"/>
</geometry>
<material name="grey">
<color rgba="0.5 0.5 0.5 1"/>
</material>
</visual>
<collision>
<geometry>
<box size="0.2 0.3 0.5"/>
</geometry>
</collision>
<inertial>
<mass value="10"/>
<inertia ixx="0.4" ixy="0.0" ixz="0.0" iyy="0.4" iyz="0.0" izz="0.2"/>
</inertial>
</link>
The <joint>
Tag
Each joint connects two links, called the parent and the child. It also defines the motion allowed between them.
name
: The name of the joint (e.g., "left_shoulder_joint").type
: The type of motion. Common types are:revolute
: Rotates around an axis (like an elbow). Requires<limit>
tags for upper and lower angles.continuous
: Rotates around an axis with no angle limits.prismatic
: Slides along an axis (like a piston).fixed
: No motion is allowed between the two links.
<parent link="..." />
: The name of the parent link.<child link="..." />
: The name of the child link.<origin xyz="..." rpy="..." />
: The transform (position and orientation) from the parent link's origin to the joint's origin.<axis xyz="..." />
: The axis of rotation or translation forrevolute
andprismatic
joints.
<joint name="neck_joint" type="revolute">
<parent link="torso"/>
<child link="head"/>
<origin xyz="0 0 0.25" rpy="0 0 0"/>
<axis xyz="0 0 1"/>
<limit lower="-0.5" upper="0.5" effort="10" velocity="1.0"/>
</joint>
Simple Humanoid URDF Example
This example describes a very simple humanoid robot. It has a torso, a head, and a single arm with a shoulder and elbow joint. This demonstrates the parent-child tree structure. The "torso" is the root link of our tree.
<!-- simple_humanoid.urdf -->
<robot name="simple_humanoid">
<!-- A. Materials (for color) -->
<material name="blue">
<color rgba="0.0 0.0 0.8 1.0"/>
</material>
<material name="white">
<color rgba="1.0 1.0 1.0 1.0"/>
</material>
<!-- B. Base Link (Torso) -->
<link name="torso">
<visual>
<geometry><box size="0.2 0.3 0.5"/></geometry>
<origin xyz="0 0 0" rpy="0 0 0"/>
<material name="blue"/>
</visual>
<collision><geometry><box size="0.2 0.3 0.5"/></geometry></collision>
<inertial>
<mass value="10"/>
<inertia ixx="1.0" ixy="0" ixz="0" iyy="1.0" iyz="0" izz="1.0"/>
</inertial>
</link>
<!-- C. Head Link -->
<link name="head">
<visual>
<geometry><sphere radius="0.1"/></geometry>
<origin xyz="0 0 0" rpy="0 0 0"/>
<material name="white"/>
</visual>
<collision><geometry><sphere radius="0.1"/></geometry></collision>
<inertial>
<mass value="1"/>
<inertia ixx="0.1" ixy="0" ixz="0" iyy="0.1" iyz="0" izz="0.1"/>
</inertial>
</link>
<!-- D. Neck Joint (Connects Torso to Head) -->
<joint name="neck_joint" type="revolute">
<parent link="torso"/>
<child link="head"/>
<origin xyz="0 0 0.30" rpy="0 0 0"/> <!-- Position of head relative to torso -->
<axis xyz="0 0 1"/>
<limit lower="-0.7" upper="0.7" effort="5" velocity="0.5"/>
</joint>
<!-- E. Right Arm -->
<link name="right_upper_arm">
<visual>
<geometry><cylinder length="0.3" radius="0.05"/></geometry>
<origin xyz="0 0 -0.15" rpy="0 0 0"/>
<material name="white"/>
</visual>
<collision><geometry><cylinder length="0.3" radius="0.05"/></geometry></collision>
<inertial>
<mass value="2"/>
<inertia ixx="0.1" ixy="0" ixz="0" iyy="0.1" iyz="0" izz="0.1"/>
</inertial>
</link>
<!-- F. Right Shoulder Joint -->
<joint name="right_shoulder_joint" type="revolute">
<parent link="torso"/>
<child link="right_upper_arm"/>
<origin xyz="0 -0.20 0.15" rpy="1.5707 0 0"/> <!-- Position of arm relative to torso -->
<axis xyz="0 0 1"/>
<limit lower="-1.57" upper="1.57" effort="10" velocity="1.0"/>
</joint>
</robot>
How to Use This URDF
- Save the code above as
simple_humanoid.urdf
. - You can check its validity using the
check_urdf
command:check_urdf simple_humanoid.urdf
. - To visualize it, you can use ROS tools like RViz. This typically requires a small launch file to publish the robot's state, which is a topic for a later module.
This simple example provides the foundation. A full humanoid model would extend this tree with more links and joints for the legs, a more complex arm, and hands.
--------------------------------------------------------------------------------
URL: https://Muham251.github.io/ai-hackathon-1/docs/module-1/urdf-essentials#how-to-use-this-urdf
Content:
URDF Basics for Humanoid Models
The Unified Robot Description Format (URDF) is an XML format used in ROS to describe all the physical elements of a robot model. For a humanoid, the URDF file is its digital blueprint. It defines the robot's body parts, how they are connected, and their physical properties.
A URDF file describes the robot as a tree of links connected by joints.
<link>
: A physical part of the robot (e.g., a forearm, a foot, a torso).<joint>
: The connection between two links. It defines how one link moves relative to another (e.g., rotating like an elbow, sliding, or being fixed).
Key URDF Tags
The <robot>
Tag
Every URDF file must start and end with the <robot>
tag. It's the root element, and you give your robot a name here.
<robot name="simple_humanoid">
... all your links and joints go here ...
</robot>
The <link>
Tag
Each link has a name and can contain three important sub-tags:
<visual>
: Defines the appearance of the link (shape, size, color, texture). This is what you see in simulation tools like RViz.<collision>
: Defines the collision geometry of the link. This is what the physics engine uses to calculate collisions with other objects. It's often a simpler shape than the visual one for performance reasons.<inertial>
: Defines the dynamic properties of the link: its mass, center of mass, and inertia matrix. This is crucial for realistic physics simulation.
<link name="torso">
<visual>
<geometry>
<box size="0.2 0.3 0.5"/>
</geometry>
<material name="grey">
<color rgba="0.5 0.5 0.5 1"/>
</material>
</visual>
<collision>
<geometry>
<box size="0.2 0.3 0.5"/>
</geometry>
</collision>
<inertial>
<mass value="10"/>
<inertia ixx="0.4" ixy="0.0" ixz="0.0" iyy="0.4" iyz="0.0" izz="0.2"/>
</inertial>
</link>
The <joint>
Tag
Each joint connects two links, called the parent and the child. It also defines the motion allowed between them.
name
: The name of the joint (e.g., "left_shoulder_joint").type
: The type of motion. Common types are:revolute
: Rotates around an axis (like an elbow). Requires<limit>
tags for upper and lower angles.continuous
: Rotates around an axis with no angle limits.prismatic
: Slides along an axis (like a piston).fixed
: No motion is allowed between the two links.
<parent link="..." />
: The name of the parent link.<child link="..." />
: The name of the child link.<origin xyz="..." rpy="..." />
: The transform (position and orientation) from the parent link's origin to the joint's origin.<axis xyz="..." />
: The axis of rotation or translation forrevolute
andprismatic
joints.
<joint name="neck_joint" type="revolute">
<parent link="torso"/>
<child link="head"/>
<origin xyz="0 0 0.25" rpy="0 0 0"/>
<axis xyz="0 0 1"/>
<limit lower="-0.5" upper="0.5" effort="10" velocity="1.0"/>
</joint>
Simple Humanoid URDF Example
This example describes a very simple humanoid robot. It has a torso, a head, and a single arm with a shoulder and elbow joint. This demonstrates the parent-child tree structure. The "torso" is the root link of our tree.
<!-- simple_humanoid.urdf -->
<robot name="simple_humanoid">
<!-- A. Materials (for color) -->
<material name="blue">
<color rgba="0.0 0.0 0.8 1.0"/>
</material>
<material name="white">
<color rgba="1.0 1.0 1.0 1.0"/>
</material>
<!-- B. Base Link (Torso) -->
<link name="torso">
<visual>
<geometry><box size="0.2 0.3 0.5"/></geometry>
<origin xyz="0 0 0" rpy="0 0 0"/>
<material name="blue"/>
</visual>
<collision><geometry><box size="0.2 0.3 0.5"/></geometry></collision>
<inertial>
<mass value="10"/>
<inertia ixx="1.0" ixy="0" ixz="0" iyy="1.0" iyz="0" izz="1.0"/>
</inertial>
</link>
<!-- C. Head Link -->
<link name="head">
<visual>
<geometry><sphere radius="0.1"/></geometry>
<origin xyz="0 0 0" rpy="0 0 0"/>
<material name="white"/>
</visual>
<collision><geometry><sphere radius="0.1"/></geometry></collision>
<inertial>
<mass value="1"/>
<inertia ixx="0.1" ixy="0" ixz="0" iyy="0.1" iyz="0" izz="0.1"/>
</inertial>
</link>
<!-- D. Neck Joint (Connects Torso to Head) -->
<joint name="neck_joint" type="revolute">
<parent link="torso"/>
<child link="head"/>
<origin xyz="0 0 0.30" rpy="0 0 0"/> <!-- Position of head relative to torso -->
<axis xyz="0 0 1"/>
<limit lower="-0.7" upper="0.7" effort="5" velocity="0.5"/>
</joint>
<!-- E. Right Arm -->
<link name="right_upper_arm">
<visual>
<geometry><cylinder length="0.3" radius="0.05"/></geometry>
<origin xyz="0 0 -0.15" rpy="0 0 0"/>
<material name="white"/>
</visual>
<collision><geometry><cylinder length="0.3" radius="0.05"/></geometry></collision>
<inertial>
<mass value="2"/>
<inertia ixx="0.1" ixy="0" ixz="0" iyy="0.1" iyz="0" izz="0.1"/>
</inertial>
</link>
<!-- F. Right Shoulder Joint -->
<joint name="right_shoulder_joint" type="revolute">
<parent link="torso"/>
<child link="right_upper_arm"/>
<origin xyz="0 -0.20 0.15" rpy="1.5707 0 0"/> <!-- Position of arm relative to torso -->
<axis xyz="0 0 1"/>
<limit lower="-1.57" upper="1.57" effort="10" velocity="1.0"/>
</joint>
</robot>
How to Use This URDF
- Save the code above as
simple_humanoid.urdf
. - You can check its validity using the
check_urdf
command:check_urdf simple_humanoid.urdf
. - To visualize it, you can use ROS tools like RViz. This typically requires a small launch file to publish the robot's state, which is a topic for a later module.
This simple example provides the foundation. A full humanoid model would extend this tree with more links and joints for the legs, a more complex arm, and hands.
--------------------------------------------------------------------------------
URL: https://Muham251.github.io/ai-hackathon-1/docs/module-1/ros-2-basics-for-humanoid-control#target-audience
Content:
Module 1: The Robotic Nervous System (ROS 2)
Welcome to Module 1! This module introduces the Robot Operating System (ROS 2), the fundamental software framework that acts as the nervous system for modern robots, including humanoids. As we journey into controlling complex humanoid robots, understanding ROS 2 is the essential first step.
Target Audience
This module is designed for students and developers who are new to robotics but have some programming experience, particularly in Python. If you're aiming to understand how to make a humanoid robot walk, talk, or interact with its environment, you're in the right place.
What is ROS 2?
ROS 2 is a flexible framework for writing robot software. It is a set of software libraries and tools that help you build robot applications. Think of it as the middleware that handles all the complex communication between different parts of your robot's software. From low-level motor control to high-level planning and perception, ROS 2 provides the tools to manage it all.
For a humanoid robot, this is crucial. A humanoid may have dozens of motors (actuators), a suite of sensors (cameras, IMUs, force sensors), and multiple computers running different software modules. ROS 2 allows these components to communicate with each other in a standardized way.
Learning Objectives
By the end of this module, you will be able to:
- Explain the core components of the ROS 2 architecture.
- Create and run ROS 2 nodes, topics, and services.
- Write a simple Python agent that communicates with a ROS 2 system.
- Understand and write a basic URDF file for a humanoid robot model.
Module Chapters
This module is divided into the following chapters:
- ROS 2 Fundamentals (Nodes, Topics, Services): A deep dive into the core communication patterns in ROS 2.
- URDF Basics for Humanoid Models: An introduction to the Unified Robot Description Format (URDF) for modeling your humanoid.
Let's get started!
--------------------------------------------------------------------------------
URL: https://Muham251.github.io/ai-hackathon-1/docs/module-1/ros-2-basics-for-humanoid-control#what-is-ros-2
Content:
Module 1: The Robotic Nervous System (ROS 2)
Welcome to Module 1! This module introduces the Robot Operating System (ROS 2), the fundamental software framework that acts as the nervous system for modern robots, including humanoids. As we journey into controlling complex humanoid robots, understanding ROS 2 is the essential first step.
Target Audience
This module is designed for students and developers who are new to robotics but have some programming experience, particularly in Python. If you're aiming to understand how to make a humanoid robot walk, talk, or interact with its environment, you're in the right place.
What is ROS 2?
ROS 2 is a flexible framework for writing robot software. It is a set of software libraries and tools that help you build robot applications. Think of it as the middleware that handles all the complex communication between different parts of your robot's software. From low-level motor control to high-level planning and perception, ROS 2 provides the tools to manage it all.
For a humanoid robot, this is crucial. A humanoid may have dozens of motors (actuators), a suite of sensors (cameras, IMUs, force sensors), and multiple computers running different software modules. ROS 2 allows these components to communicate with each other in a standardized way.
Learning Objectives
By the end of this module, you will be able to:
- Explain the core components of the ROS 2 architecture.
- Create and run ROS 2 nodes, topics, and services.
- Write a simple Python agent that communicates with a ROS 2 system.
- Understand and write a basic URDF file for a humanoid robot model.
Module Chapters
This module is divided into the following chapters:
- ROS 2 Fundamentals (Nodes, Topics, Services): A deep dive into the core communication patterns in ROS 2.
- URDF Basics for Humanoid Models: An introduction to the Unified Robot Description Format (URDF) for modeling your humanoid.
Let's get started!
--------------------------------------------------------------------------------
URL: https://Muham251.github.io/ai-hackathon-1/docs/module-1/ros-2-basics-for-humanoid-control#learning-objectives
Content:
Module 1: The Robotic Nervous System (ROS 2)
Welcome to Module 1! This module introduces the Robot Operating System (ROS 2), the fundamental software framework that acts as the nervous system for modern robots, including humanoids. As we journey into controlling complex humanoid robots, understanding ROS 2 is the essential first step.
Target Audience
This module is designed for students and developers who are new to robotics but have some programming experience, particularly in Python. If you're aiming to understand how to make a humanoid robot walk, talk, or interact with its environment, you're in the right place.
What is ROS 2?
ROS 2 is a flexible framework for writing robot software. It is a set of software libraries and tools that help you build robot applications. Think of it as the middleware that handles all the complex communication between different parts of your robot's software. From low-level motor control to high-level planning and perception, ROS 2 provides the tools to manage it all.
For a humanoid robot, this is crucial. A humanoid may have dozens of motors (actuators), a suite of sensors (cameras, IMUs, force sensors), and multiple computers running different software modules. ROS 2 allows these components to communicate with each other in a standardized way.
Learning Objectives
By the end of this module, you will be able to:
- Explain the core components of the ROS 2 architecture.
- Create and run ROS 2 nodes, topics, and services.
- Write a simple Python agent that communicates with a ROS 2 system.
- Understand and write a basic URDF file for a humanoid robot model.
Module Chapters
This module is divided into the following chapters:
- ROS 2 Fundamentals (Nodes, Topics, Services): A deep dive into the core communication patterns in ROS 2.
- URDF Basics for Humanoid Models: An introduction to the Unified Robot Description Format (URDF) for modeling your humanoid.
Let's get started!
--------------------------------------------------------------------------------
URL: https://Muham251.github.io/ai-hackathon-1/docs/module-1/ros-2-basics-for-humanoid-control#module-chapters
Content:
Module 1: The Robotic Nervous System (ROS 2)
Welcome to Module 1! This module introduces the Robot Operating System (ROS 2), the fundamental software framework that acts as the nervous system for modern robots, including humanoids. As we journey into controlling complex humanoid robots, understanding ROS 2 is the essential first step.
Target Audience
This module is designed for students and developers who are new to robotics but have some programming experience, particularly in Python. If you're aiming to understand how to make a humanoid robot walk, talk, or interact with its environment, you're in the right place.
What is ROS 2?
ROS 2 is a flexible framework for writing robot software. It is a set of software libraries and tools that help you build robot applications. Think of it as the middleware that handles all the complex communication between different parts of your robot's software. From low-level motor control to high-level planning and perception, ROS 2 provides the tools to manage it all.
For a humanoid robot, this is crucial. A humanoid may have dozens of motors (actuators), a suite of sensors (cameras, IMUs, force sensors), and multiple computers running different software modules. ROS 2 allows these components to communicate with each other in a standardized way.
Learning Objectives
By the end of this module, you will be able to:
- Explain the core components of the ROS 2 architecture.
- Create and run ROS 2 nodes, topics, and services.
- Write a simple Python agent that communicates with a ROS 2 system.
- Understand and write a basic URDF file for a humanoid robot model.
Module Chapters
This module is divided into the following chapters:
- ROS 2 Fundamentals (Nodes, Topics, Services): A deep dive into the core communication patterns in ROS 2.
- URDF Basics for Humanoid Models: An introduction to the Unified Robot Description Format (URDF) for modeling your humanoid.
Let's get started!
--------------------------------------------------------------------------------
URL: https://Muham251.github.io/ai-hackathon-1/docs/module-1/nodes-topics-services-rclpy#1-ros-2-nodes
Content:
ROS 2 Fundamentals & Python
This chapter covers the three fundamental communication concepts in ROS 2: Nodes, Topics, and Services. We'll also see how to implement them using rclpy
, the official Python client library for ROS 2.
1. ROS 2 Nodes
A Node is the smallest, most fundamental unit of a ROS 2 system. Think of a node as a single, independent process in your robot's software architecture. Each node should have a single, well-defined purpose.
For a humanoid robot, you might have nodes for:
- Reading sensor data from an IMU.
- Controlling the motors in the right leg.
- Processing camera images.
- Planning footsteps for walking.
Nodes are what allow ROS 2 systems to be modular and fault-tolerant. If one node crashes, the rest of the system can continue to run.
Example: A Simple rclpy
Node
Here is the "hello world" of rclpy
: a simple node that initializes itself, prints a message, and then shuts down.
# 'hello_node.py'
import rclpy
from rclpy.node import Node
def main(args=None):
# 1. Initialize the rclpy library
rclpy.init(args=args)
# 2. Create a Node
# The node is named 'hello_ros_node'
node = Node('hello_ros_node')
# 3. From this point, the node is running.
# You can add your logic here.
node.get_logger().info('Hello, ROS 2 World!')
# 4. The node is destroyed when the 'with' statement exits.
# This is important for cleanup.
node.destroy_node()
# 5. Shutdown the rclpy library
rclpy.shutdown()
if __name__ == '__main__':
main()
To run this code:
- Save it as
hello_node.py
. - Make sure you have a ROS 2 environment sourced.
- Run
python hello_node.py
.
2. ROS 2 Topics (Publish/Subscribe)
Topics are the primary mechanism for one-way, asynchronous communication in ROS 2. Nodes can publish messages to a topic, and other nodes can subscribe to that topic to receive those messages. This is a decoupled communication model; publishers and subscribers don't need to know about each other.
- Use Case: Continuously streaming data, like sensor readings or motor commands.
Diagram: Publisher-Subscriber Model
graph TD
A[Publisher Node] -- Publishes message --> B(Topic: /sensor_data);
B -- Message is delivered --> C[Subscriber Node 1];
B -- Message is delivered --> D[Subscriber Node 2];
Example: A Simple Publisher and Subscriber
First, we define a publisher node that sends a simple string message every second.
# 'publisher_node.py'
import rclpy
from rclpy.node import Node
from std_msgs.msg import String
class SimplePublisher(Node):
def __init__(self):
super().__init__('simple_publisher')
# Create a publisher on the 'chatter' topic
self.publisher_ = self.create_publisher(String, 'chatter', 10)
self.timer = self.create_timer(1.0, self.timer_callback)
self.i = 0
def timer_callback(self):
msg = String()
msg.data = f'Hello from publisher: {self.i}'
self.publisher_.publish(msg)
self.get_logger().info(f'Publishing: "{msg.data}"')
self.i += 1
def main(args=None):
rclpy.init(args=args)
publisher = SimplePublisher()
rclpy.spin(publisher) # Keep the node alive
publisher.destroy_node()
rclpy.shutdown()
if __name__ == '__main__':
main()
Next, a subscriber node that listens to the chatter
topic.
# 'subscriber_node.py'
import rclpy
from rclpy.node import Node
from std_msgs.msg import String
class SimpleSubscriber(Node):
def __init__(self):
super().__init__('simple_subscriber')
# Create a subscription to the 'chatter' topic
self.subscription = self.create_subscription(
String,
'chatter',
self.listener_callback,
10)
self.subscription # prevent unused variable warning
def listener_callback(self, msg):
self.get_logger().info(f'I heard: "{msg.data}"')
def main(args=None):
rclpy.init(args=args)
subscriber = SimpleSubscriber()
rclpy.spin(subscriber)
subscriber.destroy_node()
rclpy.shutdown()
if __name__ == '__main__':
main()
To run this example:
- Open two terminals with your ROS 2 environment sourced.
- In the first terminal, run
python publisher_node.py
. - In the second terminal, run
python subscriber_node.py
. You will see the messages from the publisher being received.
3. ROS 2 Services (Request/Response)
Services are for two-way, synchronous communication. A Service Server node provides a service, and a Service Client node can send a request and wait for a response. This is a tightly coupled, blocking communication model.
- Use Case: Triggering a specific action that has a clear start and end, like "open gripper" or "calculate inverse kinematics".
Diagram: Service Client/Server Model
graph TD
A[Service Client Node] -- Sends Request --> B(Service: /add_two_ints);
B -- Delivers Request --> C[Service Server Node];
C -- Processes and sends Response --> B;
B -- Delivers Response --> A;
Example: A Simple Service Server and Client
First, the server node that offers a service to add two integers. ROS 2 has a standard service definition for this, example_interfaces/srv/AddTwoInts
.
# 'service_server_node.py'
import rclpy
from rclpy.node import Node
from example_interfaces.srv import AddTwoInts
class SimpleServiceServer(Node):
def __init__(self):
super().__init__('simple_service_server')
# Create a service named 'add_two_ints'
self.srv = self.create_service(AddTwoInts, 'add_two_ints', self.add_two_ints_callback)
def add_two_ints_callback(self, request, response):
response.sum = request.a + request.b
self.get_logger().info(f'Incoming request: a={request.a}, b={request.b}. Returning sum={response.sum}')
return response
def main(args=None):
rclpy.init(args=args)
server = SimpleServiceServer()
rclpy.spin(server)
server.destroy_node()
rclpy.shutdown()
if __name__ == '__main__':
main()
Next, the client node that calls the service.
# 'service_client_node.py'
import rclpy
from rclpy.node import Node
from example_interfaces.srv import AddTwoInts
import sys
class SimpleServiceClient(Node):
def __init__(self):
super().__init__('simple_service_client')
# Create a client for the 'add_two_ints' service
self.client = self.create_client(AddTwoInts, 'add_two_ints')
while not self.client.wait_for_service(timeout_sec=1.0):
self.get_logger().info('Service not available, waiting again...')
self.req = AddTwoInts.Request()
def send_request(self, a, b):
self.req.a = a
self.req.b = b
self.future = self.client.call_async(self.req)
rclpy.spin_until_future_complete(self, self.future)
return self.future.result()
def main(args=None):
rclpy.init(args=args)
if len(sys.argv) != 3:
print("Usage: python service_client_node.py <int1> <int2>")
return
client = SimpleServiceClient()
response = client.send_request(int(sys.argv[1]), int(sys.argv[2]))
if response:
client.get_logger().info(
f'Result of add_two_ints: for {sys.argv[1]} + {sys.argv[2]} = {response.sum}')
else:
client.get_logger().error('Service call failed')
client.destroy_node()
rclpy.shutdown()
if __name__ == '__main__':
main()
To run this example:
- Open two terminals with your ROS 2 environment sourced.
- In the first, run
python service_server_node.py
. - In the second, run
python service_client_node.py 5 10
. The client will send the request and print the response.
--------------------------------------------------------------------------------
URL: https://Muham251.github.io/ai-hackathon-1/docs/module-1/nodes-topics-services-rclpy#example-a-simple-rclpy-node
Content:
ROS 2 Fundamentals & Python
This chapter covers the three fundamental communication concepts in ROS 2: Nodes, Topics, and Services. We'll also see how to implement them using rclpy
, the official Python client library for ROS 2.
1. ROS 2 Nodes
A Node is the smallest, most fundamental unit of a ROS 2 system. Think of a node as a single, independent process in your robot's software architecture. Each node should have a single, well-defined purpose.
For a humanoid robot, you might have nodes for:
- Reading sensor data from an IMU.
- Controlling the motors in the right leg.
- Processing camera images.
- Planning footsteps for walking.
Nodes are what allow ROS 2 systems to be modular and fault-tolerant. If one node crashes, the rest of the system can continue to run.
Example: A Simple rclpy
Node
Here is the "hello world" of rclpy
: a simple node that initializes itself, prints a message, and then shuts down.
# 'hello_node.py'
import rclpy
from rclpy.node import Node
def main(args=None):
# 1. Initialize the rclpy library
rclpy.init(args=args)
# 2. Create a Node
# The node is named 'hello_ros_node'
node = Node('hello_ros_node')
# 3. From this point, the node is running.
# You can add your logic here.
node.get_logger().info('Hello, ROS 2 World!')
# 4. The node is destroyed when the 'with' statement exits.
# This is important for cleanup.
node.destroy_node()
# 5. Shutdown the rclpy library
rclpy.shutdown()
if __name__ == '__main__':
main()
To run this code:
- Save it as
hello_node.py
. - Make sure you have a ROS 2 environment sourced.
- Run
python hello_node.py
.
2. ROS 2 Topics (Publish/Subscribe)
Topics are the primary mechanism for one-way, asynchronous communication in ROS 2. Nodes can publish messages to a topic, and other nodes can subscribe to that topic to receive those messages. This is a decoupled communication model; publishers and subscribers don't need to know about each other.
- Use Case: Continuously streaming data, like sensor readings or motor commands.
Diagram: Publisher-Subscriber Model
graph TD
A[Publisher Node] -- Publishes message --> B(Topic: /sensor_data);
B -- Message is delivered --> C[Subscriber Node 1];
B -- Message is delivered --> D[Subscriber Node 2];
Example: A Simple Publisher and Subscriber
First, we define a publisher node that sends a simple string message every second.
# 'publisher_node.py'
import rclpy
from rclpy.node import Node
from std_msgs.msg import String
class SimplePublisher(Node):
def __init__(self):
super().__init__('simple_publisher')
# Create a publisher on the 'chatter' topic
self.publisher_ = self.create_publisher(String, 'chatter', 10)
self.timer = self.create_timer(1.0, self.timer_callback)
self.i = 0
def timer_callback(self):
msg = String()
msg.data = f'Hello from publisher: {self.i}'
self.publisher_.publish(msg)
self.get_logger().info(f'Publishing: "{msg.data}"')
self.i += 1
def main(args=None):
rclpy.init(args=args)
publisher = SimplePublisher()
rclpy.spin(publisher) # Keep the node alive
publisher.destroy_node()
rclpy.shutdown()
if __name__ == '__main__':
main()
Next, a subscriber node that listens to the chatter
topic.
# 'subscriber_node.py'
import rclpy
from rclpy.node import Node
from std_msgs.msg import String
class SimpleSubscriber(Node):
def __init__(self):
super().__init__('simple_subscriber')
# Create a subscription to the 'chatter' topic
self.subscription = self.create_subscription(
String,
'chatter',
self.listener_callback,
10)
self.subscription # prevent unused variable warning
def listener_callback(self, msg):
self.get_logger().info(f'I heard: "{msg.data}"')
def main(args=None):
rclpy.init(args=args)
subscriber = SimpleSubscriber()
rclpy.spin(subscriber)
subscriber.destroy_node()
rclpy.shutdown()
if __name__ == '__main__':
main()
To run this example:
- Open two terminals with your ROS 2 environment sourced.
- In the first terminal, run
python publisher_node.py
. - In the second terminal, run
python subscriber_node.py
. You will see the messages from the publisher being received.
3. ROS 2 Services (Request/Response)
Services are for two-way, synchronous communication. A Service Server node provides a service, and a Service Client node can send a request and wait for a response. This is a tightly coupled, blocking communication model.
- Use Case: Triggering a specific action that has a clear start and end, like "open gripper" or "calculate inverse kinematics".
Diagram: Service Client/Server Model
graph TD
A[Service Client Node] -- Sends Request --> B(Service: /add_two_ints);
B -- Delivers Request --> C[Service Server Node];
C -- Processes and sends Response --> B;
B -- Delivers Response --> A;
Example: A Simple Service Server and Client
First, the server node that offers a service to add two integers. ROS 2 has a standard service definition for this, example_interfaces/srv/AddTwoInts
.
# 'service_server_node.py'
import rclpy
from rclpy.node import Node
from example_interfaces.srv import AddTwoInts
class SimpleServiceServer(Node):
def __init__(self):
super().__init__('simple_service_server')
# Create a service named 'add_two_ints'
self.srv = self.create_service(AddTwoInts, 'add_two_ints', self.add_two_ints_callback)
def add_two_ints_callback(self, request, response):
response.sum = request.a + request.b
self.get_logger().info(f'Incoming request: a={request.a}, b={request.b}. Returning sum={response.sum}')
return response
def main(args=None):
rclpy.init(args=args)
server = SimpleServiceServer()
rclpy.spin(server)
server.destroy_node()
rclpy.shutdown()
if __name__ == '__main__':
main()
Next, the client node that calls the service.
# 'service_client_node.py'
import rclpy
from rclpy.node import Node
from example_interfaces.srv import AddTwoInts
import sys
class SimpleServiceClient(Node):
def __init__(self):
super().__init__('simple_service_client')
# Create a client for the 'add_two_ints' service
self.client = self.create_client(AddTwoInts, 'add_two_ints')
while not self.client.wait_for_service(timeout_sec=1.0):
self.get_logger().info('Service not available, waiting again...')
self.req = AddTwoInts.Request()
def send_request(self, a, b):
self.req.a = a
self.req.b = b
self.future = self.client.call_async(self.req)
rclpy.spin_until_future_complete(self, self.future)
return self.future.result()
def main(args=None):
rclpy.init(args=args)
if len(sys.argv) != 3:
print("Usage: python service_client_node.py <int1> <int2>")
return
client = SimpleServiceClient()
response = client.send_request(int(sys.argv[1]), int(sys.argv[2]))
if response:
client.get_logger().info(
f'Result of add_two_ints: for {sys.argv[1]} + {sys.argv[2]} = {response.sum}')
else:
client.get_logger().error('Service call failed')
client.destroy_node()
rclpy.shutdown()
if __name__ == '__main__':
main()
To run this example:
- Open two terminals with your ROS 2 environment sourced.
- In the first, run
python service_server_node.py
. - In the second, run
python service_client_node.py 5 10
. The client will send the request and print the response.
--------------------------------------------------------------------------------
URL: https://Muham251.github.io/ai-hackathon-1/docs/module-1/nodes-topics-services-rclpy#2-ros-2-topics-publishsubscribe
Content:
ROS 2 Fundamentals & Python
This chapter covers the three fundamental communication concepts in ROS 2: Nodes, Topics, and Services. We'll also see how to implement them using rclpy
, the official Python client library for ROS 2.
1. ROS 2 Nodes
A Node is the smallest, most fundamental unit of a ROS 2 system. Think of a node as a single, independent process in your robot's software architecture. Each node should have a single, well-defined purpose.
For a humanoid robot, you might have nodes for:
- Reading sensor data from an IMU.
- Controlling the motors in the right leg.
- Processing camera images.
- Planning footsteps for walking.
Nodes are what allow ROS 2 systems to be modular and fault-tolerant. If one node crashes, the rest of the system can continue to run.
Example: A Simple rclpy
Node
Here is the "hello world" of rclpy
: a simple node that initializes itself, prints a message, and then shuts down.
# 'hello_node.py'
import rclpy
from rclpy.node import Node
def main(args=None):
# 1. Initialize the rclpy library
rclpy.init(args=args)
# 2. Create a Node
# The node is named 'hello_ros_node'
node = Node('hello_ros_node')
# 3. From this point, the node is running.
# You can add your logic here.
node.get_logger().info('Hello, ROS 2 World!')
# 4. The node is destroyed when the 'with' statement exits.
# This is important for cleanup.
node.destroy_node()
# 5. Shutdown the rclpy library
rclpy.shutdown()
if __name__ == '__main__':
main()
To run this code:
- Save it as
hello_node.py
. - Make sure you have a ROS 2 environment sourced.
- Run
python hello_node.py
.
2. ROS 2 Topics (Publish/Subscribe)
Topics are the primary mechanism for one-way, asynchronous communication in ROS 2. Nodes can publish messages to a topic, and other nodes can subscribe to that topic to receive those messages. This is a decoupled communication model; publishers and subscribers don't need to know about each other.
- Use Case: Continuously streaming data, like sensor readings or motor commands.
Diagram: Publisher-Subscriber Model
graph TD
A[Publisher Node] -- Publishes message --> B(Topic: /sensor_data);
B -- Message is delivered --> C[Subscriber Node 1];
B -- Message is delivered --> D[Subscriber Node 2];
Example: A Simple Publisher and Subscriber
First, we define a publisher node that sends a simple string message every second.
# 'publisher_node.py'
import rclpy
from rclpy.node import Node
from std_msgs.msg import String
class SimplePublisher(Node):
def __init__(self):
super().__init__('simple_publisher')
# Create a publisher on the 'chatter' topic
self.publisher_ = self.create_publisher(String, 'chatter', 10)
self.timer = self.create_timer(1.0, self.timer_callback)
self.i = 0
def timer_callback(self):
msg = String()
msg.data = f'Hello from publisher: {self.i}'
self.publisher_.publish(msg)
self.get_logger().info(f'Publishing: "{msg.data}"')
self.i += 1
def main(args=None):
rclpy.init(args=args)
publisher = SimplePublisher()
rclpy.spin(publisher) # Keep the node alive
publisher.destroy_node()
rclpy.shutdown()
if __name__ == '__main__':
main()
Next, a subscriber node that listens to the chatter
topic.
# 'subscriber_node.py'
import rclpy
from rclpy.node import Node
from std_msgs.msg import String
class SimpleSubscriber(Node):
def __init__(self):
super().__init__('simple_subscriber')
# Create a subscription to the 'chatter' topic
self.subscription = self.create_subscription(
String,
'chatter',
self.listener_callback,
10)
self.subscription # prevent unused variable warning
def listener_callback(self, msg):
self.get_logger().info(f'I heard: "{msg.data}"')
def main(args=None):
rclpy.init(args=args)
subscriber = SimpleSubscriber()
rclpy.spin(subscriber)
subscriber.destroy_node()
rclpy.shutdown()
if __name__ == '__main__':
main()
To run this example:
- Open two terminals with your ROS 2 environment sourced.
- In the first terminal, run
python publisher_node.py
. - In the second terminal, run
python subscriber_node.py
. You will see the messages from the publisher being received.
3. ROS 2 Services (Request/Response)
Services are for two-way, synchronous communication. A Service Server node provides a service, and a Service Client node can send a request and wait for a response. This is a tightly coupled, blocking communication model.
- Use Case: Triggering a specific action that has a clear start and end, like "open gripper" or "calculate inverse kinematics".
Diagram: Service Client/Server Model
graph TD
A[Service Client Node] -- Sends Request --> B(Service: /add_two_ints);
B -- Delivers Request --> C[Service Server Node];
C -- Processes and sends Response --> B;
B -- Delivers Response --> A;
Example: A Simple Service Server and Client
First, the server node that offers a service to add two integers. ROS 2 has a standard service definition for this, example_interfaces/srv/AddTwoInts
.
# 'service_server_node.py'
import rclpy
from rclpy.node import Node
from example_interfaces.srv import AddTwoInts
class SimpleServiceServer(Node):
def __init__(self):
super().__init__('simple_service_server')
# Create a service named 'add_two_ints'
self.srv = self.create_service(AddTwoInts, 'add_two_ints', self.add_two_ints_callback)
def add_two_ints_callback(self, request, response):
response.sum = request.a + request.b
self.get_logger().info(f'Incoming request: a={request.a}, b={request.b}. Returning sum={response.sum}')
return response
def main(args=None):
rclpy.init(args=args)
server = SimpleServiceServer()
rclpy.spin(server)
server.destroy_node()
rclpy.shutdown()
if __name__ == '__main__':
main()
Next, the client node that calls the service.
# 'service_client_node.py'
import rclpy
from rclpy.node import Node
from example_interfaces.srv import AddTwoInts
import sys
class SimpleServiceClient(Node):
def __init__(self):
super().__init__('simple_service_client')
# Create a client for the 'add_two_ints' service
self.client = self.create_client(AddTwoInts, 'add_two_ints')
while not self.client.wait_for_service(timeout_sec=1.0):
self.get_logger().info('Service not available, waiting again...')
self.req = AddTwoInts.Request()
def send_request(self, a, b):
self.req.a = a
self.req.b = b
self.future = self.client.call_async(self.req)
rclpy.spin_until_future_complete(self, self.future)
return self.future.result()
def main(args=None):
rclpy.init(args=args)
if len(sys.argv) != 3:
print("Usage: python service_client_node.py <int1> <int2>")
return
client = SimpleServiceClient()
response = client.send_request(int(sys.argv[1]), int(sys.argv[2]))
if response:
client.get_logger().info(
f'Result of add_two_ints: for {sys.argv[1]} + {sys.argv[2]} = {response.sum}')
else:
client.get_logger().error('Service call failed')
client.destroy_node()
rclpy.shutdown()
if __name__ == '__main__':
main()
To run this example:
- Open two terminals with your ROS 2 environment sourced.
- In the first, run
python service_server_node.py
. - In the second, run
python service_client_node.py 5 10
. The client will send the request and print the response.
--------------------------------------------------------------------------------
URL: https://Muham251.github.io/ai-hackathon-1/docs/module-1/nodes-topics-services-rclpy#diagram-publisher-subscriber-model
Content:
ROS 2 Fundamentals & Python
This chapter covers the three fundamental communication concepts in ROS 2: Nodes, Topics, and Services. We'll also see how to implement them using rclpy
, the official Python client library for ROS 2.
1. ROS 2 Nodes
A Node is the smallest, most fundamental unit of a ROS 2 system. Think of a node as a single, independent process in your robot's software architecture. Each node should have a single, well-defined purpose.
For a humanoid robot, you might have nodes for:
- Reading sensor data from an IMU.
- Controlling the motors in the right leg.
- Processing camera images.
- Planning footsteps for walking.
Nodes are what allow ROS 2 systems to be modular and fault-tolerant. If one node crashes, the rest of the system can continue to run.
Example: A Simple rclpy
Node
Here is the "hello world" of rclpy
: a simple node that initializes itself, prints a message, and then shuts down.
# 'hello_node.py'
import rclpy
from rclpy.node import Node
def main(args=None):
# 1. Initialize the rclpy library
rclpy.init(args=args)
# 2. Create a Node
# The node is named 'hello_ros_node'
node = Node('hello_ros_node')
# 3. From this point, the node is running.
# You can add your logic here.
node.get_logger().info('Hello, ROS 2 World!')
# 4. The node is destroyed when the 'with' statement exits.
# This is important for cleanup.
node.destroy_node()
# 5. Shutdown the rclpy library
rclpy.shutdown()
if __name__ == '__main__':
main()
To run this code:
- Save it as
hello_node.py
. - Make sure you have a ROS 2 environment sourced.
- Run
python hello_node.py
.
2. ROS 2 Topics (Publish/Subscribe)
Topics are the primary mechanism for one-way, asynchronous communication in ROS 2. Nodes can publish messages to a topic, and other nodes can subscribe to that topic to receive those messages. This is a decoupled communication model; publishers and subscribers don't need to know about each other.
- Use Case: Continuously streaming data, like sensor readings or motor commands.
Diagram: Publisher-Subscriber Model
graph TD
A[Publisher Node] -- Publishes message --> B(Topic: /sensor_data);
B -- Message is delivered --> C[Subscriber Node 1];
B -- Message is delivered --> D[Subscriber Node 2];
Example: A Simple Publisher and Subscriber
First, we define a publisher node that sends a simple string message every second.
# 'publisher_node.py'
import rclpy
from rclpy.node import Node
from std_msgs.msg import String
class SimplePublisher(Node):
def __init__(self):
super().__init__('simple_publisher')
# Create a publisher on the 'chatter' topic
self.publisher_ = self.create_publisher(String, 'chatter', 10)
self.timer = self.create_timer(1.0, self.timer_callback)
self.i = 0
def timer_callback(self):
msg = String()
msg.data = f'Hello from publisher: {self.i}'
self.publisher_.publish(msg)
self.get_logger().info(f'Publishing: "{msg.data}"')
self.i += 1
def main(args=None):
rclpy.init(args=args)
publisher = SimplePublisher()
rclpy.spin(publisher) # Keep the node alive
publisher.destroy_node()
rclpy.shutdown()
if __name__ == '__main__':
main()
Next, a subscriber node that listens to the chatter
topic.
# 'subscriber_node.py'
import rclpy
from rclpy.node import Node
from std_msgs.msg import String
class SimpleSubscriber(Node):
def __init__(self):
super().__init__('simple_subscriber')
# Create a subscription to the 'chatter' topic
self.subscription = self.create_subscription(
String,
'chatter',
self.listener_callback,
10)
self.subscription # prevent unused variable warning
def listener_callback(self, msg):
self.get_logger().info(f'I heard: "{msg.data}"')
def main(args=None):
rclpy.init(args=args)
subscriber = SimpleSubscriber()
rclpy.spin(subscriber)
subscriber.destroy_node()
rclpy.shutdown()
if __name__ == '__main__':
main()
To run this example:
- Open two terminals with your ROS 2 environment sourced.
- In the first terminal, run
python publisher_node.py
. - In the second terminal, run
python subscriber_node.py
. You will see the messages from the publisher being received.
3. ROS 2 Services (Request/Response)
Services are for two-way, synchronous communication. A Service Server node provides a service, and a Service Client node can send a request and wait for a response. This is a tightly coupled, blocking communication model.
- Use Case: Triggering a specific action that has a clear start and end, like "open gripper" or "calculate inverse kinematics".
Diagram: Service Client/Server Model
graph TD
A[Service Client Node] -- Sends Request --> B(Service: /add_two_ints);
B -- Delivers Request --> C[Service Server Node];
C -- Processes and sends Response --> B;
B -- Delivers Response --> A;
Example: A Simple Service Server and Client
First, the server node that offers a service to add two integers. ROS 2 has a standard service definition for this, example_interfaces/srv/AddTwoInts
.
# 'service_server_node.py'
import rclpy
from rclpy.node import Node
from example_interfaces.srv import AddTwoInts
class SimpleServiceServer(Node):
def __init__(self):
super().__init__('simple_service_server')
# Create a service named 'add_two_ints'
self.srv = self.create_service(AddTwoInts, 'add_two_ints', self.add_two_ints_callback)
def add_two_ints_callback(self, request, response):
response.sum = request.a + request.b
self.get_logger().info(f'Incoming request: a={request.a}, b={request.b}. Returning sum={response.sum}')
return response
def main(args=None):
rclpy.init(args=args)
server = SimpleServiceServer()
rclpy.spin(server)
server.destroy_node()
rclpy.shutdown()
if __name__ == '__main__':
main()
Next, the client node that calls the service.
# 'service_client_node.py'
import rclpy
from rclpy.node import Node
from example_interfaces.srv import AddTwoInts
import sys
class SimpleServiceClient(Node):
def __init__(self):
super().__init__('simple_service_client')
# Create a client for the 'add_two_ints' service
self.client = self.create_client(AddTwoInts, 'add_two_ints')
while not self.client.wait_for_service(timeout_sec=1.0):
self.get_logger().info('Service not available, waiting again...')
self.req = AddTwoInts.Request()
def send_request(self, a, b):
self.req.a = a
self.req.b = b
self.future = self.client.call_async(self.req)
rclpy.spin_until_future_complete(self, self.future)
return self.future.result()
def main(args=None):
rclpy.init(args=args)
if len(sys.argv) != 3:
print("Usage: python service_client_node.py <int1> <int2>")
return
client = SimpleServiceClient()
response = client.send_request(int(sys.argv[1]), int(sys.argv[2]))
if response:
client.get_logger().info(
f'Result of add_two_ints: for {sys.argv[1]} + {sys.argv[2]} = {response.sum}')
else:
client.get_logger().error('Service call failed')
client.destroy_node()
rclpy.shutdown()
if __name__ == '__main__':
main()
To run this example:
- Open two terminals with your ROS 2 environment sourced.
- In the first, run
python service_server_node.py
. - In the second, run
python service_client_node.py 5 10
. The client will send the request and print the response.
--------------------------------------------------------------------------------
URL: https://Muham251.github.io/ai-hackathon-1/docs/module-1/nodes-topics-services-rclpy#example-a-simple-publisher-and-subscriber
Content:
ROS 2 Fundamentals & Python
This chapter covers the three fundamental communication concepts in ROS 2: Nodes, Topics, and Services. We'll also see how to implement them using rclpy
, the official Python client library for ROS 2.
1. ROS 2 Nodes
A Node is the smallest, most fundamental unit of a ROS 2 system. Think of a node as a single, independent process in your robot's software architecture. Each node should have a single, well-defined purpose.
For a humanoid robot, you might have nodes for:
- Reading sensor data from an IMU.
- Controlling the motors in the right leg.
- Processing camera images.
- Planning footsteps for walking.
Nodes are what allow ROS 2 systems to be modular and fault-tolerant. If one node crashes, the rest of the system can continue to run.
Example: A Simple rclpy
Node
Here is the "hello world" of rclpy
: a simple node that initializes itself, prints a message, and then shuts down.
# 'hello_node.py'
import rclpy
from rclpy.node import Node
def main(args=None):
# 1. Initialize the rclpy library
rclpy.init(args=args)
# 2. Create a Node
# The node is named 'hello_ros_node'
node = Node('hello_ros_node')
# 3. From this point, the node is running.
# You can add your logic here.
node.get_logger().info('Hello, ROS 2 World!')
# 4. The node is destroyed when the 'with' statement exits.
# This is important for cleanup.
node.destroy_node()
# 5. Shutdown the rclpy library
rclpy.shutdown()
if __name__ == '__main__':
main()
To run this code:
- Save it as
hello_node.py
. - Make sure you have a ROS 2 environment sourced.
- Run
python hello_node.py
.
2. ROS 2 Topics (Publish/Subscribe)
Topics are the primary mechanism for one-way, asynchronous communication in ROS 2. Nodes can publish messages to a topic, and other nodes can subscribe to that topic to receive those messages. This is a decoupled communication model; publishers and subscribers don't need to know about each other.
- Use Case: Continuously streaming data, like sensor readings or motor commands.
Diagram: Publisher-Subscriber Model
graph TD
A[Publisher Node] -- Publishes message --> B(Topic: /sensor_data);
B -- Message is delivered --> C[Subscriber Node 1];
B -- Message is delivered --> D[Subscriber Node 2];
Example: A Simple Publisher and Subscriber
First, we define a publisher node that sends a simple string message every second.
# 'publisher_node.py'
import rclpy
from rclpy.node import Node
from std_msgs.msg import String
class SimplePublisher(Node):
def __init__(self):
super().__init__('simple_publisher')
# Create a publisher on the 'chatter' topic
self.publisher_ = self.create_publisher(String, 'chatter', 10)
self.timer = self.create_timer(1.0, self.timer_callback)
self.i = 0
def timer_callback(self):
msg = String()
msg.data = f'Hello from publisher: {self.i}'
self.publisher_.publish(msg)
self.get_logger().info(f'Publishing: "{msg.data}"')
self.i += 1
def main(args=None):
rclpy.init(args=args)
publisher = SimplePublisher()
rclpy.spin(publisher) # Keep the node alive
publisher.destroy_node()
rclpy.shutdown()
if __name__ == '__main__':
main()
Next, a subscriber node that listens to the chatter
topic.
# 'subscriber_node.py'
import rclpy
from rclpy.node import Node
from std_msgs.msg import String
class SimpleSubscriber(Node):
def __init__(self):
super().__init__('simple_subscriber')
# Create a subscription to the 'chatter' topic
self.subscription = self.create_subscription(
String,
'chatter',
self.listener_callback,
10)
self.subscription # prevent unused variable warning
def listener_callback(self, msg):
self.get_logger().info(f'I heard: "{msg.data}"')
def main(args=None):
rclpy.init(args=args)
subscriber = SimpleSubscriber()
rclpy.spin(subscriber)
subscriber.destroy_node()
rclpy.shutdown()
if __name__ == '__main__':
main()
To run this example:
- Open two terminals with your ROS 2 environment sourced.
- In the first terminal, run
python publisher_node.py
. - In the second terminal, run
python subscriber_node.py
. You will see the messages from the publisher being received.
3. ROS 2 Services (Request/Response)
Services are for two-way, synchronous communication. A Service Server node provides a service, and a Service Client node can send a request and wait for a response. This is a tightly coupled, blocking communication model.
- Use Case: Triggering a specific action that has a clear start and end, like "open gripper" or "calculate inverse kinematics".
Diagram: Service Client/Server Model
graph TD
A[Service Client Node] -- Sends Request --> B(Service: /add_two_ints);
B -- Delivers Request --> C[Service Server Node];
C -- Processes and sends Response --> B;
B -- Delivers Response --> A;
Example: A Simple Service Server and Client
First, the server node that offers a service to add two integers. ROS 2 has a standard service definition for this, example_interfaces/srv/AddTwoInts
.
# 'service_server_node.py'
import rclpy
from rclpy.node import Node
from example_interfaces.srv import AddTwoInts
class SimpleServiceServer(Node):
def __init__(self):
super().__init__('simple_service_server')
# Create a service named 'add_two_ints'
self.srv = self.create_service(AddTwoInts, 'add_two_ints', self.add_two_ints_callback)
def add_two_ints_callback(self, request, response):
response.sum = request.a + request.b
self.get_logger().info(f'Incoming request: a={request.a}, b={request.b}. Returning sum={response.sum}')
return response
def main(args=None):
rclpy.init(args=args)
server = SimpleServiceServer()
rclpy.spin(server)
server.destroy_node()
rclpy.shutdown()
if __name__ == '__main__':
main()
Next, the client node that calls the service.
# 'service_client_node.py'
import rclpy
from rclpy.node import Node
from example_interfaces.srv import AddTwoInts
import sys
class SimpleServiceClient(Node):
def __init__(self):
super().__init__('simple_service_client')
# Create a client for the 'add_two_ints' service
self.client = self.create_client(AddTwoInts, 'add_two_ints')
while not self.client.wait_for_service(timeout_sec=1.0):
self.get_logger().info('Service not available, waiting again...')
self.req = AddTwoInts.Request()
def send_request(self, a, b):
self.req.a = a
self.req.b = b
self.future = self.client.call_async(self.req)
rclpy.spin_until_future_complete(self, self.future)
return self.future.result()
def main(args=None):
rclpy.init(args=args)
if len(sys.argv) != 3:
print("Usage: python service_client_node.py <int1> <int2>")
return
client = SimpleServiceClient()
response = client.send_request(int(sys.argv[1]), int(sys.argv[2]))
if response:
client.get_logger().info(
f'Result of add_two_ints: for {sys.argv[1]} + {sys.argv[2]} = {response.sum}')
else:
client.get_logger().error('Service call failed')
client.destroy_node()
rclpy.shutdown()
if __name__ == '__main__':
main()
To run this example:
- Open two terminals with your ROS 2 environment sourced.
- In the first, run
python service_server_node.py
. - In the second, run
python service_client_node.py 5 10
. The client will send the request and print the response.
--------------------------------------------------------------------------------
URL: https://Muham251.github.io/ai-hackathon-1/docs/module-1/nodes-topics-services-rclpy#3-ros-2-services-requestresponse
Content:
ROS 2 Fundamentals & Python
This chapter covers the three fundamental communication concepts in ROS 2: Nodes, Topics, and Services. We'll also see how to implement them using rclpy
, the official Python client library for ROS 2.
1. ROS 2 Nodes
A Node is the smallest, most fundamental unit of a ROS 2 system. Think of a node as a single, independent process in your robot's software architecture. Each node should have a single, well-defined purpose.
For a humanoid robot, you might have nodes for:
- Reading sensor data from an IMU.
- Controlling the motors in the right leg.
- Processing camera images.
- Planning footsteps for walking.
Nodes are what allow ROS 2 systems to be modular and fault-tolerant. If one node crashes, the rest of the system can continue to run.
Example: A Simple rclpy
Node
Here is the "hello world" of rclpy
: a simple node that initializes itself, prints a message, and then shuts down.
# 'hello_node.py'
import rclpy
from rclpy.node import Node
def main(args=None):
# 1. Initialize the rclpy library
rclpy.init(args=args)
# 2. Create a Node
# The node is named 'hello_ros_node'
node = Node('hello_ros_node')
# 3. From this point, the node is running.
# You can add your logic here.
node.get_logger().info('Hello, ROS 2 World!')
# 4. The node is destroyed when the 'with' statement exits.
# This is important for cleanup.
node.destroy_node()
# 5. Shutdown the rclpy library
rclpy.shutdown()
if __name__ == '__main__':
main()
To run this code:
- Save it as
hello_node.py
. - Make sure you have a ROS 2 environment sourced.
- Run
python hello_node.py
.
2. ROS 2 Topics (Publish/Subscribe)
Topics are the primary mechanism for one-way, asynchronous communication in ROS 2. Nodes can publish messages to a topic, and other nodes can subscribe to that topic to receive those messages. This is a decoupled communication model; publishers and subscribers don't need to know about each other.
- Use Case: Continuously streaming data, like sensor readings or motor commands.
Diagram: Publisher-Subscriber Model
graph TD
A[Publisher Node] -- Publishes message --> B(Topic: /sensor_data);
B -- Message is delivered --> C[Subscriber Node 1];
B -- Message is delivered --> D[Subscriber Node 2];
Example: A Simple Publisher and Subscriber
First, we define a publisher node that sends a simple string message every second.
# 'publisher_node.py'
import rclpy
from rclpy.node import Node
from std_msgs.msg import String
class SimplePublisher(Node):
def __init__(self):
super().__init__('simple_publisher')
# Create a publisher on the 'chatter' topic
self.publisher_ = self.create_publisher(String, 'chatter', 10)
self.timer = self.create_timer(1.0, self.timer_callback)
self.i = 0
def timer_callback(self):
msg = String()
msg.data = f'Hello from publisher: {self.i}'
self.publisher_.publish(msg)
self.get_logger().info(f'Publishing: "{msg.data}"')
self.i += 1
def main(args=None):
rclpy.init(args=args)
publisher = SimplePublisher()
rclpy.spin(publisher) # Keep the node alive
publisher.destroy_node()
rclpy.shutdown()
if __name__ == '__main__':
main()
Next, a subscriber node that listens to the chatter
topic.
# 'subscriber_node.py'
import rclpy
from rclpy.node import Node
from std_msgs.msg import String
class SimpleSubscriber(Node):
def __init__(self):
super().__init__('simple_subscriber')
# Create a subscription to the 'chatter' topic
self.subscription = self.create_subscription(
String,
'chatter',
self.listener_callback,
10)
self.subscription # prevent unused variable warning
def listener_callback(self, msg):
self.get_logger().info(f'I heard: "{msg.data}"')
def main(args=None):
rclpy.init(args=args)
subscriber = SimpleSubscriber()
rclpy.spin(subscriber)
subscriber.destroy_node()
rclpy.shutdown()
if __name__ == '__main__':
main()
To run this example:
- Open two terminals with your ROS 2 environment sourced.
- In the first terminal, run
python publisher_node.py
. - In the second terminal, run
python subscriber_node.py
. You will see the messages from the publisher being received.
3. ROS 2 Services (Request/Response)
Services are for two-way, synchronous communication. A Service Server node provides a service, and a Service Client node can send a request and wait for a response. This is a tightly coupled, blocking communication model.
- Use Case: Triggering a specific action that has a clear start and end, like "open gripper" or "calculate inverse kinematics".
Diagram: Service Client/Server Model
graph TD
A[Service Client Node] -- Sends Request --> B(Service: /add_two_ints);
B -- Delivers Request --> C[Service Server Node];
C -- Processes and sends Response --> B;
B -- Delivers Response --> A;
Example: A Simple Service Server and Client
First, the server node that offers a service to add two integers. ROS 2 has a standard service definition for this, example_interfaces/srv/AddTwoInts
.
# 'service_server_node.py'
import rclpy
from rclpy.node import Node
from example_interfaces.srv import AddTwoInts
class SimpleServiceServer(Node):
def __init__(self):
super().__init__('simple_service_server')
# Create a service named 'add_two_ints'
self.srv = self.create_service(AddTwoInts, 'add_two_ints', self.add_two_ints_callback)
def add_two_ints_callback(self, request, response):
response.sum = request.a + request.b
self.get_logger().info(f'Incoming request: a={request.a}, b={request.b}. Returning sum={response.sum}')
return response
def main(args=None):
rclpy.init(args=args)
server = SimpleServiceServer()
rclpy.spin(server)
server.destroy_node()
rclpy.shutdown()
if __name__ == '__main__':
main()
Next, the client node that calls the service.
# 'service_client_node.py'
import rclpy
from rclpy.node import Node
from example_interfaces.srv import AddTwoInts
import sys
class SimpleServiceClient(Node):
def __init__(self):
super().__init__('simple_service_client')
# Create a client for the 'add_two_ints' service
self.client = self.create_client(AddTwoInts, 'add_two_ints')
while not self.client.wait_for_service(timeout_sec=1.0):
self.get_logger().info('Service not available, waiting again...')
self.req = AddTwoInts.Request()
def send_request(self, a, b):
self.req.a = a
self.req.b = b
self.future = self.client.call_async(self.req)
rclpy.spin_until_future_complete(self, self.future)
return self.future.result()
def main(args=None):
rclpy.init(args=args)
if len(sys.argv) != 3:
print("Usage: python service_client_node.py <int1> <int2>")
return
client = SimpleServiceClient()
response = client.send_request(int(sys.argv[1]), int(sys.argv[2]))
if response:
client.get_logger().info(
f'Result of add_two_ints: for {sys.argv[1]} + {sys.argv[2]} = {response.sum}')
else:
client.get_logger().error('Service call failed')
client.destroy_node()
rclpy.shutdown()
if __name__ == '__main__':
main()
To run this example:
- Open two terminals with your ROS 2 environment sourced.
- In the first, run
python service_server_node.py
. - In the second, run
python service_client_node.py 5 10
. The client will send the request and print the response.
--------------------------------------------------------------------------------
URL: https://Muham251.github.io/ai-hackathon-1/docs/module-1/nodes-topics-services-rclpy#diagram-service-clientserver-model
Content:
ROS 2 Fundamentals & Python
This chapter covers the three fundamental communication concepts in ROS 2: Nodes, Topics, and Services. We'll also see how to implement them using rclpy
, the official Python client library for ROS 2.
1. ROS 2 Nodes
A Node is the smallest, most fundamental unit of a ROS 2 system. Think of a node as a single, independent process in your robot's software architecture. Each node should have a single, well-defined purpose.
For a humanoid robot, you might have nodes for:
- Reading sensor data from an IMU.
- Controlling the motors in the right leg.
- Processing camera images.
- Planning footsteps for walking.
Nodes are what allow ROS 2 systems to be modular and fault-tolerant. If one node crashes, the rest of the system can continue to run.
Example: A Simple rclpy
Node
Here is the "hello world" of rclpy
: a simple node that initializes itself, prints a message, and then shuts down.
# 'hello_node.py'
import rclpy
from rclpy.node import Node
def main(args=None):
# 1. Initialize the rclpy library
rclpy.init(args=args)
# 2. Create a Node
# The node is named 'hello_ros_node'
node = Node('hello_ros_node')
# 3. From this point, the node is running.
# You can add your logic here.
node.get_logger().info('Hello, ROS 2 World!')
# 4. The node is destroyed when the 'with' statement exits.
# This is important for cleanup.
node.destroy_node()
# 5. Shutdown the rclpy library
rclpy.shutdown()
if __name__ == '__main__':
main()
To run this code:
- Save it as
hello_node.py
. - Make sure you have a ROS 2 environment sourced.
- Run
python hello_node.py
.
2. ROS 2 Topics (Publish/Subscribe)
Topics are the primary mechanism for one-way, asynchronous communication in ROS 2. Nodes can publish messages to a topic, and other nodes can subscribe to that topic to receive those messages. This is a decoupled communication model; publishers and subscribers don't need to know about each other.
- Use Case: Continuously streaming data, like sensor readings or motor commands.
Diagram: Publisher-Subscriber Model
graph TD
A[Publisher Node] -- Publishes message --> B(Topic: /sensor_data);
B -- Message is delivered --> C[Subscriber Node 1];
B -- Message is delivered --> D[Subscriber Node 2];
Example: A Simple Publisher and Subscriber
First, we define a publisher node that sends a simple string message every second.
# 'publisher_node.py'
import rclpy
from rclpy.node import Node
from std_msgs.msg import String
class SimplePublisher(Node):
def __init__(self):
super().__init__('simple_publisher')
# Create a publisher on the 'chatter' topic
self.publisher_ = self.create_publisher(String, 'chatter', 10)
self.timer = self.create_timer(1.0, self.timer_callback)
self.i = 0
def timer_callback(self):
msg = String()
msg.data = f'Hello from publisher: {self.i}'
self.publisher_.publish(msg)
self.get_logger().info(f'Publishing: "{msg.data}"')
self.i += 1
def main(args=None):
rclpy.init(args=args)
publisher = SimplePublisher()
rclpy.spin(publisher) # Keep the node alive
publisher.destroy_node()
rclpy.shutdown()
if __name__ == '__main__':
main()
Next, a subscriber node that listens to the chatter
topic.
# 'subscriber_node.py'
import rclpy
from rclpy.node import Node
from std_msgs.msg import String
class SimpleSubscriber(Node):
def __init__(self):
super().__init__('simple_subscriber')
# Create a subscription to the 'chatter' topic
self.subscription = self.create_subscription(
String,
'chatter',
self.listener_callback,
10)
self.subscription # prevent unused variable warning
def listener_callback(self, msg):
self.get_logger().info(f'I heard: "{msg.data}"')
def main(args=None):
rclpy.init(args=args)
subscriber = SimpleSubscriber()
rclpy.spin(subscriber)
subscriber.destroy_node()
rclpy.shutdown()
if __name__ == '__main__':
main()
To run this example:
- Open two terminals with your ROS 2 environment sourced.
- In the first terminal, run
python publisher_node.py
. - In the second terminal, run
python subscriber_node.py
. You will see the messages from the publisher being received.
3. ROS 2 Services (Request/Response)
Services are for two-way, synchronous communication. A Service Server node provides a service, and a Service Client node can send a request and wait for a response. This is a tightly coupled, blocking communication model.
- Use Case: Triggering a specific action that has a clear start and end, like "open gripper" or "calculate inverse kinematics".
Diagram: Service Client/Server Model
graph TD
A[Service Client Node] -- Sends Request --> B(Service: /add_two_ints);
B -- Delivers Request --> C[Service Server Node];
C -- Processes and sends Response --> B;
B -- Delivers Response --> A;
Example: A Simple Service Server and Client
First, the server node that offers a service to add two integers. ROS 2 has a standard service definition for this, example_interfaces/srv/AddTwoInts
.
# 'service_server_node.py'
import rclpy
from rclpy.node import Node
from example_interfaces.srv import AddTwoInts
class SimpleServiceServer(Node):
def __init__(self):
super().__init__('simple_service_server')
# Create a service named 'add_two_ints'
self.srv = self.create_service(AddTwoInts, 'add_two_ints', self.add_two_ints_callback)
def add_two_ints_callback(self, request, response):
response.sum = request.a + request.b
self.get_logger().info(f'Incoming request: a={request.a}, b={request.b}. Returning sum={response.sum}')
return response
def main(args=None):
rclpy.init(args=args)
server = SimpleServiceServer()
rclpy.spin(server)
server.destroy_node()
rclpy.shutdown()
if __name__ == '__main__':
main()
Next, the client node that calls the service.
# 'service_client_node.py'
import rclpy
from rclpy.node import Node
from example_interfaces.srv import AddTwoInts
import sys
class SimpleServiceClient(Node):
def __init__(self):
super().__init__('simple_service_client')
# Create a client for the 'add_two_ints' service
self.client = self.create_client(AddTwoInts, 'add_two_ints')
while not self.client.wait_for_service(timeout_sec=1.0):
self.get_logger().info('Service not available, waiting again...')
self.req = AddTwoInts.Request()
def send_request(self, a, b):
self.req.a = a
self.req.b = b
self.future = self.client.call_async(self.req)
rclpy.spin_until_future_complete(self, self.future)
return self.future.result()
def main(args=None):
rclpy.init(args=args)
if len(sys.argv) != 3:
print("Usage: python service_client_node.py <int1> <int2>")
return
client = SimpleServiceClient()
response = client.send_request(int(sys.argv[1]), int(sys.argv[2]))
if response:
client.get_logger().info(
f'Result of add_two_ints: for {sys.argv[1]} + {sys.argv[2]} = {response.sum}')
else:
client.get_logger().error('Service call failed')
client.destroy_node()
rclpy.shutdown()
if __name__ == '__main__':
main()
To run this example:
- Open two terminals with your ROS 2 environment sourced.
- In the first, run
python service_server_node.py
. - In the second, run
python service_client_node.py 5 10
. The client will send the request and print the response.
--------------------------------------------------------------------------------
URL: https://Muham251.github.io/ai-hackathon-1/docs/module-1/nodes-topics-services-rclpy#example-a-simple-service-server-and-client
Content:
ROS 2 Fundamentals & Python
This chapter covers the three fundamental communication concepts in ROS 2: Nodes, Topics, and Services. We'll also see how to implement them using rclpy
, the official Python client library for ROS 2.
1. ROS 2 Nodes
A Node is the smallest, most fundamental unit of a ROS 2 system. Think of a node as a single, independent process in your robot's software architecture. Each node should have a single, well-defined purpose.
For a humanoid robot, you might have nodes for:
- Reading sensor data from an IMU.
- Controlling the motors in the right leg.
- Processing camera images.
- Planning footsteps for walking.
Nodes are what allow ROS 2 systems to be modular and fault-tolerant. If one node crashes, the rest of the system can continue to run.
Example: A Simple rclpy
Node
Here is the "hello world" of rclpy
: a simple node that initializes itself, prints a message, and then shuts down.
# 'hello_node.py'
import rclpy
from rclpy.node import Node
def main(args=None):
# 1. Initialize the rclpy library
rclpy.init(args=args)
# 2. Create a Node
# The node is named 'hello_ros_node'
node = Node('hello_ros_node')
# 3. From this point, the node is running.
# You can add your logic here.
node.get_logger().info('Hello, ROS 2 World!')
# 4. The node is destroyed when the 'with' statement exits.
# This is important for cleanup.
node.destroy_node()
# 5. Shutdown the rclpy library
rclpy.shutdown()
if __name__ == '__main__':
main()
To run this code:
- Save it as
hello_node.py
. - Make sure you have a ROS 2 environment sourced.
- Run
python hello_node.py
.
2. ROS 2 Topics (Publish/Subscribe)
Topics are the primary mechanism for one-way, asynchronous communication in ROS 2. Nodes can publish messages to a topic, and other nodes can subscribe to that topic to receive those messages. This is a decoupled communication model; publishers and subscribers don't need to know about each other.
- Use Case: Continuously streaming data, like sensor readings or motor commands.
Diagram: Publisher-Subscriber Model
graph TD
A[Publisher Node] -- Publishes message --> B(Topic: /sensor_data);
B -- Message is delivered --> C[Subscriber Node 1];
B -- Message is delivered --> D[Subscriber Node 2];
Example: A Simple Publisher and Subscriber
First, we define a publisher node that sends a simple string message every second.
# 'publisher_node.py'
import rclpy
from rclpy.node import Node
from std_msgs.msg import String
class SimplePublisher(Node):
def __init__(self):
super().__init__('simple_publisher')
# Create a publisher on the 'chatter' topic
self.publisher_ = self.create_publisher(String, 'chatter', 10)
self.timer = self.create_timer(1.0, self.timer_callback)
self.i = 0
def timer_callback(self):
msg = String()
msg.data = f'Hello from publisher: {self.i}'
self.publisher_.publish(msg)
self.get_logger().info(f'Publishing: "{msg.data}"')
self.i += 1
def main(args=None):
rclpy.init(args=args)
publisher = SimplePublisher()
rclpy.spin(publisher) # Keep the node alive
publisher.destroy_node()
rclpy.shutdown()
if __name__ == '__main__':
main()
Next, a subscriber node that listens to the chatter
topic.
# 'subscriber_node.py'
import rclpy
from rclpy.node import Node
from std_msgs.msg import String
class SimpleSubscriber(Node):
def __init__(self):
super().__init__('simple_subscriber')
# Create a subscription to the 'chatter' topic
self.subscription = self.create_subscription(
String,
'chatter',
self.listener_callback,
10)
self.subscription # prevent unused variable warning
def listener_callback(self, msg):
self.get_logger().info(f'I heard: "{msg.data}"')
def main(args=None):
rclpy.init(args=args)
subscriber = SimpleSubscriber()
rclpy.spin(subscriber)
subscriber.destroy_node()
rclpy.shutdown()
if __name__ == '__main__':
main()
To run this example:
- Open two terminals with your ROS 2 environment sourced.
- In the first terminal, run
python publisher_node.py
. - In the second terminal, run
python subscriber_node.py
. You will see the messages from the publisher being received.
3. ROS 2 Services (Request/Response)
Services are for two-way, synchronous communication. A Service Server node provides a service, and a Service Client node can send a request and wait for a response. This is a tightly coupled, blocking communication model.
- Use Case: Triggering a specific action that has a clear start and end, like "open gripper" or "calculate inverse kinematics".
Diagram: Service Client/Server Model
graph TD
A[Service Client Node] -- Sends Request --> B(Service: /add_two_ints);
B -- Delivers Request --> C[Service Server Node];
C -- Processes and sends Response --> B;
B -- Delivers Response --> A;
Example: A Simple Service Server and Client
First, the server node that offers a service to add two integers. ROS 2 has a standard service definition for this, example_interfaces/srv/AddTwoInts
.
# 'service_server_node.py'
import rclpy
from rclpy.node import Node
from example_interfaces.srv import AddTwoInts
class SimpleServiceServer(Node):
def __init__(self):
super().__init__('simple_service_server')
# Create a service named 'add_two_ints'
self.srv = self.create_service(AddTwoInts, 'add_two_ints', self.add_two_ints_callback)
def add_two_ints_callback(self, request, response):
response.sum = request.a + request.b
self.get_logger().info(f'Incoming request: a={request.a}, b={request.b}. Returning sum={response.sum}')
return response
def main(args=None):
rclpy.init(args=args)
server = SimpleServiceServer()
rclpy.spin(server)
server.destroy_node()
rclpy.shutdown()
if __name__ == '__main__':
main()
Next, the client node that calls the service.
# 'service_client_node.py'
import rclpy
from rclpy.node import Node
from example_interfaces.srv import AddTwoInts
import sys
class SimpleServiceClient(Node):
def __init__(self):
super().__init__('simple_service_client')
# Create a client for the 'add_two_ints' service
self.client = self.create_client(AddTwoInts, 'add_two_ints')
while not self.client.wait_for_service(timeout_sec=1.0):
self.get_logger().info('Service not available, waiting again...')
self.req = AddTwoInts.Request()
def send_request(self, a, b):
self.req.a = a
self.req.b = b
self.future = self.client.call_async(self.req)
rclpy.spin_until_future_complete(self, self.future)
return self.future.result()
def main(args=None):
rclpy.init(args=args)
if len(sys.argv) != 3:
print("Usage: python service_client_node.py <int1> <int2>")
return
client = SimpleServiceClient()
response = client.send_request(int(sys.argv[1]), int(sys.argv[2]))
if response:
client.get_logger().info(
f'Result of add_two_ints: for {sys.argv[1]} + {sys.argv[2]} = {response.sum}')
else:
client.get_logger().error('Service call failed')
client.destroy_node()
rclpy.shutdown()
if __name__ == '__main__':
main()
To run this example:
- Open two terminals with your ROS 2 environment sourced.
- In the first, run
python service_server_node.py
. - In the second, run
python service_client_node.py 5 10
. The client will send the request and print the response.
--------------------------------------------------------------------------------
URL: https://Muham251.github.io/ai-hackathon-1/docs/module-1/#user-scenarios--testing-mandatory
Content:
Feature Specification: Module 1 ‚Äî The Robotic Nervous System (ROS 2)
Feature Branch: module-1
Created: December 7, 2025
Status: Draft
Input: User description: "Module: 1 ‚Äî The Robotic Nervous System (ROS 2)"
User Scenarios & Testing (mandatory)
User Story 1 - ROS 2 Fundamentals (Priority: P1)
As a beginner robotics student, I want to understand ROS 2 nodes and topics so I can run basic robot programs.
Why this priority: This is the foundational knowledge required to work with any ROS 2 system, and without understanding these core concepts, the learner cannot progress to more advanced topics.
Independent Test: Can be fully tested by successfully running a basic publisher/subscriber example and explaining the communication patterns.
Acceptance Scenarios:
- Given a basic understanding of programming concepts, When the user reads the ROS 2 fundamentals chapter, Then they can explain the roles of nodes and topics
- Given a working ROS 2 environment, When the user runs the provided publisher/subscriber example, Then they can observe the message passing between nodes
User Story 2 - Python Integration with rclpy (Priority: P2)
As an AI developer, I want to use rclpy to connect Python agents to robot controllers.
Why this priority: Python is the primary language for AI development, so connecting AI agents to ROS 2 systems is critical for the target audience.
Independent Test: Can be fully tested by successfully running a Python script that communicates with ROS 2 and demonstrates control concepts.
Acceptance Scenarios:
- Given a ROS 2 environment with rclpy installed, When the user runs the Python examples, Then they can see the interaction between Python code and robot controllers
User Story 3 - URDF for Humanoid Robots (Priority: P3)
As a humanoid robotics learner, I want to understand URDF so I can model a robot body.
Why this priority: Understanding robot modeling is essential for working with humanoid robots, which is the main focus of this curriculum.
Independent Test: Can be fully tested by successfully loading and modifying a basic humanoid URDF model.
Acceptance Scenarios:
- Given a basic URDF file, When the user modifies it following the tutorial, Then they can create a valid robot body model
Edge Cases
- ROS 2 distribution mismatch
- Missing dependencies during setup
- URDF syntax errors causing launch failures
- Python environment misconfiguration
Requirements (mandatory)
Functional Requirements
- FR-001: The module MUST explain ROS 2 nodes, topics, and services accurately.
- FR-002: The module MUST include executable rclpy examples.
- FR-003: The module MUST include a valid humanoid URDF example.
- FR-004: The module MUST avoid ROS 1 concepts.
Key Entities
- ROS 2 Nodes
- Topics and Services
- rclpy Python scripts
- URDF files (links, joints, hierarchy)
- Code blocks and CLI commands
- Diagrams (conceptual or ASCII)
Success Criteria (mandatory)
Measurable Outcomes
- SC-001: Users can successfully run a ROS 2 publisher/subscriber example.
- SC-002: Users can modify and execute a Python rclpy node.
- SC-003: Users can explain the role of nodes and topics.
- SC-004: Users can understand and edit a basic humanoid URDF model.
Out of Scope
- ROS 1
- Simulation tools (Gazebo / Isaac)
- Hardware deployment
Content Structure
- Chapter 1: ROS 2 Fundamentals for Humanoid Robots
- Chapter 2: Nodes, Topics, Services & rclpy Integration
- Chapter 3: URDF Basics for Humanoids
--------------------------------------------------------------------------------
URL: https://Muham251.github.io/ai-hackathon-1/docs/module-1/#user-story-1---ros-2-fundamentals-priority-p1
Content:
Feature Specification: Module 1 ‚Äî The Robotic Nervous System (ROS 2)
Feature Branch: module-1
Created: December 7, 2025
Status: Draft
Input: User description: "Module: 1 ‚Äî The Robotic Nervous System (ROS 2)"
User Scenarios & Testing (mandatory)
User Story 1 - ROS 2 Fundamentals (Priority: P1)
As a beginner robotics student, I want to understand ROS 2 nodes and topics so I can run basic robot programs.
Why this priority: This is the foundational knowledge required to work with any ROS 2 system, and without understanding these core concepts, the learner cannot progress to more advanced topics.
Independent Test: Can be fully tested by successfully running a basic publisher/subscriber example and explaining the communication patterns.
Acceptance Scenarios:
- Given a basic understanding of programming concepts, When the user reads the ROS 2 fundamentals chapter, Then they can explain the roles of nodes and topics
- Given a working ROS 2 environment, When the user runs the provided publisher/subscriber example, Then they can observe the message passing between nodes
User Story 2 - Python Integration with rclpy (Priority: P2)
As an AI developer, I want to use rclpy to connect Python agents to robot controllers.
Why this priority: Python is the primary language for AI development, so connecting AI agents to ROS 2 systems is critical for the target audience.
Independent Test: Can be fully tested by successfully running a Python script that communicates with ROS 2 and demonstrates control concepts.
Acceptance Scenarios:
- Given a ROS 2 environment with rclpy installed, When the user runs the Python examples, Then they can see the interaction between Python code and robot controllers
User Story 3 - URDF for Humanoid Robots (Priority: P3)
As a humanoid robotics learner, I want to understand URDF so I can model a robot body.
Why this priority: Understanding robot modeling is essential for working with humanoid robots, which is the main focus of this curriculum.
Independent Test: Can be fully tested by successfully loading and modifying a basic humanoid URDF model.
Acceptance Scenarios:
- Given a basic URDF file, When the user modifies it following the tutorial, Then they can create a valid robot body model
Edge Cases
- ROS 2 distribution mismatch
- Missing dependencies during setup
- URDF syntax errors causing launch failures
- Python environment misconfiguration
Requirements (mandatory)
Functional Requirements
- FR-001: The module MUST explain ROS 2 nodes, topics, and services accurately.
- FR-002: The module MUST include executable rclpy examples.
- FR-003: The module MUST include a valid humanoid URDF example.
- FR-004: The module MUST avoid ROS 1 concepts.
Key Entities
- ROS 2 Nodes
- Topics and Services
- rclpy Python scripts
- URDF files (links, joints, hierarchy)
- Code blocks and CLI commands
- Diagrams (conceptual or ASCII)
Success Criteria (mandatory)
Measurable Outcomes
- SC-001: Users can successfully run a ROS 2 publisher/subscriber example.
- SC-002: Users can modify and execute a Python rclpy node.
- SC-003: Users can explain the role of nodes and topics.
- SC-004: Users can understand and edit a basic humanoid URDF model.
Out of Scope
- ROS 1
- Simulation tools (Gazebo / Isaac)
- Hardware deployment
Content Structure
- Chapter 1: ROS 2 Fundamentals for Humanoid Robots
- Chapter 2: Nodes, Topics, Services & rclpy Integration
- Chapter 3: URDF Basics for Humanoids
--------------------------------------------------------------------------------
URL: https://Muham251.github.io/ai-hackathon-1/docs/module-1/#user-story-2---python-integration-with-rclpy-priority-p2
Content:
Feature Specification: Module 1 ‚Äî The Robotic Nervous System (ROS 2)
Feature Branch: module-1
Created: December 7, 2025
Status: Draft
Input: User description: "Module: 1 ‚Äî The Robotic Nervous System (ROS 2)"
User Scenarios & Testing (mandatory)
User Story 1 - ROS 2 Fundamentals (Priority: P1)
As a beginner robotics student, I want to understand ROS 2 nodes and topics so I can run basic robot programs.
Why this priority: This is the foundational knowledge required to work with any ROS 2 system, and without understanding these core concepts, the learner cannot progress to more advanced topics.
Independent Test: Can be fully tested by successfully running a basic publisher/subscriber example and explaining the communication patterns.
Acceptance Scenarios:
- Given a basic understanding of programming concepts, When the user reads the ROS 2 fundamentals chapter, Then they can explain the roles of nodes and topics
- Given a working ROS 2 environment, When the user runs the provided publisher/subscriber example, Then they can observe the message passing between nodes
User Story 2 - Python Integration with rclpy (Priority: P2)
As an AI developer, I want to use rclpy to connect Python agents to robot controllers.
Why this priority: Python is the primary language for AI development, so connecting AI agents to ROS 2 systems is critical for the target audience.
Independent Test: Can be fully tested by successfully running a Python script that communicates with ROS 2 and demonstrates control concepts.
Acceptance Scenarios:
- Given a ROS 2 environment with rclpy installed, When the user runs the Python examples, Then they can see the interaction between Python code and robot controllers
User Story 3 - URDF for Humanoid Robots (Priority: P3)
As a humanoid robotics learner, I want to understand URDF so I can model a robot body.
Why this priority: Understanding robot modeling is essential for working with humanoid robots, which is the main focus of this curriculum.
Independent Test: Can be fully tested by successfully loading and modifying a basic humanoid URDF model.
Acceptance Scenarios:
- Given a basic URDF file, When the user modifies it following the tutorial, Then they can create a valid robot body model
Edge Cases
- ROS 2 distribution mismatch
- Missing dependencies during setup
- URDF syntax errors causing launch failures
- Python environment misconfiguration
Requirements (mandatory)
Functional Requirements
- FR-001: The module MUST explain ROS 2 nodes, topics, and services accurately.
- FR-002: The module MUST include executable rclpy examples.
- FR-003: The module MUST include a valid humanoid URDF example.
- FR-004: The module MUST avoid ROS 1 concepts.
Key Entities
- ROS 2 Nodes
- Topics and Services
- rclpy Python scripts
- URDF files (links, joints, hierarchy)
- Code blocks and CLI commands
- Diagrams (conceptual or ASCII)
Success Criteria (mandatory)
Measurable Outcomes
- SC-001: Users can successfully run a ROS 2 publisher/subscriber example.
- SC-002: Users can modify and execute a Python rclpy node.
- SC-003: Users can explain the role of nodes and topics.
- SC-004: Users can understand and edit a basic humanoid URDF model.
Out of Scope
- ROS 1
- Simulation tools (Gazebo / Isaac)
- Hardware deployment
Content Structure
- Chapter 1: ROS 2 Fundamentals for Humanoid Robots
- Chapter 2: Nodes, Topics, Services & rclpy Integration
- Chapter 3: URDF Basics for Humanoids
--------------------------------------------------------------------------------
URL: https://Muham251.github.io/ai-hackathon-1/docs/module-1/#user-story-3---urdf-for-humanoid-robots-priority-p3
Content:
Feature Specification: Module 1 ‚Äî The Robotic Nervous System (ROS 2)
Feature Branch: module-1
Created: December 7, 2025
Status: Draft
Input: User description: "Module: 1 ‚Äî The Robotic Nervous System (ROS 2)"
User Scenarios & Testing (mandatory)
User Story 1 - ROS 2 Fundamentals (Priority: P1)
As a beginner robotics student, I want to understand ROS 2 nodes and topics so I can run basic robot programs.
Why this priority: This is the foundational knowledge required to work with any ROS 2 system, and without understanding these core concepts, the learner cannot progress to more advanced topics.
Independent Test: Can be fully tested by successfully running a basic publisher/subscriber example and explaining the communication patterns.
Acceptance Scenarios:
- Given a basic understanding of programming concepts, When the user reads the ROS 2 fundamentals chapter, Then they can explain the roles of nodes and topics
- Given a working ROS 2 environment, When the user runs the provided publisher/subscriber example, Then they can observe the message passing between nodes
User Story 2 - Python Integration with rclpy (Priority: P2)
As an AI developer, I want to use rclpy to connect Python agents to robot controllers.
Why this priority: Python is the primary language for AI development, so connecting AI agents to ROS 2 systems is critical for the target audience.
Independent Test: Can be fully tested by successfully running a Python script that communicates with ROS 2 and demonstrates control concepts.
Acceptance Scenarios:
- Given a ROS 2 environment with rclpy installed, When the user runs the Python examples, Then they can see the interaction between Python code and robot controllers
User Story 3 - URDF for Humanoid Robots (Priority: P3)
As a humanoid robotics learner, I want to understand URDF so I can model a robot body.
Why this priority: Understanding robot modeling is essential for working with humanoid robots, which is the main focus of this curriculum.
Independent Test: Can be fully tested by successfully loading and modifying a basic humanoid URDF model.
Acceptance Scenarios:
- Given a basic URDF file, When the user modifies it following the tutorial, Then they can create a valid robot body model
Edge Cases
- ROS 2 distribution mismatch
- Missing dependencies during setup
- URDF syntax errors causing launch failures
- Python environment misconfiguration
Requirements (mandatory)
Functional Requirements
- FR-001: The module MUST explain ROS 2 nodes, topics, and services accurately.
- FR-002: The module MUST include executable rclpy examples.
- FR-003: The module MUST include a valid humanoid URDF example.
- FR-004: The module MUST avoid ROS 1 concepts.
Key Entities
- ROS 2 Nodes
- Topics and Services
- rclpy Python scripts
- URDF files (links, joints, hierarchy)
- Code blocks and CLI commands
- Diagrams (conceptual or ASCII)
Success Criteria (mandatory)
Measurable Outcomes
- SC-001: Users can successfully run a ROS 2 publisher/subscriber example.
- SC-002: Users can modify and execute a Python rclpy node.
- SC-003: Users can explain the role of nodes and topics.
- SC-004: Users can understand and edit a basic humanoid URDF model.
Out of Scope
- ROS 1
- Simulation tools (Gazebo / Isaac)
- Hardware deployment
Content Structure
- Chapter 1: ROS 2 Fundamentals for Humanoid Robots
- Chapter 2: Nodes, Topics, Services & rclpy Integration
- Chapter 3: URDF Basics for Humanoids
--------------------------------------------------------------------------------
URL: https://Muham251.github.io/ai-hackathon-1/docs/module-1/#edge-cases
Content:
Feature Specification: Module 1 ‚Äî The Robotic Nervous System (ROS 2)
Feature Branch: module-1
Created: December 7, 2025
Status: Draft
Input: User description: "Module: 1 ‚Äî The Robotic Nervous System (ROS 2)"
User Scenarios & Testing (mandatory)
User Story 1 - ROS 2 Fundamentals (Priority: P1)
As a beginner robotics student, I want to understand ROS 2 nodes and topics so I can run basic robot programs.
Why this priority: This is the foundational knowledge required to work with any ROS 2 system, and without understanding these core concepts, the learner cannot progress to more advanced topics.
Independent Test: Can be fully tested by successfully running a basic publisher/subscriber example and explaining the communication patterns.
Acceptance Scenarios:
- Given a basic understanding of programming concepts, When the user reads the ROS 2 fundamentals chapter, Then they can explain the roles of nodes and topics
- Given a working ROS 2 environment, When the user runs the provided publisher/subscriber example, Then they can observe the message passing between nodes
User Story 2 - Python Integration with rclpy (Priority: P2)
As an AI developer, I want to use rclpy to connect Python agents to robot controllers.
Why this priority: Python is the primary language for AI development, so connecting AI agents to ROS 2 systems is critical for the target audience.
Independent Test: Can be fully tested by successfully running a Python script that communicates with ROS 2 and demonstrates control concepts.
Acceptance Scenarios:
- Given a ROS 2 environment with rclpy installed, When the user runs the Python examples, Then they can see the interaction between Python code and robot controllers
User Story 3 - URDF for Humanoid Robots (Priority: P3)
As a humanoid robotics learner, I want to understand URDF so I can model a robot body.
Why this priority: Understanding robot modeling is essential for working with humanoid robots, which is the main focus of this curriculum.
Independent Test: Can be fully tested by successfully loading and modifying a basic humanoid URDF model.
Acceptance Scenarios:
- Given a basic URDF file, When the user modifies it following the tutorial, Then they can create a valid robot body model
Edge Cases
- ROS 2 distribution mismatch
- Missing dependencies during setup
- URDF syntax errors causing launch failures
- Python environment misconfiguration
Requirements (mandatory)
Functional Requirements
- FR-001: The module MUST explain ROS 2 nodes, topics, and services accurately.
- FR-002: The module MUST include executable rclpy examples.
- FR-003: The module MUST include a valid humanoid URDF example.
- FR-004: The module MUST avoid ROS 1 concepts.
Key Entities
- ROS 2 Nodes
- Topics and Services
- rclpy Python scripts
- URDF files (links, joints, hierarchy)
- Code blocks and CLI commands
- Diagrams (conceptual or ASCII)
Success Criteria (mandatory)
Measurable Outcomes
- SC-001: Users can successfully run a ROS 2 publisher/subscriber example.
- SC-002: Users can modify and execute a Python rclpy node.
- SC-003: Users can explain the role of nodes and topics.
- SC-004: Users can understand and edit a basic humanoid URDF model.
Out of Scope
- ROS 1
- Simulation tools (Gazebo / Isaac)
- Hardware deployment
Content Structure
- Chapter 1: ROS 2 Fundamentals for Humanoid Robots
- Chapter 2: Nodes, Topics, Services & rclpy Integration
- Chapter 3: URDF Basics for Humanoids
--------------------------------------------------------------------------------
URL: https://Muham251.github.io/ai-hackathon-1/docs/module-1/#requirements-mandatory
Content:
Feature Specification: Module 1 ‚Äî The Robotic Nervous System (ROS 2)
Feature Branch: module-1
Created: December 7, 2025
Status: Draft
Input: User description: "Module: 1 ‚Äî The Robotic Nervous System (ROS 2)"
User Scenarios & Testing (mandatory)
User Story 1 - ROS 2 Fundamentals (Priority: P1)
As a beginner robotics student, I want to understand ROS 2 nodes and topics so I can run basic robot programs.
Why this priority: This is the foundational knowledge required to work with any ROS 2 system, and without understanding these core concepts, the learner cannot progress to more advanced topics.
Independent Test: Can be fully tested by successfully running a basic publisher/subscriber example and explaining the communication patterns.
Acceptance Scenarios:
- Given a basic understanding of programming concepts, When the user reads the ROS 2 fundamentals chapter, Then they can explain the roles of nodes and topics
- Given a working ROS 2 environment, When the user runs the provided publisher/subscriber example, Then they can observe the message passing between nodes
User Story 2 - Python Integration with rclpy (Priority: P2)
As an AI developer, I want to use rclpy to connect Python agents to robot controllers.
Why this priority: Python is the primary language for AI development, so connecting AI agents to ROS 2 systems is critical for the target audience.
Independent Test: Can be fully tested by successfully running a Python script that communicates with ROS 2 and demonstrates control concepts.
Acceptance Scenarios:
- Given a ROS 2 environment with rclpy installed, When the user runs the Python examples, Then they can see the interaction between Python code and robot controllers
User Story 3 - URDF for Humanoid Robots (Priority: P3)
As a humanoid robotics learner, I want to understand URDF so I can model a robot body.
Why this priority: Understanding robot modeling is essential for working with humanoid robots, which is the main focus of this curriculum.
Independent Test: Can be fully tested by successfully loading and modifying a basic humanoid URDF model.
Acceptance Scenarios:
- Given a basic URDF file, When the user modifies it following the tutorial, Then they can create a valid robot body model
Edge Cases
- ROS 2 distribution mismatch
- Missing dependencies during setup
- URDF syntax errors causing launch failures
- Python environment misconfiguration
Requirements (mandatory)
Functional Requirements
- FR-001: The module MUST explain ROS 2 nodes, topics, and services accurately.
- FR-002: The module MUST include executable rclpy examples.
- FR-003: The module MUST include a valid humanoid URDF example.
- FR-004: The module MUST avoid ROS 1 concepts.
Key Entities
- ROS 2 Nodes
- Topics and Services
- rclpy Python scripts
- URDF files (links, joints, hierarchy)
- Code blocks and CLI commands
- Diagrams (conceptual or ASCII)
Success Criteria (mandatory)
Measurable Outcomes
- SC-001: Users can successfully run a ROS 2 publisher/subscriber example.
- SC-002: Users can modify and execute a Python rclpy node.
- SC-003: Users can explain the role of nodes and topics.
- SC-004: Users can understand and edit a basic humanoid URDF model.
Out of Scope
- ROS 1
- Simulation tools (Gazebo / Isaac)
- Hardware deployment
Content Structure
- Chapter 1: ROS 2 Fundamentals for Humanoid Robots
- Chapter 2: Nodes, Topics, Services & rclpy Integration
- Chapter 3: URDF Basics for Humanoids
--------------------------------------------------------------------------------
URL: https://Muham251.github.io/ai-hackathon-1/docs/module-1/#functional-requirements
Content:
Feature Specification: Module 1 ‚Äî The Robotic Nervous System (ROS 2)
Feature Branch: module-1
Created: December 7, 2025
Status: Draft
Input: User description: "Module: 1 ‚Äî The Robotic Nervous System (ROS 2)"
User Scenarios & Testing (mandatory)
User Story 1 - ROS 2 Fundamentals (Priority: P1)
As a beginner robotics student, I want to understand ROS 2 nodes and topics so I can run basic robot programs.
Why this priority: This is the foundational knowledge required to work with any ROS 2 system, and without understanding these core concepts, the learner cannot progress to more advanced topics.
Independent Test: Can be fully tested by successfully running a basic publisher/subscriber example and explaining the communication patterns.
Acceptance Scenarios:
- Given a basic understanding of programming concepts, When the user reads the ROS 2 fundamentals chapter, Then they can explain the roles of nodes and topics
- Given a working ROS 2 environment, When the user runs the provided publisher/subscriber example, Then they can observe the message passing between nodes
User Story 2 - Python Integration with rclpy (Priority: P2)
As an AI developer, I want to use rclpy to connect Python agents to robot controllers.
Why this priority: Python is the primary language for AI development, so connecting AI agents to ROS 2 systems is critical for the target audience.
Independent Test: Can be fully tested by successfully running a Python script that communicates with ROS 2 and demonstrates control concepts.
Acceptance Scenarios:
- Given a ROS 2 environment with rclpy installed, When the user runs the Python examples, Then they can see the interaction between Python code and robot controllers
User Story 3 - URDF for Humanoid Robots (Priority: P3)
As a humanoid robotics learner, I want to understand URDF so I can model a robot body.
Why this priority: Understanding robot modeling is essential for working with humanoid robots, which is the main focus of this curriculum.
Independent Test: Can be fully tested by successfully loading and modifying a basic humanoid URDF model.
Acceptance Scenarios:
- Given a basic URDF file, When the user modifies it following the tutorial, Then they can create a valid robot body model
Edge Cases
- ROS 2 distribution mismatch
- Missing dependencies during setup
- URDF syntax errors causing launch failures
- Python environment misconfiguration
Requirements (mandatory)
Functional Requirements
- FR-001: The module MUST explain ROS 2 nodes, topics, and services accurately.
- FR-002: The module MUST include executable rclpy examples.
- FR-003: The module MUST include a valid humanoid URDF example.
- FR-004: The module MUST avoid ROS 1 concepts.
Key Entities
- ROS 2 Nodes
- Topics and Services
- rclpy Python scripts
- URDF files (links, joints, hierarchy)
- Code blocks and CLI commands
- Diagrams (conceptual or ASCII)
Success Criteria (mandatory)
Measurable Outcomes
- SC-001: Users can successfully run a ROS 2 publisher/subscriber example.
- SC-002: Users can modify and execute a Python rclpy node.
- SC-003: Users can explain the role of nodes and topics.
- SC-004: Users can understand and edit a basic humanoid URDF model.
Out of Scope
- ROS 1
- Simulation tools (Gazebo / Isaac)
- Hardware deployment
Content Structure
- Chapter 1: ROS 2 Fundamentals for Humanoid Robots
- Chapter 2: Nodes, Topics, Services & rclpy Integration
- Chapter 3: URDF Basics for Humanoids
--------------------------------------------------------------------------------
URL: https://Muham251.github.io/ai-hackathon-1/docs/module-1/#key-entities
Content:
Feature Specification: Module 1 ‚Äî The Robotic Nervous System (ROS 2)
Feature Branch: module-1
Created: December 7, 2025
Status: Draft
Input: User description: "Module: 1 ‚Äî The Robotic Nervous System (ROS 2)"
User Scenarios & Testing (mandatory)
User Story 1 - ROS 2 Fundamentals (Priority: P1)
As a beginner robotics student, I want to understand ROS 2 nodes and topics so I can run basic robot programs.
Why this priority: This is the foundational knowledge required to work with any ROS 2 system, and without understanding these core concepts, the learner cannot progress to more advanced topics.
Independent Test: Can be fully tested by successfully running a basic publisher/subscriber example and explaining the communication patterns.
Acceptance Scenarios:
- Given a basic understanding of programming concepts, When the user reads the ROS 2 fundamentals chapter, Then they can explain the roles of nodes and topics
- Given a working ROS 2 environment, When the user runs the provided publisher/subscriber example, Then they can observe the message passing between nodes
User Story 2 - Python Integration with rclpy (Priority: P2)
As an AI developer, I want to use rclpy to connect Python agents to robot controllers.
Why this priority: Python is the primary language for AI development, so connecting AI agents to ROS 2 systems is critical for the target audience.
Independent Test: Can be fully tested by successfully running a Python script that communicates with ROS 2 and demonstrates control concepts.
Acceptance Scenarios:
- Given a ROS 2 environment with rclpy installed, When the user runs the Python examples, Then they can see the interaction between Python code and robot controllers
User Story 3 - URDF for Humanoid Robots (Priority: P3)
As a humanoid robotics learner, I want to understand URDF so I can model a robot body.
Why this priority: Understanding robot modeling is essential for working with humanoid robots, which is the main focus of this curriculum.
Independent Test: Can be fully tested by successfully loading and modifying a basic humanoid URDF model.
Acceptance Scenarios:
- Given a basic URDF file, When the user modifies it following the tutorial, Then they can create a valid robot body model
Edge Cases
- ROS 2 distribution mismatch
- Missing dependencies during setup
- URDF syntax errors causing launch failures
- Python environment misconfiguration
Requirements (mandatory)
Functional Requirements
- FR-001: The module MUST explain ROS 2 nodes, topics, and services accurately.
- FR-002: The module MUST include executable rclpy examples.
- FR-003: The module MUST include a valid humanoid URDF example.
- FR-004: The module MUST avoid ROS 1 concepts.
Key Entities
- ROS 2 Nodes
- Topics and Services
- rclpy Python scripts
- URDF files (links, joints, hierarchy)
- Code blocks and CLI commands
- Diagrams (conceptual or ASCII)
Success Criteria (mandatory)
Measurable Outcomes
- SC-001: Users can successfully run a ROS 2 publisher/subscriber example.
- SC-002: Users can modify and execute a Python rclpy node.
- SC-003: Users can explain the role of nodes and topics.
- SC-004: Users can understand and edit a basic humanoid URDF model.
Out of Scope
- ROS 1
- Simulation tools (Gazebo / Isaac)
- Hardware deployment
Content Structure
- Chapter 1: ROS 2 Fundamentals for Humanoid Robots
- Chapter 2: Nodes, Topics, Services & rclpy Integration
- Chapter 3: URDF Basics for Humanoids
--------------------------------------------------------------------------------
URL: https://Muham251.github.io/ai-hackathon-1/docs/module-1/#success-criteria-mandatory
Content:
Feature Specification: Module 1 ‚Äî The Robotic Nervous System (ROS 2)
Feature Branch: module-1
Created: December 7, 2025
Status: Draft
Input: User description: "Module: 1 ‚Äî The Robotic Nervous System (ROS 2)"
User Scenarios & Testing (mandatory)
User Story 1 - ROS 2 Fundamentals (Priority: P1)
As a beginner robotics student, I want to understand ROS 2 nodes and topics so I can run basic robot programs.
Why this priority: This is the foundational knowledge required to work with any ROS 2 system, and without understanding these core concepts, the learner cannot progress to more advanced topics.
Independent Test: Can be fully tested by successfully running a basic publisher/subscriber example and explaining the communication patterns.
Acceptance Scenarios:
- Given a basic understanding of programming concepts, When the user reads the ROS 2 fundamentals chapter, Then they can explain the roles of nodes and topics
- Given a working ROS 2 environment, When the user runs the provided publisher/subscriber example, Then they can observe the message passing between nodes
User Story 2 - Python Integration with rclpy (Priority: P2)
As an AI developer, I want to use rclpy to connect Python agents to robot controllers.
Why this priority: Python is the primary language for AI development, so connecting AI agents to ROS 2 systems is critical for the target audience.
Independent Test: Can be fully tested by successfully running a Python script that communicates with ROS 2 and demonstrates control concepts.
Acceptance Scenarios:
- Given a ROS 2 environment with rclpy installed, When the user runs the Python examples, Then they can see the interaction between Python code and robot controllers
User Story 3 - URDF for Humanoid Robots (Priority: P3)
As a humanoid robotics learner, I want to understand URDF so I can model a robot body.
Why this priority: Understanding robot modeling is essential for working with humanoid robots, which is the main focus of this curriculum.
Independent Test: Can be fully tested by successfully loading and modifying a basic humanoid URDF model.
Acceptance Scenarios:
- Given a basic URDF file, When the user modifies it following the tutorial, Then they can create a valid robot body model
Edge Cases
- ROS 2 distribution mismatch
- Missing dependencies during setup
- URDF syntax errors causing launch failures
- Python environment misconfiguration
Requirements (mandatory)
Functional Requirements
- FR-001: The module MUST explain ROS 2 nodes, topics, and services accurately.
- FR-002: The module MUST include executable rclpy examples.
- FR-003: The module MUST include a valid humanoid URDF example.
- FR-004: The module MUST avoid ROS 1 concepts.
Key Entities
- ROS 2 Nodes
- Topics and Services
- rclpy Python scripts
- URDF files (links, joints, hierarchy)
- Code blocks and CLI commands
- Diagrams (conceptual or ASCII)
Success Criteria (mandatory)
Measurable Outcomes
- SC-001: Users can successfully run a ROS 2 publisher/subscriber example.
- SC-002: Users can modify and execute a Python rclpy node.
- SC-003: Users can explain the role of nodes and topics.
- SC-004: Users can understand and edit a basic humanoid URDF model.
Out of Scope
- ROS 1
- Simulation tools (Gazebo / Isaac)
- Hardware deployment
Content Structure
- Chapter 1: ROS 2 Fundamentals for Humanoid Robots
- Chapter 2: Nodes, Topics, Services & rclpy Integration
- Chapter 3: URDF Basics for Humanoids
--------------------------------------------------------------------------------
URL: https://Muham251.github.io/ai-hackathon-1/docs/module-1/#measurable-outcomes
Content:
Feature Specification: Module 1 ‚Äî The Robotic Nervous System (ROS 2)
Feature Branch: module-1
Created: December 7, 2025
Status: Draft
Input: User description: "Module: 1 ‚Äî The Robotic Nervous System (ROS 2)"
User Scenarios & Testing (mandatory)
User Story 1 - ROS 2 Fundamentals (Priority: P1)
As a beginner robotics student, I want to understand ROS 2 nodes and topics so I can run basic robot programs.
Why this priority: This is the foundational knowledge required to work with any ROS 2 system, and without understanding these core concepts, the learner cannot progress to more advanced topics.
Independent Test: Can be fully tested by successfully running a basic publisher/subscriber example and explaining the communication patterns.
Acceptance Scenarios:
- Given a basic understanding of programming concepts, When the user reads the ROS 2 fundamentals chapter, Then they can explain the roles of nodes and topics
- Given a working ROS 2 environment, When the user runs the provided publisher/subscriber example, Then they can observe the message passing between nodes
User Story 2 - Python Integration with rclpy (Priority: P2)
As an AI developer, I want to use rclpy to connect Python agents to robot controllers.
Why this priority: Python is the primary language for AI development, so connecting AI agents to ROS 2 systems is critical for the target audience.
Independent Test: Can be fully tested by successfully running a Python script that communicates with ROS 2 and demonstrates control concepts.
Acceptance Scenarios:
- Given a ROS 2 environment with rclpy installed, When the user runs the Python examples, Then they can see the interaction between Python code and robot controllers
User Story 3 - URDF for Humanoid Robots (Priority: P3)
As a humanoid robotics learner, I want to understand URDF so I can model a robot body.
Why this priority: Understanding robot modeling is essential for working with humanoid robots, which is the main focus of this curriculum.
Independent Test: Can be fully tested by successfully loading and modifying a basic humanoid URDF model.
Acceptance Scenarios:
- Given a basic URDF file, When the user modifies it following the tutorial, Then they can create a valid robot body model
Edge Cases
- ROS 2 distribution mismatch
- Missing dependencies during setup
- URDF syntax errors causing launch failures
- Python environment misconfiguration
Requirements (mandatory)
Functional Requirements
- FR-001: The module MUST explain ROS 2 nodes, topics, and services accurately.
- FR-002: The module MUST include executable rclpy examples.
- FR-003: The module MUST include a valid humanoid URDF example.
- FR-004: The module MUST avoid ROS 1 concepts.
Key Entities
- ROS 2 Nodes
- Topics and Services
- rclpy Python scripts
- URDF files (links, joints, hierarchy)
- Code blocks and CLI commands
- Diagrams (conceptual or ASCII)
Success Criteria (mandatory)
Measurable Outcomes
- SC-001: Users can successfully run a ROS 2 publisher/subscriber example.
- SC-002: Users can modify and execute a Python rclpy node.
- SC-003: Users can explain the role of nodes and topics.
- SC-004: Users can understand and edit a basic humanoid URDF model.
Out of Scope
- ROS 1
- Simulation tools (Gazebo / Isaac)
- Hardware deployment
Content Structure
- Chapter 1: ROS 2 Fundamentals for Humanoid Robots
- Chapter 2: Nodes, Topics, Services & rclpy Integration
- Chapter 3: URDF Basics for Humanoids
--------------------------------------------------------------------------------
URL: https://Muham251.github.io/ai-hackathon-1/docs/module-1/#out-of-scope
Content:
Feature Specification: Module 1 ‚Äî The Robotic Nervous System (ROS 2)
Feature Branch: module-1
Created: December 7, 2025
Status: Draft
Input: User description: "Module: 1 ‚Äî The Robotic Nervous System (ROS 2)"
User Scenarios & Testing (mandatory)
User Story 1 - ROS 2 Fundamentals (Priority: P1)
As a beginner robotics student, I want to understand ROS 2 nodes and topics so I can run basic robot programs.
Why this priority: This is the foundational knowledge required to work with any ROS 2 system, and without understanding these core concepts, the learner cannot progress to more advanced topics.
Independent Test: Can be fully tested by successfully running a basic publisher/subscriber example and explaining the communication patterns.
Acceptance Scenarios:
- Given a basic understanding of programming concepts, When the user reads the ROS 2 fundamentals chapter, Then they can explain the roles of nodes and topics
- Given a working ROS 2 environment, When the user runs the provided publisher/subscriber example, Then they can observe the message passing between nodes
User Story 2 - Python Integration with rclpy (Priority: P2)
As an AI developer, I want to use rclpy to connect Python agents to robot controllers.
Why this priority: Python is the primary language for AI development, so connecting AI agents to ROS 2 systems is critical for the target audience.
Independent Test: Can be fully tested by successfully running a Python script that communicates with ROS 2 and demonstrates control concepts.
Acceptance Scenarios:
- Given a ROS 2 environment with rclpy installed, When the user runs the Python examples, Then they can see the interaction between Python code and robot controllers
User Story 3 - URDF for Humanoid Robots (Priority: P3)
As a humanoid robotics learner, I want to understand URDF so I can model a robot body.
Why this priority: Understanding robot modeling is essential for working with humanoid robots, which is the main focus of this curriculum.
Independent Test: Can be fully tested by successfully loading and modifying a basic humanoid URDF model.
Acceptance Scenarios:
- Given a basic URDF file, When the user modifies it following the tutorial, Then they can create a valid robot body model
Edge Cases
- ROS 2 distribution mismatch
- Missing dependencies during setup
- URDF syntax errors causing launch failures
- Python environment misconfiguration
Requirements (mandatory)
Functional Requirements
- FR-001: The module MUST explain ROS 2 nodes, topics, and services accurately.
- FR-002: The module MUST include executable rclpy examples.
- FR-003: The module MUST include a valid humanoid URDF example.
- FR-004: The module MUST avoid ROS 1 concepts.
Key Entities
- ROS 2 Nodes
- Topics and Services
- rclpy Python scripts
- URDF files (links, joints, hierarchy)
- Code blocks and CLI commands
- Diagrams (conceptual or ASCII)
Success Criteria (mandatory)
Measurable Outcomes
- SC-001: Users can successfully run a ROS 2 publisher/subscriber example.
- SC-002: Users can modify and execute a Python rclpy node.
- SC-003: Users can explain the role of nodes and topics.
- SC-004: Users can understand and edit a basic humanoid URDF model.
Out of Scope
- ROS 1
- Simulation tools (Gazebo / Isaac)
- Hardware deployment
Content Structure
- Chapter 1: ROS 2 Fundamentals for Humanoid Robots
- Chapter 2: Nodes, Topics, Services & rclpy Integration
- Chapter 3: URDF Basics for Humanoids
--------------------------------------------------------------------------------
URL: https://Muham251.github.io/ai-hackathon-1/docs/module-1/#content-structure
Content:
Feature Specification: Module 1 ‚Äî The Robotic Nervous System (ROS 2)
Feature Branch: module-1
Created: December 7, 2025
Status: Draft
Input: User description: "Module: 1 ‚Äî The Robotic Nervous System (ROS 2)"
User Scenarios & Testing (mandatory)
User Story 1 - ROS 2 Fundamentals (Priority: P1)
As a beginner robotics student, I want to understand ROS 2 nodes and topics so I can run basic robot programs.
Why this priority: This is the foundational knowledge required to work with any ROS 2 system, and without understanding these core concepts, the learner cannot progress to more advanced topics.
Independent Test: Can be fully tested by successfully running a basic publisher/subscriber example and explaining the communication patterns.
Acceptance Scenarios:
- Given a basic understanding of programming concepts, When the user reads the ROS 2 fundamentals chapter, Then they can explain the roles of nodes and topics
- Given a working ROS 2 environment, When the user runs the provided publisher/subscriber example, Then they can observe the message passing between nodes
User Story 2 - Python Integration with rclpy (Priority: P2)
As an AI developer, I want to use rclpy to connect Python agents to robot controllers.
Why this priority: Python is the primary language for AI development, so connecting AI agents to ROS 2 systems is critical for the target audience.
Independent Test: Can be fully tested by successfully running a Python script that communicates with ROS 2 and demonstrates control concepts.
Acceptance Scenarios:
- Given a ROS 2 environment with rclpy installed, When the user runs the Python examples, Then they can see the interaction between Python code and robot controllers
User Story 3 - URDF for Humanoid Robots (Priority: P3)
As a humanoid robotics learner, I want to understand URDF so I can model a robot body.
Why this priority: Understanding robot modeling is essential for working with humanoid robots, which is the main focus of this curriculum.
Independent Test: Can be fully tested by successfully loading and modifying a basic humanoid URDF model.
Acceptance Scenarios:
- Given a basic URDF file, When the user modifies it following the tutorial, Then they can create a valid robot body model
Edge Cases
- ROS 2 distribution mismatch
- Missing dependencies during setup
- URDF syntax errors causing launch failures
- Python environment misconfiguration
Requirements (mandatory)
Functional Requirements
- FR-001: The module MUST explain ROS 2 nodes, topics, and services accurately.
- FR-002: The module MUST include executable rclpy examples.
- FR-003: The module MUST include a valid humanoid URDF example.
- FR-004: The module MUST avoid ROS 1 concepts.
Key Entities
- ROS 2 Nodes
- Topics and Services
- rclpy Python scripts
- URDF files (links, joints, hierarchy)
- Code blocks and CLI commands
- Diagrams (conceptual or ASCII)
Success Criteria (mandatory)
Measurable Outcomes
- SC-001: Users can successfully run a ROS 2 publisher/subscriber example.
- SC-002: Users can modify and execute a Python rclpy node.
- SC-003: Users can explain the role of nodes and topics.
- SC-004: Users can understand and edit a basic humanoid URDF model.
Out of Scope
- ROS 1
- Simulation tools (Gazebo / Isaac)
- Hardware deployment
Content Structure
- Chapter 1: ROS 2 Fundamentals for Humanoid Robots
- Chapter 2: Nodes, Topics, Services & rclpy Integration
- Chapter 3: URDF Basics for Humanoids
--------------------------------------------------------------------------------
URL: https://Muham251.github.io/ai-hackathon-1/docs/intro#getting-started
Content:
Tutorial Intro
Let's discover Docusaurus in less than 5 minutes.
Getting Started
Get started by creating a new site.
Or try Docusaurus immediately with docusaurus.new.
What you'll need
- Node.js version 20.0 or above:
- When installing Node.js, you are recommended to check all checkboxes related to dependencies.
Generate a new site
Generate a new Docusaurus site using the classic template.
The classic template will automatically be added to your project after you run the command:
npm init docusaurus@latest my-website classic
You can type this command into Command Prompt, Powershell, Terminal, or any other integrated terminal of your code editor.
The command also installs all necessary dependencies you need to run Docusaurus.
Start your site
Run the development server:
cd my-website
npm run start
The cd
command changes the directory you're working with. In order to work with your newly created Docusaurus site, you'll need to navigate the terminal there.
The npm run start
command builds your website locally and serves it through a development server, ready for you to view at http://localhost:3000/.
Open docs/intro.md
(this page) and edit some lines: the site reloads automatically and displays your changes.
--------------------------------------------------------------------------------
URL: https://Muham251.github.io/ai-hackathon-1/docs/intro#what-youll-need
Content:
Tutorial Intro
Let's discover Docusaurus in less than 5 minutes.
Getting Started
Get started by creating a new site.
Or try Docusaurus immediately with docusaurus.new.
What you'll need
- Node.js version 20.0 or above:
- When installing Node.js, you are recommended to check all checkboxes related to dependencies.
Generate a new site
Generate a new Docusaurus site using the classic template.
The classic template will automatically be added to your project after you run the command:
npm init docusaurus@latest my-website classic
You can type this command into Command Prompt, Powershell, Terminal, or any other integrated terminal of your code editor.
The command also installs all necessary dependencies you need to run Docusaurus.
Start your site
Run the development server:
cd my-website
npm run start
The cd
command changes the directory you're working with. In order to work with your newly created Docusaurus site, you'll need to navigate the terminal there.
The npm run start
command builds your website locally and serves it through a development server, ready for you to view at http://localhost:3000/.
Open docs/intro.md
(this page) and edit some lines: the site reloads automatically and displays your changes.
--------------------------------------------------------------------------------
URL: https://Muham251.github.io/ai-hackathon-1/docs/intro#generate-a-new-site
Content:
Tutorial Intro
Let's discover Docusaurus in less than 5 minutes.
Getting Started
Get started by creating a new site.
Or try Docusaurus immediately with docusaurus.new.
What you'll need
- Node.js version 20.0 or above:
- When installing Node.js, you are recommended to check all checkboxes related to dependencies.
Generate a new site
Generate a new Docusaurus site using the classic template.
The classic template will automatically be added to your project after you run the command:
npm init docusaurus@latest my-website classic
You can type this command into Command Prompt, Powershell, Terminal, or any other integrated terminal of your code editor.
The command also installs all necessary dependencies you need to run Docusaurus.
Start your site
Run the development server:
cd my-website
npm run start
The cd
command changes the directory you're working with. In order to work with your newly created Docusaurus site, you'll need to navigate the terminal there.
The npm run start
command builds your website locally and serves it through a development server, ready for you to view at http://localhost:3000/.
Open docs/intro.md
(this page) and edit some lines: the site reloads automatically and displays your changes.
--------------------------------------------------------------------------------
URL: https://Muham251.github.io/ai-hackathon-1/docs/intro#start-your-site
Content:
Tutorial Intro
Let's discover Docusaurus in less than 5 minutes.
Getting Started
Get started by creating a new site.
Or try Docusaurus immediately with docusaurus.new.
What you'll need
- Node.js version 20.0 or above:
- When installing Node.js, you are recommended to check all checkboxes related to dependencies.
Generate a new site
Generate a new Docusaurus site using the classic template.
The classic template will automatically be added to your project after you run the command:
npm init docusaurus@latest my-website classic
You can type this command into Command Prompt, Powershell, Terminal, or any other integrated terminal of your code editor.
The command also installs all necessary dependencies you need to run Docusaurus.
Start your site
Run the development server:
cd my-website
npm run start
The cd
command changes the directory you're working with. In order to work with your newly created Docusaurus site, you'll need to navigate the terminal there.
The npm run start
command builds your website locally and serves it through a development server, ready for you to view at http://localhost:3000/.
Open docs/intro.md
(this page) and edit some lines: the site reloads automatically and displays your changes.
--------------------------------------------------------------------------------
